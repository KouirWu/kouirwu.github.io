<!DOCTYPE html>


<html theme="dark" showBanner="true" hasBanner="true" > 
<link href="https://cdn.staticfile.org/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet">
<link href="https://cdn.staticfile.org/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet">
<link href="https://cdn.staticfile.org/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet">
<script src="/js/color.global.min.js" ></script>
<script src="/js/load-settings.js" ></script>
<head>
  <meta charset="utf-8">
  
  
  

  
  <title>动手搭建大模型 | 季禾子寒舍</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="preload" href="/css/fonts/Roboto-Regular.ttf" as="font" type="font/ttf" crossorigin="anonymous">
  <link rel="preload" href="/css/fonts/Roboto-Bold.ttf" as="font" type="font/ttf" crossorigin="anonymous">

  <meta name="description" content="动手搭建大模型5.1 动手实现一个 LLaMA2 大模型Meta（原Facebook）于2023年2月发布第一款基于Transformer结构的大型语言模型LLaMA，并于同年7月发布同系列模型LLaMA2。我们在第四章已经学习了解的了LLM，记忆如何训练LLM等等。那本小节我们就来学习，如何动手写一个LLaMA2模型。 5.1.1 定义超参数首先我们需要定义一些超参数，这些超参数包括模型的大小、">
<meta property="og:type" content="article">
<meta property="og:title" content="动手搭建大模型">
<meta property="og:url" content="https://kouirwu.github.io/2025/06/09/%E5%8A%A8%E6%89%8B%E6%90%AD%E5%BB%BA%E5%A4%A7%E6%A8%A1%E5%9E%8B/index.html">
<meta property="og:site_name" content="季禾子寒舍">
<meta property="og:description" content="动手搭建大模型5.1 动手实现一个 LLaMA2 大模型Meta（原Facebook）于2023年2月发布第一款基于Transformer结构的大型语言模型LLaMA，并于同年7月发布同系列模型LLaMA2。我们在第四章已经学习了解的了LLM，记忆如何训练LLM等等。那本小节我们就来学习，如何动手写一个LLaMA2模型。 5.1.1 定义超参数首先我们需要定义一些超参数，这些超参数包括模型的大小、">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://kouirwu.github.io/2025/06/09/images/5-images/pretrain_dataset.png">
<meta property="og:image" content="https://kouirwu.github.io/2025/06/09/images/5-images/sftdataset.png">
<meta property="article:published_time" content="2025-06-09T09:03:32.000Z">
<meta property="article:modified_time" content="2025-06-09T09:07:19.607Z">
<meta property="article:author" content="Kouir Wu">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://kouirwu.github.io/2025/06/09/images/5-images/pretrain_dataset.png">
  
    <link rel="alternate" href="/atom.xml" title="季禾子寒舍" type="application/atom+xml">
  
  
    <link rel="icon" media="(prefers-color-scheme: light)" href="/images/favicon-light-32.png" sizes="32x32">
    <link rel="icon" media="(prefers-color-scheme: light)" href="/images/favicon-light-128.png" sizes="128x128">
    <link rel="icon" media="(prefers-color-scheme: light)" href="/images/favicon-light-180.png" sizes="180x180">
    <link rel="icon" media="(prefers-color-scheme: light)" href="/images/favicon-light-192.png" sizes="192x192">
    <link rel="icon" media="(prefers-color-scheme: dark)" href="/images/favicon-dark-32.png" sizes="32x32">
    <link rel="icon" media="(prefers-color-scheme: dark)" href="/images/favicon-dark-128.png" sizes="128x128">
    <link rel="icon" media="(prefers-color-scheme: dark)" href="/images/favicon-dark-180.png" sizes="180x180">
    <link rel="icon" media="(prefers-color-scheme: dark)" href="/images/favicon-dark-192.png" sizes="192x192">
  
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  
  
    
<div id="banner" class="">
  <img src="/images/background.jpg" itemprop="image">
  <div id="banner-dim"></div>
</div>
 
   
  <div id="main-grid" class="  ">
    <div id="nav" class=""  >
      <navbar id="navbar">
  <nav id="title-nav">
    <a href="/">
      <div id="vivia-logo">
        <div class="dot"></div>
        <div class="dot"></div>
        <div class="dot"></div>
        <div class="dot"></div>
      </div>
      <div>季禾子寒舍 </div>
    </a>
  </nav>
  <nav id="main-nav">
    
      <a class="main-nav-link" href="/">主页</a>
    
      <a class="main-nav-link" href="/leetcode">Leetcode指南</a>
    
      <a class="main-nav-link" href="/machine_learning">机器学习</a>
    
      <a class="main-nav-link" href="/archives">归档</a>
    
      <a class="main-nav-link" href="/about">关于我</a>
    
  </nav>
  <nav id="sub-nav">
    <a id="theme-btn" class="nav-icon">
      <span class="light-mode-icon"><svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" width="20"><path d="M438.5-829.913v-48q0-17.452 11.963-29.476 11.964-12.024 29.326-12.024 17.363 0 29.537 12.024t12.174 29.476v48q0 17.452-11.963 29.476-11.964 12.024-29.326 12.024-17.363 0-29.537-12.024T438.5-829.913Zm0 747.826v-48q0-17.452 11.963-29.476 11.964-12.024 29.326-12.024 17.363 0 29.537 12.024t12.174 29.476v48q0 17.452-11.963 29.476-11.964 12.024-29.326 12.024-17.363 0-29.537-12.024T438.5-82.087ZM877.913-438.5h-48q-17.452 0-29.476-11.963-12.024-11.964-12.024-29.326 0-17.363 12.024-29.537t29.476-12.174h48q17.452 0 29.476 11.963 12.024 11.964 12.024 29.326 0 17.363-12.024 29.537T877.913-438.5Zm-747.826 0h-48q-17.452 0-29.476-11.963-12.024-11.964-12.024-29.326 0-17.363 12.024-29.537T82.087-521.5h48q17.452 0 29.476 11.963 12.024 11.964 12.024 29.326 0 17.363-12.024 29.537T130.087-438.5Zm660.174-290.87-34.239 32q-12.913 12.674-29.565 12.174-16.653-.5-29.327-13.174-12.674-12.673-12.554-28.826.12-16.152 12.794-28.826l33-35q12.913-12.674 30.454-12.674t30.163 12.847q12.709 12.846 12.328 30.826-.38 17.98-13.054 30.653ZM262.63-203.978l-32 34q-12.913 12.674-30.454 12.674t-30.163-12.847q-12.709-12.846-12.328-30.826.38-17.98 13.054-30.653l33.239-31q12.913-12.674 29.565-12.174 16.653.5 29.327 13.174 12.674 12.673 12.554 28.826-.12 16.152-12.794 28.826Zm466.74 33.239-32-33.239q-12.674-12.913-12.174-29.565.5-16.653 13.174-29.327 12.673-12.674 28.826-13.054 16.152-.38 28.826 12.294l35 33q12.674 12.913 12.674 30.454t-12.847 30.163q-12.846 12.709-30.826 12.328-17.98-.38-30.653-13.054ZM203.978-697.37l-34-33q-12.674-12.913-13.174-29.945-.5-17.033 12.174-29.707t31.326-13.293q18.653-.62 31.326 13.054l32 34.239q11.674 12.913 11.174 29.565-.5 16.653-13.174 29.327-12.673 12.674-28.826 12.554-16.152-.12-28.826-12.794ZM480-240q-100 0-170-70t-70-170q0-100 70-170t170-70q100 0 170 70t70 170q0 100-70 170t-170 70Zm-.247-82q65.703 0 111.475-46.272Q637-414.544 637-480.247t-45.525-111.228Q545.95-637 480.247-637t-111.475 45.525Q323-545.95 323-480.247t45.525 111.975Q414.05-322 479.753-322ZM481-481Z"/></svg></span>
      <span class="dark-mode-icon"><svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" width="20"><path d="M480.239-116.413q-152.63 0-258.228-105.478Q116.413-327.37 116.413-480q0-130.935 77.739-227.435t206.304-125.043q43.022-9.631 63.87 10.869t3.478 62.805q-8.891 22.043-14.315 44.463-5.424 22.42-5.424 46.689 0 91.694 64.326 155.879 64.325 64.186 156.218 64.186 24.369 0 46.978-4.946 22.609-4.945 44.413-14.076 42.826-17.369 62.967 1.142 20.142 18.511 10.511 61.054Q807.174-280 712.63-198.206q-94.543 81.793-232.391 81.793Zm0-95q79.783 0 143.337-40.217 63.554-40.218 95.793-108.283-15.608 4.044-31.097 5.326-15.49 1.283-31.859.805-123.706-4.066-210.777-90.539-87.071-86.473-91.614-212.092-.24-16.369.923-31.978 1.164-15.609 5.446-30.978-67.826 32.478-108.282 96.152Q211.652-559.543 211.652-480q0 111.929 78.329 190.258 78.329 78.329 190.258 78.329ZM466.13-465.891Z"/></svg></span>
    </a>
    
      <a id="nav-rss-link" class="nav-icon mobile-hide" href="/atom.xml" title="RSS 订阅">
        <svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" width="20"><path d="M198-120q-25.846 0-44.23-18.384-18.384-18.385-18.384-44.23 0-25.846 18.384-44.23 18.384-18.385 44.23-18.385 25.846 0 44.23 18.385 18.384 18.384 18.384 44.23 0 25.845-18.384 44.23Q223.846-120 198-120Zm538.385 0q-18.846 0-32.923-13.769-14.076-13.769-15.922-33.23-8.692-100.616-51.077-188.654-42.385-88.039-109.885-155.539-67.5-67.501-155.539-109.885Q283-663.462 182.385-672.154q-19.461-1.846-33.23-16.23-13.769-14.385-13.769-33.846t14.076-32.922q14.077-13.461 32.923-12.23 120.076 8.692 226.038 58.768 105.961 50.077 185.73 129.846 79.769 79.769 129.846 185.731 50.077 105.961 58.769 226.038 1.231 18.846-12.538 32.922Q756.461-120 736.385-120Zm-252 0q-18.231 0-32.423-13.461t-18.653-33.538Q418.155-264.23 348.886-333.5q-69.27-69.27-166.501-84.423-20.077-4.462-33.538-18.961-13.461-14.5-13.461-33.346 0-19.076 13.884-33.23 13.884-14.153 33.115-10.922 136.769 15.384 234.384 112.999 97.615 97.615 112.999 234.384 3.231 19.23-10.538 33.115Q505.461-120 484.385-120Z"/></svg>
      </a>
    
    <div id="nav-menu-btn" class="nav-icon">
      <svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" width="20"><path d="M177.37-252.282q-17.453 0-29.477-11.964-12.024-11.963-12.024-29.326t12.024-29.537q12.024-12.174 29.477-12.174h605.26q17.453 0 29.477 11.964 12.024 11.963 12.024 29.326t-12.024 29.537q-12.024 12.174-29.477 12.174H177.37Zm0-186.218q-17.453 0-29.477-11.963-12.024-11.964-12.024-29.326 0-17.363 12.024-29.537T177.37-521.5h605.26q17.453 0 29.477 11.963 12.024 11.964 12.024 29.326 0 17.363-12.024 29.537T782.63-438.5H177.37Zm0-186.217q-17.453 0-29.477-11.964-12.024-11.963-12.024-29.326t12.024-29.537q12.024-12.174 29.477-12.174h605.26q17.453 0 29.477 11.964 12.024 11.963 12.024 29.326t-12.024 29.537q-12.024 12.174-29.477 12.174H177.37Z"/></svg>
    </div>
  </nav>
</navbar>
<div id="nav-dropdown" class="hidden">
  <div id="dropdown-link-list">
    
      <a class="nav-dropdown-link" href="/">主页</a>
    
      <a class="nav-dropdown-link" href="/leetcode">Leetcode指南</a>
    
      <a class="nav-dropdown-link" href="/machine_learning">机器学习</a>
    
      <a class="nav-dropdown-link" href="/archives">归档</a>
    
      <a class="nav-dropdown-link" href="/about">关于我</a>
    
    
      <a class="nav-dropdown-link" href="/atom.xml" title="RSS 订阅">RSS</a>
     
    </div>
</div>
<script>
  let dropdownBtn = document.getElementById("nav-menu-btn");
  let dropdownEle = document.getElementById("nav-dropdown");
  dropdownBtn.onclick = function() {
    dropdownEle.classList.toggle("hidden");
  }
</script>
    </div>
    <div id="sidebar-wrapper">
      <sidebar id="sidebar">
  
    <div class="widget-wrap">
  <div class="info-card">
    <div class="avatar">
      
        <image src=/images/touxiang.jpg></image>
      
      <div class="img-dim"></div>
    </div>
    <div class="info">
      <div class="username">Kouir Wu </div>
      <div class="dot"></div>
      <div class="subtitle">当你深处深渊，退无可退的时候，眼前只剩下向上的一条路。 </div>
      <div class="link-list">
        
          <a class="link-btn" target="_blank" rel="noopener" href="https://twitter.com" title="Twitter"><i class="fa-brands fa-twitter"></i></a>
        
          <a class="link-btn" target="_blank" rel="noopener" href="https://github.com/KouirWu" title="GitHub"><i class="fa-brands fa-github"></i></a>
         
      </div>  
    </div>
  </div>
</div>

  
  <div class="sticky">
    
      
  <div class="widget-wrap">
    <div class="widget">
      <h3 class="widget-title">分类</h3>
      <div class="category-box">
            <a class="category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/">
                机器学习实战
                <div class="category-count">1</div>
            </a>
        <div class="children" style="display: none;"><div class="category-box">
            <a class="category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/Titanic/">
                Titanic
                <div class="category-count">1</div>
            </a>
        </div></div>
            <a class="category-link" href="/categories/Deepseek/">
                Deepseek
                <div class="category-count">2</div>
            </a>
        <div class="children" style="display: none;"><div class="category-box">
            <a class="category-link" href="/categories/Deepseek/Linux/">
                Linux
                <div class="category-count">2</div>
            </a>
        <div class="children" style="display: none;"><div class="category-box">
            <a class="category-link" href="/categories/Deepseek/Linux/Ollama/">
                Ollama
                <div class="category-count">2</div>
            </a>
        </div></div></div></div>
            <a class="category-link" href="/categories/%E5%89%8D%E7%AB%AF%E5%BC%80%E5%8F%91/">
                前端开发
                <div class="category-count">3</div>
            </a>
        <div class="children" style="display: none;"><div class="category-box">
            <a class="category-link" href="/categories/%E5%89%8D%E7%AB%AF%E5%BC%80%E5%8F%91/Vue3/">
                Vue3
                <div class="category-count">3</div>
            </a>
        <div class="children" style="display: none;"><div class="category-box">
            <a class="category-link" href="/categories/%E5%89%8D%E7%AB%AF%E5%BC%80%E5%8F%91/Vue3/%E5%B0%8F%E5%85%94%E9%B2%9C/">
                小兔鲜
                <div class="category-count">2</div>
            </a>
        
            <a class="category-link" href="/categories/%E5%89%8D%E7%AB%AF%E5%BC%80%E5%8F%91/Vue3/%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3/">
                问题解决
                <div class="category-count">1</div>
            </a>
        <div class="children" style="display: none;"><div class="category-box">
            <a class="category-link" href="/categories/%E5%89%8D%E7%AB%AF%E5%BC%80%E5%8F%91/Vue3/%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3/vue3%E9%99%AA%E8%AF%8A%E7%B3%BB%E7%BB%9F/">
                vue3陪诊系统
                <div class="category-count">1</div>
            </a>
        </div></div></div></div></div></div>
            <a class="category-link" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">
                人工智能
                <div class="category-count">1</div>
            </a>
        <div class="children" style="display: none;"><div class="category-box">
            <a class="category-link" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/Trae-AI/">
                Trae AI
                <div class="category-count">1</div>
            </a>
        </div></div>
            <a class="category-link" href="/categories/leetcode/">
                leetcode
                <div class="category-count">4</div>
            </a>
        <div class="children" style="display: none;"><div class="category-box">
            <a class="category-link" href="/categories/leetcode/hot100/">
                hot100
                <div class="category-count">4</div>
            </a>
        </div></div>
            <a class="category-link" href="/categories/LLM%E5%A4%A7%E6%A8%A1%E5%9E%8B/">
                LLM大模型
                <div class="category-count">3</div>
            </a>
        
            <a class="category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">
                机器学习
                <div class="category-count">1</div>
            </a>
        <div class="children" style="display: none;"><div class="category-box">
            <a class="category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/">
                环境配置
                <div class="category-count">1</div>
            </a>
        </div></div></div>
    </div>
  </div>
  <script>
    document.querySelectorAll('.category-link').forEach(link => {
      link.addEventListener('click', function(e) {
        const children = this.nextElementSibling;
        if (children && children.classList.contains('children')) {
          e.preventDefault();
          if (children.style.display === 'none') {
            children.style.display = 'block';
          } else {
            children.style.display = 'none';
          }
        }
      });
    });
    // 设置初始状态为折叠状态
    document.querySelectorAll('.children').forEach(children => {
      children.style.display = 'none';
    });
  </script>


    
      
  <div class="widget-wrap">
    <div class="widget">
      <h3 class="widget-title">标签</h3>
      <ul class="widget-tag-list" itemprop="keywords"><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/Deepseek/" rel="tag">Deepseek</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/Element-Plus/" rel="tag">Element Plus</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/Kaggle/" rel="tag">Kaggle</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/Linux/" rel="tag">Linux</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/MCP/" rel="tag">MCP</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/Ollama/" rel="tag">Ollama</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/Python/" rel="tag">Python</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/Titanic/" rel="tag">Titanic</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/Trae-AI/" rel="tag">Trae AI</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/Vue3/" rel="tag">Vue3</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/vue3%E9%99%AA%E8%AF%8A%E7%B3%BB%E7%BB%9F/" rel="tag">vue3陪诊系统</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E5%89%8D%E7%AB%AF%E5%BC%80%E5%8F%91/" rel="tag">前端开发</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E5%8F%8C%E6%8C%87%E9%92%88/" rel="tag">双指针</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E5%93%88%E5%B8%8C%E8%A1%A8/" rel="tag">哈希表</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2/" rel="tag">字符串</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E5%B0%8F%E5%85%94%E9%B2%9C/" rel="tag">小兔鲜</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E5%B0%8F%E7%99%BD%E6%95%99%E7%A8%8B/" rel="tag">小白教程</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E5%B9%B6%E6%9F%A5%E9%9B%86/" rel="tag">并查集</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E6%8E%92%E5%BA%8F/" rel="tag">排序</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/" rel="tag">数据分析</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E6%95%B0%E7%BB%84/" rel="tag">数组</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/" rel="tag">环境搭建</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E9%85%8D%E7%BD%AE/" rel="tag">配置</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3/" rel="tag">问题解决</a></li></ul>
    </div>
  </div>


    
      
  <div class="widget-wrap">
    <div class="widget">
      <h3 class="widget-title">最新文章</h3>
      <ul>
        
          <a class="recent-link" href="/2025/06/09/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/" title="大模型应用" >
            <div class="recent-link-text">
              大模型应用
            </div>
          </a>
        
          <a class="recent-link" href="/2025/06/09/%E5%8A%A8%E6%89%8B%E6%90%AD%E5%BB%BA%E5%A4%A7%E6%A8%A1%E5%9E%8B/" title="动手搭建大模型" >
            <div class="recent-link-text">
              动手搭建大模型
            </div>
          </a>
        
          <a class="recent-link" href="/2025/06/09/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B%E5%AE%9E%E8%B7%B5/" title="大模型训练流程实践" >
            <div class="recent-link-text">
              大模型训练流程实践
            </div>
          </a>
        
          <a class="recent-link" href="/2025/06/06/hot100%E7%A7%BB%E5%8A%A8%E9%9B%B6/" title="✨ LeetCode Hot 100 - 283. 移动零 ✨" >
            <div class="recent-link-text">
              ✨ LeetCode Hot 100 - 283. 移动零 ✨
            </div>
          </a>
        
          <a class="recent-link" href="/2025/06/06/hot100%E6%9C%80%E9%95%BF%E8%BF%9E%E7%BB%AD%E5%BA%8F%E5%88%97/" title="✨ LeetCode Hot 100 - 128. 最长连续序列 ✨" >
            <div class="recent-link-text">
              ✨ LeetCode Hot 100 - 128. 最长连续序列 ✨
            </div>
          </a>
        
      </ul>
    </div>
  </div>

    
  </div>
</sidebar>
    </div>
    <div id="content-body">
       


<article id="post-动手搭建大模型" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  
    
   
  <div class="article-inner">
    <div class="article-main">
      <header class="article-header">
        
<div class="main-title-bar">
  <div class="main-title-dot"></div>
  
    
      <h1 class="p-name article-title" itemprop="headline name">
        动手搭建大模型
      </h1>
    
  
</div>

        <div class='meta-info-bar'>
          <div class="meta-info">
  <time class="dt-published" datetime="2025-06-09T09:03:32.000Z" itemprop="datePublished">2025-06-09</time>
</div>
          <div class="need-seperator meta-info">
            <div class="meta-cate-flex">
  
  <a class="meta-cate-link" href="/categories/LLM%E5%A4%A7%E6%A8%A1%E5%9E%8B/">LLM大模型</a>
   
</div>
  
          </div>
          <div class="wordcount need-seperator meta-info">
            63k 词 
          </div>
        </div>
        
      </header>
      <div class="e-content article-entry" itemprop="articleBody">
        
          <h1 id="动手搭建大模型"><a href="#动手搭建大模型" class="headerlink" title="动手搭建大模型"></a>动手搭建大模型</h1><h2 id="5-1-动手实现一个-LLaMA2-大模型"><a href="#5-1-动手实现一个-LLaMA2-大模型" class="headerlink" title="5.1 动手实现一个 LLaMA2 大模型"></a>5.1 动手实现一个 LLaMA2 大模型</h2><p>Meta（原Facebook）于2023年2月发布第一款基于Transformer结构的大型语言模型LLaMA，并于同年7月发布同系列模型LLaMA2。我们在第四章已经学习了解的了LLM，记忆如何训练LLM等等。那本小节我们就来学习，如何动手写一个LLaMA2模型。</p>
<h3 id="5-1-1-定义超参数"><a href="#5-1-1-定义超参数" class="headerlink" title="5.1.1 定义超参数"></a>5.1.1 定义超参数</h3><p>首先我们需要定义一些超参数，这些超参数包括模型的大小、层数、头数、词嵌入维度、隐藏层维度等等。这些超参数可以根据实际情况进行调整。</p>
<p>这里我们自定义一个<code>ModelConfig</code>类，来存储和记录我们的超参数，这里我们继承了<code>PretrainedConfig</code>类，这是<code>transformers</code>库中的参数类，我们可以通过继承这个类来方便的使用<code>transformers</code>库中的一些功能，也方便在后续导出Hugging Face模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> PretrainedConfig</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ModelConfig</span>(<span class="title class_ inherited__">PretrainedConfig</span>):</span><br><span class="line">    model_type = <span class="string">&quot;Tiny-K&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">            self,</span></span><br><span class="line"><span class="params">            dim: <span class="built_in">int</span> = <span class="number">768</span>, <span class="comment"># 模型维度</span></span></span><br><span class="line"><span class="params">            n_layers: <span class="built_in">int</span> = <span class="number">12</span>, <span class="comment"># Transformer的层数</span></span></span><br><span class="line"><span class="params">            n_heads: <span class="built_in">int</span> = <span class="number">16</span>, <span class="comment"># 注意力机制的头数</span></span></span><br><span class="line"><span class="params">            n_kv_heads: <span class="built_in">int</span> = <span class="number">8</span>, <span class="comment"># 键值头的数量</span></span></span><br><span class="line"><span class="params">            vocab_size: <span class="built_in">int</span> = <span class="number">6144</span>, <span class="comment"># 词汇表大小</span></span></span><br><span class="line"><span class="params">            hidden_dim: <span class="built_in">int</span> = <span class="literal">None</span>, <span class="comment"># 隐藏层维度</span></span></span><br><span class="line"><span class="params">            multiple_of: <span class="built_in">int</span> = <span class="number">64</span>, </span></span><br><span class="line"><span class="params">            norm_eps: <span class="built_in">float</span> = <span class="number">1e-5</span>, <span class="comment"># 归一化层的eps</span></span></span><br><span class="line"><span class="params">            max_seq_len: <span class="built_in">int</span> = <span class="number">512</span>, <span class="comment"># 最大序列长度</span></span></span><br><span class="line"><span class="params">            dropout: <span class="built_in">float</span> = <span class="number">0.0</span>, <span class="comment"># dropout概率</span></span></span><br><span class="line"><span class="params">            flash_attn: <span class="built_in">bool</span> = <span class="literal">True</span>, <span class="comment"># 是否使用Flash Attention</span></span></span><br><span class="line"><span class="params">            **kwargs,</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="variable language_">self</span>.dim = dim</span><br><span class="line">        <span class="variable language_">self</span>.n_layers = n_layers</span><br><span class="line">        <span class="variable language_">self</span>.n_heads = n_heads</span><br><span class="line">        <span class="variable language_">self</span>.n_kv_heads = n_kv_heads</span><br><span class="line">        <span class="variable language_">self</span>.vocab_size = vocab_size</span><br><span class="line">        <span class="variable language_">self</span>.hidden_dim = hidden_dim</span><br><span class="line">        <span class="variable language_">self</span>.multiple_of = multiple_of</span><br><span class="line">        <span class="variable language_">self</span>.norm_eps = norm_eps</span><br><span class="line">        <span class="variable language_">self</span>.max_seq_len = max_seq_len</span><br><span class="line">        <span class="variable language_">self</span>.dropout = dropout</span><br><span class="line">        <span class="variable language_">self</span>.flash_attn = flash_attn</span><br><span class="line">        <span class="built_in">super</span>().__init__(**kwargs)</span><br></pre></td></tr></table></figure>

<p>我们来看一下其中的一些超参数的含义，比如<code>dim</code>是模型维度，<code>n_layers</code>是Transformer的层数，<code>n_heads</code>是注意力机制的头数，<code>vocab_size</code>是词汇表大小，<code>max_seq_len</code>是输入的最大序列长度等等。上面的代码中也对每一个参数做了详细的注释，在后面的代码中我们会根据这些超参数来构建我们的模型。</p>
<h3 id="5-1-2-构建-RMSNorm"><a href="#5-1-2-构建-RMSNorm" class="headerlink" title="5.1.2 构建 RMSNorm"></a>5.1.2 构建 RMSNorm</h3><p><code>RMSNorm</code>可以用如下的数学公式表示：</p>
<p>$$<br>\text{RMSNorm}(x) &#x3D; \frac{x}{\sqrt{\frac{1}{n}\sum_{i&#x3D;1}^{n}x_i^2 + \epsilon}} \cdot \gamma<br>$$</p>
<p>其中：</p>
<ul>
<li>$x_i$ 是输入向量的第 $i$ 个元素</li>
<li>$\gamma$ 是可学习的缩放参数（对应代码中的 <code>self.weight</code>）</li>
<li>$n$ 是输入向量的维度数量</li>
<li>$\epsilon$ 是一个小常数，用于数值稳定性（以避免除以零的情况）</li>
</ul>
<p>这种归一化有助于通过确保权重的规模不会变得过大或过小来稳定学习过程，这在具有许多层的深度学习模型中特别有用。</p>
<p>我们可以通过如下代码实现<code>RMSNorm</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">RMSNorm</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim: <span class="built_in">int</span>, eps: <span class="built_in">float</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># eps是为了防止除以0的情况</span></span><br><span class="line">        <span class="variable language_">self</span>.eps = eps</span><br><span class="line">        <span class="comment"># weight是一个可学习的参数，全部初始化为1</span></span><br><span class="line">        <span class="variable language_">self</span>.weight = nn.Parameter(torch.ones(dim))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_norm</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 计算RMSNorm的核心部分</span></span><br><span class="line">        <span class="comment"># x.pow(2).mean(-1, keepdim=True)计算了输入x的平方的均值</span></span><br><span class="line">        <span class="comment"># torch.rsqrt是平方根的倒数，这样就得到了RMSNorm的分母部分，再加上eps防止分母为0</span></span><br><span class="line">        <span class="comment"># 最后乘以x，得到RMSNorm的结果</span></span><br><span class="line">        <span class="keyword">return</span> x * torch.rsqrt(x.<span class="built_in">pow</span>(<span class="number">2</span>).mean(-<span class="number">1</span>, keepdim=<span class="literal">True</span>) + <span class="variable language_">self</span>.eps)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># forward函数是模型的前向传播</span></span><br><span class="line">        <span class="comment"># 首先将输入x转为float类型，然后进行RMSNorm，最后再转回原来的数据类型</span></span><br><span class="line">        <span class="comment"># 最后乘以weight，这是RMSNorm的一个可学习的缩放因子</span></span><br><span class="line">        output = <span class="variable language_">self</span>._norm(x.<span class="built_in">float</span>()).type_as(x)</span><br><span class="line">        <span class="keyword">return</span> output * <span class="variable language_">self</span>.weight</span><br></pre></td></tr></table></figure>

<p>并且，我们可以用下面的代码来对<code>RMSNorm</code>模块进行测试，可以看到代码最终输出的形状为<code>torch.Size([1, 50, 288])</code>，与我们输入的形状一致，说明模块的实现是正确的，归一化并不会改变输入的形状。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">norm = RMSNorm(args.dim, args.norm_eps)</span><br><span class="line">x = torch.randn(<span class="number">1</span>, <span class="number">50</span>, args.dim)</span><br><span class="line">output = norm(x)</span><br><span class="line"><span class="built_in">print</span>(output.shape)</span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">orch.Size([<span class="number">1</span>, <span class="number">50</span>, <span class="number">288</span>])</span><br></pre></td></tr></table></figure>

<h3 id="5-1-3-构建-LLaMA2-Attention"><a href="#5-1-3-构建-LLaMA2-Attention" class="headerlink" title="5.1.3 构建 LLaMA2 Attention"></a>5.1.3 构建 LLaMA2 Attention</h3><p>在 LLaMA2 模型中，虽然只有 LLaMA2-70B模型使用了分组查询注意力机制（Grouped-Query Attention，GQA），但我们依然选择使用 GQA 来构建我们的 LLaMA Attention 模块，它可以提高模型的效率，并节省一些显存占用。</p>
<h4 id="5-1-3-1-repeat-kv"><a href="#5-1-3-1-repeat-kv" class="headerlink" title="5.1.3.1 repeat_kv"></a>5.1.3.1 repeat_kv</h4><p>在 LLaMA2 模型中，我们需要将键和值的维度扩展到和查询的维度一样，这样才能进行注意力计算。我们可以通过如下代码实现<code>repeat_kv</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">repeat_kv</span>(<span class="params">x: torch.Tensor, n_rep: <span class="built_in">int</span></span>) -&gt; torch.Tensor:</span><br><span class="line">    <span class="comment"># 获取输入张量的形状：批量大小、序列长度、键/值对头的数量、每个头的维度大小</span></span><br><span class="line">    bs, slen, n_kv_heads, head_dim = x.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 如果重复次数为1，则不需要重复，直接返回原始张量</span></span><br><span class="line">    <span class="keyword">if</span> n_rep == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 对张量进行扩展和重塑操作以重复键值对</span></span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        x[:, :, :, <span class="literal">None</span>, :]  <span class="comment"># 在第四个维度（头的维度前）添加一个新的维度</span></span><br><span class="line">        .expand(bs, slen, n_kv_heads, n_rep, head_dim)  <span class="comment"># 将新添加的维度扩展到n_rep大小，实现重复的效果</span></span><br><span class="line">        .reshape(bs, slen, n_kv_heads * n_rep, head_dim)  <span class="comment"># 重新塑形，合并键/值对头的数量和重复次数的维度</span></span><br><span class="line">    )</span><br></pre></td></tr></table></figure>

<p>在上述代码中：</p>
<ul>
<li><p>首先，获取输入张量的形状：首先，代码通过 x.shape 获取输入张量的形状，包括批量大小（bs）、序列长度（slen）、键&#x2F;值对头的数量（n_kv_heads）以及每个头的维度大小（head_dim）。</p>
</li>
<li><p>然后，检查重复次数：接着，代码检查重复次数 n_rep 是否为1。如果是1，则说明不需要对键和值进行重复，直接返回原始张量 x。</p>
</li>
<li><p>最后，扩展和重塑张量：</p>
<ul>
<li>在第三个维度（即键&#x2F;值对头的维度）之后添加一个新的维度，形成 <code>x[:, :, :, None, :]</code>。</li>
<li>使用 <code>expand</code> 方法将新添加的维度扩展到 <code>n_rep</code> 大小，实现键&#x2F;值对的重复效果。</li>
<li>最后，通过 reshape 方法重新塑形，将扩展后的维度合并回键&#x2F;值对头的数量中，即 <code>x.reshape(bs, slen, n_kv_heads * n_rep, head_dim)</code>，这样最终的张量形状就达到了与查询维度一致的效果。</li>
</ul>
</li>
</ul>
<h4 id="5-1-3-2-旋转嵌入"><a href="#5-1-3-2-旋转嵌入" class="headerlink" title="5.1.3.2 旋转嵌入"></a>5.1.3.2 旋转嵌入</h4><p>接着我们来实现旋转嵌入，旋转嵌入是 LLaMA2 模型中的一个重要组件，它可以为注意力机制提供更强的上下文信息，从而提高模型的性能。</p>
<p>首先，我们要构造获得旋转嵌入的实部和虚部的函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 注意：此处的dim应为 dim//n_head，因为我们是对每个head进行旋转嵌入</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">precompute_freqs_cis</span>(<span class="params">dim: <span class="built_in">int</span>, end: <span class="built_in">int</span>, theta: <span class="built_in">float</span> = <span class="number">10000.0</span></span>):</span><br><span class="line">    <span class="comment"># torch.arange(0, dim, 2)[: (dim // 2)].float()生成了一个从0开始，步长为2的序列，长度为dim的一半</span></span><br><span class="line">    <span class="comment"># 然后每个元素除以dim，再取theta的倒数，得到频率</span></span><br><span class="line">    freqs = <span class="number">1.0</span> / (theta ** (torch.arange(<span class="number">0</span>, dim, <span class="number">2</span>)[: (dim // <span class="number">2</span>)].<span class="built_in">float</span>() / dim))</span><br><span class="line">    <span class="comment"># 生成一个从0到end的序列，长度为end</span></span><br><span class="line">    t = torch.arange(end, device=freqs.device)</span><br><span class="line">    <span class="comment"># 计算外积，得到一个二维矩阵，每一行是t的元素乘以freqs的元素</span></span><br><span class="line">    freqs = torch.outer(t, freqs).<span class="built_in">float</span>()</span><br><span class="line">    <span class="comment"># 计算频率的余弦值，得到实部</span></span><br><span class="line">    freqs_cos = torch.cos(freqs)</span><br><span class="line">    <span class="comment"># 计算频率的正弦值，得到虚部</span></span><br><span class="line">    freqs_sin = torch.sin(freqs)</span><br><span class="line">    <span class="keyword">return</span> freqs_cos, freqs_sin</span><br></pre></td></tr></table></figure>

<ul>
<li>计算频率序列：<ul>
<li><code>torch.arange(0, dim, 2)[: (dim // 2)].float()</code> 生成了一个从0开始，步长为2的序列，其长度为<code>dim</code>的一半。</li>
<li>每个元素除以<code>dim</code>后取<code>theta</code>的倒数，得到一个频率序列 <code>freqs</code>。这一步是为了生成适合旋转嵌入的频率。</li>
</ul>
</li>
<li>生成时间序列：<ul>
<li><code>t = torch.arange(end, device=freqs.device)</code> 生成一个从<code>0</code>到<code>end</code>的序列，长度为<code>end</code>。<code>end</code>通常是序列的最大长度。</li>
</ul>
</li>
<li>计算频率的外积<ul>
<li><code>freqs = torch.outer(t, freqs).float()</code> 计算时间序列 <code>t</code> 和频率序列 <code>freqs</code> 的外积，得到一个二维矩阵 <code>freqs</code>。每一行是时间序列 <code>t</code> 的元素乘以频率序列 <code>freqs</code> 的元素。</li>
</ul>
</li>
<li>计算实部和虚部<ul>
<li><code>freqs_cos = torch.cos(freqs)</code> 计算频率矩阵 <code>freqs</code> 的余弦值，得到旋转嵌入的实部。</li>
<li><code>freqs_sin = torch.sin(freqs)</code> 计算频率矩阵 <code>freqs</code> 的正弦值，得到旋转嵌入的虚部。</li>
</ul>
</li>
</ul>
<p>最终，该函数返回两个矩阵 <code>freqs_cos</code> 和 <code>freqs_sin</code>，分别表示旋转嵌入的实部和虚部，用于后续的计算。</p>
<p>接着，我们来构造调整张量形状的<code>reshape_for_broadcast</code>函数，这个函数的主要目的是调整 <code>freqs_cis</code> 的形状，使其在进行广播操作时与 <code>x</code> 的维度对齐，从而能够进行正确的张量运算。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">reshape_for_broadcast</span>(<span class="params">freqs_cis: torch.Tensor, x: torch.Tensor</span>):</span><br><span class="line">    <span class="comment"># 获取x的维度数</span></span><br><span class="line">    ndim = x.ndim</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 断言，确保1在x的维度范围内</span></span><br><span class="line">    <span class="keyword">assert</span> <span class="number">0</span> &lt;= <span class="number">1</span> &lt; ndim</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 断言，确保freqs_cis的形状与x的第二维和最后一维相同</span></span><br><span class="line">    <span class="keyword">assert</span> freqs_cis.shape == (x.shape[<span class="number">1</span>], x.shape[-<span class="number">1</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 构造一个新的形状，除了第二维和最后一维，其他维度都为1，这样做是为了能够将freqs_cis与x进行广播操作</span></span><br><span class="line">    shape = [d <span class="keyword">if</span> i == <span class="number">1</span> <span class="keyword">or</span> i == ndim - <span class="number">1</span> <span class="keyword">else</span> <span class="number">1</span> <span class="keyword">for</span> i, d <span class="keyword">in</span> <span class="built_in">enumerate</span>(x.shape)]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将freqs_cis调整为新的形状，并返回</span></span><br><span class="line">    <span class="keyword">return</span> freqs_cis.view(shape)</span><br></pre></td></tr></table></figure>

<p>最后，我们可以通过如下代码实现旋转嵌入：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">apply_rotary_emb</span>(<span class="params"></span></span><br><span class="line"><span class="params">    xq: torch.Tensor,</span></span><br><span class="line"><span class="params">    xk: torch.Tensor,</span></span><br><span class="line"><span class="params">    freqs_cos: torch.Tensor,</span></span><br><span class="line"><span class="params">    freqs_sin: torch.Tensor</span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="type">Tuple</span>[torch.Tensor, torch.Tensor]:</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将查询和键张量转换为浮点数，并重塑形状以分离实部和虚部</span></span><br><span class="line">    xq_r, xq_i = xq.<span class="built_in">float</span>().reshape(xq.shape[:-<span class="number">1</span>] + (-<span class="number">1</span>, <span class="number">2</span>)).unbind(-<span class="number">1</span>)</span><br><span class="line">    xk_r, xk_i = xk.<span class="built_in">float</span>().reshape(xk.shape[:-<span class="number">1</span>] + (-<span class="number">1</span>, <span class="number">2</span>)).unbind(-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 重新塑形频率张量以进行广播</span></span><br><span class="line">    freqs_cos = reshape_for_broadcast(freqs_cos, xq_r)</span><br><span class="line">    freqs_sin = reshape_for_broadcast(freqs_sin, xq_r)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 应用旋转，分别计算旋转后的实部和虚部</span></span><br><span class="line">    xq_out_r = xq_r * freqs_cos - xq_i * freqs_sin</span><br><span class="line">    xq_out_i = xq_r * freqs_sin + xq_i * freqs_cos</span><br><span class="line">    xk_out_r = xk_r * freqs_cos - xk_i * freqs_sin</span><br><span class="line">    xk_out_i = xk_r * freqs_sin + xk_i * freqs_cos</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将最后两个维度合并，并还原为原始张量的形状</span></span><br><span class="line">    xq_out = torch.stack([xq_out_r, xq_out_i], dim=-<span class="number">1</span>).flatten(<span class="number">3</span>)</span><br><span class="line">    xk_out = torch.stack([xk_out_r, xk_out_i], dim=-<span class="number">1</span>).flatten(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> xq_out.type_as(xq), xk_out.type_as(xk)</span><br></pre></td></tr></table></figure>

<p>这里我们给出可以测试<code>apply_rotary_emb</code>函数的代码，大家也可以尝试在代码中添加断点，来查看每一步的计算结果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">xq = torch.randn(<span class="number">1</span>, <span class="number">50</span>, <span class="number">6</span>, <span class="number">48</span>) <span class="comment"># bs, seq_len, dim//n_head, n_head_dim</span></span><br><span class="line">xk = torch.randn(<span class="number">1</span>, <span class="number">50</span>, <span class="number">6</span>, <span class="number">48</span>) <span class="comment"># bs, seq_len, dim//n_head, n_head_dim</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 precompute_freqs_cis 函数获取 sin和cos</span></span><br><span class="line">cos, sin = precompute_freqs_cis(<span class="number">288</span>//<span class="number">6</span>, <span class="number">50</span>)</span><br><span class="line"><span class="built_in">print</span>(cos.shape, sin.shape)</span><br><span class="line">xq_out, xk_out = apply_rotary_emb(xq, xk, cos, sin)</span><br><span class="line"></span><br><span class="line">xq_out.shape, xk_out.shape</span><br></pre></td></tr></table></figure>

<p>OUT:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([50, 24]) torch.Size([50, 24])</span><br><span class="line"></span><br><span class="line">(torch.Size([1, 50, 6, 48]), torch.Size([1, 50, 6, 48]))</span><br></pre></td></tr></table></figure>

<h4 id="5-1-3-3-组装-LLaMA2-Attention"><a href="#5-1-3-3-组装-LLaMA2-Attention" class="headerlink" title="5.1.3.3 组装 LLaMA2 Attention"></a>5.1.3.3 组装 LLaMA2 Attention</h4><p>在上面我们已经完成了旋转嵌入的实现，接下来我们就可以构建 LLaMA2 Attention 模块了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Attention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, args: ModelConfig</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 根据是否指定n_kv_heads，确定用于键（key）和值（value）的头的数量。</span></span><br><span class="line">        <span class="variable language_">self</span>.n_kv_heads = args.n_heads <span class="keyword">if</span> args.n_kv_heads <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> args.n_kv_heads</span><br><span class="line">        <span class="comment"># 确保总头数可以被键值头数整除。</span></span><br><span class="line">        <span class="keyword">assert</span> args.n_heads % <span class="variable language_">self</span>.n_kv_heads == <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 模型并行处理大小，默认为1。</span></span><br><span class="line">        model_parallel_size = <span class="number">1</span></span><br><span class="line">        <span class="comment"># 本地计算头数，等于总头数除以模型并行处理大小。</span></span><br><span class="line">        <span class="variable language_">self</span>.n_local_heads = args.n_heads // model_parallel_size</span><br><span class="line">        <span class="comment"># 本地键值头数，等于键值头数除以模型并行处理大小。</span></span><br><span class="line">        <span class="variable language_">self</span>.n_local_kv_heads = <span class="variable language_">self</span>.n_kv_heads // model_parallel_size</span><br><span class="line">        <span class="comment"># 重复次数，用于扩展键和值的尺寸。</span></span><br><span class="line">        <span class="variable language_">self</span>.n_rep = <span class="variable language_">self</span>.n_local_heads // <span class="variable language_">self</span>.n_local_kv_heads</span><br><span class="line">        <span class="comment"># 每个头的维度，等于模型维度除以头的总数。</span></span><br><span class="line">        <span class="variable language_">self</span>.head_dim = args.dim // args.n_heads</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义权重矩阵。</span></span><br><span class="line">        <span class="variable language_">self</span>.wq = nn.Linear(args.dim, args.n_heads * <span class="variable language_">self</span>.head_dim, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.wk = nn.Linear(args.dim, <span class="variable language_">self</span>.n_kv_heads * <span class="variable language_">self</span>.head_dim, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.wv = nn.Linear(args.dim, <span class="variable language_">self</span>.n_kv_heads * <span class="variable language_">self</span>.head_dim, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># 输出权重矩阵。</span></span><br><span class="line">        <span class="variable language_">self</span>.wo = nn.Linear(args.n_heads * <span class="variable language_">self</span>.head_dim, args.dim, bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义dropout。</span></span><br><span class="line">        <span class="variable language_">self</span>.attn_dropout = nn.Dropout(args.dropout)</span><br><span class="line">        <span class="variable language_">self</span>.resid_dropout = nn.Dropout(args.dropout)</span><br><span class="line">        <span class="comment"># 保存dropout概率。</span></span><br><span class="line">        <span class="variable language_">self</span>.dropout = args.dropout</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 检查是否使用Flash Attention（需要PyTorch &gt;= 2.0）。</span></span><br><span class="line">        <span class="variable language_">self</span>.flash = <span class="built_in">hasattr</span>(torch.nn.functional, <span class="string">&#x27;scaled_dot_product_attention&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="variable language_">self</span>.flash:</span><br><span class="line">            <span class="comment"># 若不支持Flash Attention，则使用手动实现的注意力机制，并设置mask。</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;WARNING: using slow attention. Flash Attention requires PyTorch &gt;= 2.0&quot;</span>)</span><br><span class="line">            <span class="comment"># 创建一个上三角矩阵，用于遮蔽未来信息。</span></span><br><span class="line">            mask = torch.full((<span class="number">1</span>, <span class="number">1</span>, args.max_seq_len, args.max_seq_len), <span class="built_in">float</span>(<span class="string">&quot;-inf&quot;</span>))</span><br><span class="line">            mask = torch.triu(mask, diagonal=<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># 注册为模型的缓冲区</span></span><br><span class="line">            <span class="variable language_">self</span>.register_buffer(<span class="string">&quot;mask&quot;</span>, mask)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: torch.Tensor, freqs_cos: torch.Tensor, freqs_sin: torch.Tensor</span>):</span><br><span class="line">        <span class="comment"># 获取批次大小和序列长度，[batch_size, seq_len, dim]</span></span><br><span class="line">        bsz, seqlen, _ = x.shape</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算查询（Q）、键（K）、值（V）。</span></span><br><span class="line">        xq, xk, xv = <span class="variable language_">self</span>.wq(x), <span class="variable language_">self</span>.wk(x), <span class="variable language_">self</span>.wv(x)</span><br><span class="line">        <span class="comment"># 调整形状以适应头的维度。</span></span><br><span class="line">        xq = xq.view(bsz, seqlen, <span class="variable language_">self</span>.n_local_heads, <span class="variable language_">self</span>.head_dim)</span><br><span class="line">        xk = xk.view(bsz, seqlen, <span class="variable language_">self</span>.n_local_kv_heads, <span class="variable language_">self</span>.head_dim)</span><br><span class="line">        xv = xv.view(bsz, seqlen, <span class="variable language_">self</span>.n_local_kv_heads, <span class="variable language_">self</span>.head_dim)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 应用旋转位置嵌入（RoPE）。</span></span><br><span class="line">        xq, xk = apply_rotary_emb(xq, xk, freqs_cos, freqs_sin)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 对键和值进行扩展以适应重复次数。</span></span><br><span class="line">        xk = repeat_kv(xk, <span class="variable language_">self</span>.n_rep)</span><br><span class="line">        xv = repeat_kv(xv, <span class="variable language_">self</span>.n_rep)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将头作为批次维度处理。</span></span><br><span class="line">        xq = xq.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        xk = xk.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        xv = xv.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 根据是否支持Flash Attention，选择实现方式。</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.flash:</span><br><span class="line">            <span class="comment"># 使用Flash Attention。</span></span><br><span class="line">            output = torch.nn.functional.scaled_dot_product_attention(xq, xk, xv, attn_mask=<span class="literal">None</span>, dropout_p=<span class="variable language_">self</span>.dropout <span class="keyword">if</span> <span class="variable language_">self</span>.training <span class="keyword">else</span> <span class="number">0.0</span>, is_causal=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 使用手动实现的注意力机制。</span></span><br><span class="line">            scores = torch.matmul(xq, xk.transpose(<span class="number">2</span>, <span class="number">3</span>)) / math.sqrt(<span class="variable language_">self</span>.head_dim)</span><br><span class="line">            <span class="keyword">assert</span> <span class="built_in">hasattr</span>(<span class="variable language_">self</span>, <span class="string">&#x27;mask&#x27;</span>)</span><br><span class="line">            scores = scores + <span class="variable language_">self</span>.mask[:, :, :seqlen, :seqlen]</span><br><span class="line">            scores = F.softmax(scores.<span class="built_in">float</span>(), dim=-<span class="number">1</span>).type_as(xq)</span><br><span class="line">            scores = <span class="variable language_">self</span>.attn_dropout(scores)</span><br><span class="line">            output = torch.matmul(scores, xv)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 恢复时间维度并合并头。</span></span><br><span class="line">        output = output.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(bsz, seqlen, -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 最终投影回残差流。</span></span><br><span class="line">        output = <span class="variable language_">self</span>.wo(output)</span><br><span class="line">        output = <span class="variable language_">self</span>.resid_dropout(output)</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>

<p>同样大家可以使用下面的代码来对注意力模块进行测试，可以看到代码最终输出的形状为<code>torch.Size([1, 50, 768])</code>，与我们输入的形状一致，说明模块的实现是正确的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建Attention实例</span></span><br><span class="line">attention_model = Attention(args)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟输入数据</span></span><br><span class="line">batch_size = <span class="number">1</span></span><br><span class="line">seq_len = <span class="number">50</span>  <span class="comment"># 假设实际使用的序列长度为50</span></span><br><span class="line">dim = args.dim</span><br><span class="line">x = torch.rand(batch_size, seq_len, dim)  <span class="comment"># 随机生成输入张量</span></span><br><span class="line"><span class="comment"># freqs_cos = torch.rand(seq_len, dim // 2)  # 模拟cos频率，用于RoPE</span></span><br><span class="line"><span class="comment"># freqs_sin = torch.rand(seq_len, dim // 2)  # 模拟sin频率，用于RoPE</span></span><br><span class="line"></span><br><span class="line">freqs_cos, freqs_sin = precompute_freqs_cis(dim//args.n_heads, seq_len)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行Attention模型</span></span><br><span class="line">output = attention_model(x, freqs_cos, freqs_sin)</span><br><span class="line"></span><br><span class="line"><span class="comment"># attention出来之后的形状 依然是[batch_size, seq_len, dim]</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Output shape:&quot;</span>, output.shape)</span><br></pre></td></tr></table></figure>

<p>OUT:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Output shape: torch.Size([1, 50, 768])</span><br></pre></td></tr></table></figure>

<h3 id="5-1-4-构建-LLaMA2-MLP模块"><a href="#5-1-4-构建-LLaMA2-MLP模块" class="headerlink" title="5.1.4 构建 LLaMA2 MLP模块"></a>5.1.4 构建 LLaMA2 MLP模块</h3><p>相对于前面我们实现的LLaMA2 Attention模块，LLaMA2 MLP模块的实现要简单一些。我们可以通过如下代码实现<code>MLP</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MLP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim: <span class="built_in">int</span>, hidden_dim: <span class="built_in">int</span>, multiple_of: <span class="built_in">int</span>, dropout: <span class="built_in">float</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 如果没有指定隐藏层的维度，我们将其设置为输入维度的4倍</span></span><br><span class="line">        <span class="comment"># 然后将其减少到2/3，最后确保它是multiple_of的倍数</span></span><br><span class="line">        <span class="keyword">if</span> hidden_dim <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            hidden_dim = <span class="number">4</span> * dim</span><br><span class="line">            hidden_dim = <span class="built_in">int</span>(<span class="number">2</span> * hidden_dim / <span class="number">3</span>)</span><br><span class="line">            hidden_dim = multiple_of * ((hidden_dim + multiple_of - <span class="number">1</span>) // multiple_of)</span><br><span class="line">        <span class="comment"># 定义第一层线性变换，从输入维度到隐藏维度</span></span><br><span class="line">        <span class="variable language_">self</span>.w1 = nn.Linear(dim, hidden_dim, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># 定义第二层线性变换，从隐藏维度到输入维度</span></span><br><span class="line">        <span class="variable language_">self</span>.w2 = nn.Linear(hidden_dim, dim, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># 定义第三层线性变换，从输入维度到隐藏维度</span></span><br><span class="line">        <span class="variable language_">self</span>.w3 = nn.Linear(dim, hidden_dim, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># 定义dropout层，用于防止过拟合</span></span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 前向传播函数</span></span><br><span class="line">        <span class="comment"># 首先，输入x通过第一层线性变换和SILU激活函数</span></span><br><span class="line">        <span class="comment"># 然后，结果乘以输入x通过第三层线性变换的结果</span></span><br><span class="line">        <span class="comment"># 最后，通过第二层线性变换和dropout层</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.dropout(<span class="variable language_">self</span>.w2(F.silu(<span class="variable language_">self</span>.w1(x)) * <span class="variable language_">self</span>.w3(x)))</span><br></pre></td></tr></table></figure>

<p>我们着重观察一下<code>forward</code>函数的实现，首先，输入 <code>x</code> 通过第一层线性变换 <code>self.w1</code> 和 <code>SILU</code> 激活函数，然后，结果乘以输入 <code>x</code> 通过第三层线性变换 <code>self.w3</code> 的结果，最后，通过第二层线性变换 <code>self.w2</code> 和 <code>dropout</code> 层，得到最终输出。</p>
<p>同样大家可以使用下面的代码来对<code>LLaMAMLP</code>模块进行测试，可以看到代码最终输出的形状为<code>torch.Size([1, 50, 768])</code>，与我们输入的形状一致，说明模块的实现是正确的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建MLP实例</span></span><br><span class="line">mlp = MLP(args.dim, args.hidden_dim, args.multiple_of, args.dropout)</span><br><span class="line"><span class="comment"># 随机生成数据</span></span><br><span class="line">x = torch.randn(<span class="number">1</span>, <span class="number">50</span>, args.dim)</span><br><span class="line"><span class="comment"># 运行MLP模型</span></span><br><span class="line">output = mlp(x)</span><br><span class="line"><span class="built_in">print</span>(output.shape)</span><br></pre></td></tr></table></figure>

<p>OUT:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([1, 50, 768])</span><br></pre></td></tr></table></figure>

<h3 id="5-1-5-LLaMA2-Decoder-Layer"><a href="#5-1-5-LLaMA2-Decoder-Layer" class="headerlink" title="5.1.5 LLaMA2 Decoder Layer"></a>5.1.5 LLaMA2 Decoder Layer</h3><p>到这里，我们已经实现了<code>LLaMA2</code>模型的<code>Attention</code>模块和<code>MLP</code>模块，接下来我们就可以构建<code>LLaMA2</code>的<code>Decoder Layer</code>了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DecoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, layer_id: <span class="built_in">int</span>, args: ModelConfig</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 定义多头注意力的头数</span></span><br><span class="line">        <span class="variable language_">self</span>.n_heads = args.n_heads</span><br><span class="line">        <span class="comment"># 定义输入维度</span></span><br><span class="line">        <span class="variable language_">self</span>.dim = args.dim</span><br><span class="line">        <span class="comment"># 定义每个头的维度，等于输入维度除以头数</span></span><br><span class="line">        <span class="variable language_">self</span>.head_dim = args.dim // args.n_heads</span><br><span class="line">        <span class="comment"># 定义LLaMA2Attention对象，用于进行多头注意力计算</span></span><br><span class="line">        <span class="variable language_">self</span>.attention = Attention(args)</span><br><span class="line">        <span class="comment"># 定义LLaMAMLP对象，用于进行前馈神经网络计算</span></span><br><span class="line">        <span class="variable language_">self</span>.feed_forward = MLP(</span><br><span class="line">            dim=args.dim,</span><br><span class="line">            hidden_dim=args.hidden_dim,</span><br><span class="line">            multiple_of=args.multiple_of,</span><br><span class="line">            dropout=args.dropout,</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 定义层的ID</span></span><br><span class="line">        <span class="variable language_">self</span>.layer_id = layer_id</span><br><span class="line">        <span class="comment"># 定义注意力计算的归一化层</span></span><br><span class="line">        <span class="variable language_">self</span>.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)</span><br><span class="line">        <span class="comment"># 定义前馈神经网络计算的归一化层</span></span><br><span class="line">        <span class="variable language_">self</span>.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, freqs_cos, freqs_sin</span>):</span><br><span class="line">        <span class="comment"># 前向传播函数</span></span><br><span class="line">        <span class="comment"># 首先，输入x经过注意力归一化层，然后进行注意力计算，结果与输入x相加得到h</span></span><br><span class="line">        <span class="comment"># 然后，h经过前馈神经网络归一化层，然后进行前馈神经网络计算，结果与h相加得到输出</span></span><br><span class="line">        h = x + <span class="variable language_">self</span>.attention.forward(<span class="variable language_">self</span>.attention_norm(x), freqs_cos, freqs_sin)</span><br><span class="line">        out = h + <span class="variable language_">self</span>.feed_forward.forward(<span class="variable language_">self</span>.ffn_norm(h))</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>

<p><code>DecoderLayer</code>就是把我们上面完成的<code>Attention</code>模块和<code>MLP</code>模块组合在一起，实现了一个完整的<code>Transformer</code>模块。</p>
<p>同样大家可以使用下面的代码来对<code>DecoderLayer</code>模块进行测试，可以看到代码最终输出的形状为<code>torch.Size([1, 50, 768])</code>，与我们输入的形状一致，说明模块的实现是正确的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建LLaMADecoderLayer实例</span></span><br><span class="line">decoderlayer = DecoderLayer(<span class="number">0</span>, args)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟输入数据</span></span><br><span class="line">dim = args.dim</span><br><span class="line">seq_len = <span class="number">50</span></span><br><span class="line"></span><br><span class="line">x = torch.randn(<span class="number">1</span>, seq_len, dim) <span class="comment"># [bs, seq_len, dim]</span></span><br><span class="line"></span><br><span class="line">freqs_cos, freqs_sin = precompute_freqs_cis(dim//args.n_heads, seq_len)</span><br><span class="line"></span><br><span class="line">out = decoderlayer(x, freqs_cos, freqs_sin)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(out.shape) <span class="comment"># 形状和输入的x一样 [batch_size, seq_len, dim]</span></span><br></pre></td></tr></table></figure>

<p>OUT:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([1, 50, 768])</span><br></pre></td></tr></table></figure>

<h3 id="5-1-6-构建-LLaMA2-模型"><a href="#5-1-6-构建-LLaMA2-模型" class="headerlink" title="5.1.6 构建 LLaMA2 模型"></a>5.1.6 构建 LLaMA2 模型</h3><p>好了，我们已经完了上述所有的模块的实现，接下来就是激动人心的时刻，我们可以构建<code>LLaMA2</code>模型了。，<code>LLaMA2</code>模型就是将<code>DecoderLayer</code>模块堆叠起来，构成一个完整的<code>Transformer</code>模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Transformer</span>(<span class="title class_ inherited__">PreTrainedModel</span>):</span><br><span class="line">    config_class = ModelConfig  <span class="comment"># 配置类</span></span><br><span class="line">    last_loss: <span class="type">Optional</span>[torch.Tensor] <span class="comment"># 记录最后一次计算的损失</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, args: ModelConfig = <span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(args)</span><br><span class="line">        <span class="comment"># 初始化模型参数</span></span><br><span class="line">        <span class="variable language_">self</span>.args = args</span><br><span class="line">        <span class="comment"># 词汇表大小</span></span><br><span class="line">        <span class="variable language_">self</span>.vocab_size = args.vocab_size</span><br><span class="line">        <span class="comment"># 层数</span></span><br><span class="line">        <span class="variable language_">self</span>.n_layers = args.n_layers</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 词嵌入层</span></span><br><span class="line">        <span class="variable language_">self</span>.tok_embeddings = nn.Embedding(args.vocab_size, args.dim)</span><br><span class="line">        <span class="comment"># Dropout层</span></span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(args.dropout)</span><br><span class="line">        <span class="comment"># Decoder层</span></span><br><span class="line">        <span class="variable language_">self</span>.layers = torch.nn.ModuleList()</span><br><span class="line">        <span class="keyword">for</span> layer_id <span class="keyword">in</span> <span class="built_in">range</span>(args.n_layers):</span><br><span class="line">            <span class="variable language_">self</span>.layers.append(DecoderLayer(layer_id, args))</span><br><span class="line">        <span class="comment"># 归一化层</span></span><br><span class="line">        <span class="variable language_">self</span>.norm = RMSNorm(args.dim, eps=args.norm_eps)</span><br><span class="line">        <span class="comment"># 输出层</span></span><br><span class="line">        <span class="variable language_">self</span>.output = nn.Linear(args.dim, args.vocab_size, bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将词嵌入层的权重与输出层的权重共享</span></span><br><span class="line">        <span class="variable language_">self</span>.tok_embeddings.weight = <span class="variable language_">self</span>.output.weight </span><br><span class="line"></span><br><span class="line">        <span class="comment"># 预计算相对位置嵌入的频率</span></span><br><span class="line">        freqs_cos, freqs_sin = precompute_freqs_cis(<span class="variable language_">self</span>.args.dim // <span class="variable language_">self</span>.args.n_heads, <span class="variable language_">self</span>.args.max_seq_len)</span><br><span class="line">        <span class="variable language_">self</span>.register_buffer(<span class="string">&quot;freqs_cos&quot;</span>, freqs_cos, persistent=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.register_buffer(<span class="string">&quot;freqs_sin&quot;</span>, freqs_sin, persistent=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化所有权重</span></span><br><span class="line">        <span class="variable language_">self</span>.apply(<span class="variable language_">self</span>._init_weights)</span><br><span class="line">        <span class="comment"># 对残差投影进行特殊的缩放初始化</span></span><br><span class="line">        <span class="keyword">for</span> pn, p <span class="keyword">in</span> <span class="variable language_">self</span>.named_parameters():</span><br><span class="line">            <span class="keyword">if</span> pn.endswith(<span class="string">&#x27;w3.weight&#x27;</span>) <span class="keyword">or</span> pn.endswith(<span class="string">&#x27;wo.weight&#x27;</span>):</span><br><span class="line">                torch.nn.init.normal_(p, mean=<span class="number">0.0</span>, std=<span class="number">0.02</span>/math.sqrt(<span class="number">2</span> * args.n_layers))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化最后一次前向传播的损失属性</span></span><br><span class="line">        <span class="variable language_">self</span>.last_loss = <span class="literal">None</span></span><br><span class="line">        <span class="variable language_">self</span>.OUT = CausalLMOutputWithPast()  <span class="comment"># 输出容器</span></span><br><span class="line">        <span class="variable language_">self</span>._no_split_modules = [name <span class="keyword">for</span> name, _ <span class="keyword">in</span> <span class="variable language_">self</span>.named_modules()]  <span class="comment"># 不分割的模块列表</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_init_weights</span>(<span class="params">self, module</span>):</span><br><span class="line">        <span class="comment"># 初始化权重的函数</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(module, nn.Linear):</span><br><span class="line">            torch.nn.init.normal_(module.weight, mean=<span class="number">0.0</span>, std=<span class="number">0.02</span>)</span><br><span class="line">            <span class="keyword">if</span> module.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                torch.nn.init.zeros_(module.bias)</span><br><span class="line">        <span class="keyword">elif</span> <span class="built_in">isinstance</span>(module, nn.Embedding):</span><br><span class="line">            torch.nn.init.normal_(module.weight, mean=<span class="number">0.0</span>, std=<span class="number">0.02</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, tokens: torch.Tensor, targets: <span class="type">Optional</span>[torch.Tensor] = <span class="literal">None</span>, **keyargs</span>) -&gt; torch.Tensor:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        - tokens: Optional[torch.Tensor], 输入 token 张量。</span></span><br><span class="line"><span class="string">        - targets: Optional[torch.Tensor], 目标 token 张量。</span></span><br><span class="line"><span class="string">        - kv_cache: bool, 是否使用键值缓存。</span></span><br><span class="line"><span class="string">        - keyargs: 其他关键字参数。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        - self.OUT: CausalLMOutputWithPast, 包含 logits 和损失。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="string">&#x27;input_ids&#x27;</span> <span class="keyword">in</span> keyargs:</span><br><span class="line">            tokens = keyargs[<span class="string">&#x27;input_ids&#x27;</span>]</span><br><span class="line">        <span class="keyword">if</span> <span class="string">&#x27;attention_mask&#x27;</span> <span class="keyword">in</span> keyargs:</span><br><span class="line">            targets = keyargs[<span class="string">&#x27;attention_mask&#x27;</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 前向传播函数</span></span><br><span class="line">        _bsz, seqlen = tokens.shape</span><br><span class="line">        <span class="comment"># 通过词嵌入层和Dropout层</span></span><br><span class="line">        h = <span class="variable language_">self</span>.tok_embeddings(tokens)</span><br><span class="line">        h = <span class="variable language_">self</span>.dropout(h)</span><br><span class="line">        <span class="comment"># 获取相对位置嵌入的频率</span></span><br><span class="line">        freqs_cos = <span class="variable language_">self</span>.freqs_cos[:seqlen]</span><br><span class="line">        freqs_sin = <span class="variable language_">self</span>.freqs_sin[:seqlen]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 通过Decoder层</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="variable language_">self</span>.layers:</span><br><span class="line">            h = layer(h, freqs_cos, freqs_sin)</span><br><span class="line">        <span class="comment"># 通过归一化层</span></span><br><span class="line">        h = <span class="variable language_">self</span>.norm(h)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> targets <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 如果给定了目标，计算损失</span></span><br><span class="line">            logits = <span class="variable language_">self</span>.output(h)</span><br><span class="line">            <span class="variable language_">self</span>.last_loss = F.cross_entropy(logits.view(-<span class="number">1</span>, logits.size(-<span class="number">1</span>)), targets.view(-<span class="number">1</span>), ignore_index=<span class="number">0</span>, reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 推理时的小优化：只对最后一个位置的输出进行前向传播</span></span><br><span class="line">            logits = <span class="variable language_">self</span>.output(h[:, [-<span class="number">1</span>], :]) </span><br><span class="line">            <span class="variable language_">self</span>.last_loss = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 设置输出</span></span><br><span class="line">        <span class="variable language_">self</span>.OUT.__setitem__(<span class="string">&#x27;logits&#x27;</span>, logits)</span><br><span class="line">        <span class="variable language_">self</span>.OUT.__setitem__(<span class="string">&#x27;last_loss&#x27;</span>, <span class="variable language_">self</span>.last_loss)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.OUT</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"><span class="meta">    @torch.inference_mode()</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">generate</span>(<span class="params">self, idx, stop_id=<span class="literal">None</span>, max_new_tokens=<span class="number">256</span>, temperature=<span class="number">1.0</span>, top_k=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        给定输入序列 idx（形状为 (bz,seq_len) 的长整型张量），通过多次生成新 token 来完成序列。</span></span><br><span class="line"><span class="string">        在 model.eval() 模式下运行。效率较低的采样版本，没有使用键k/v cache。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        index = idx.shape[<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(max_new_tokens):</span><br><span class="line">            <span class="comment"># 如果序列上下文过长，截断它到最大长度</span></span><br><span class="line">            idx_cond = idx <span class="keyword">if</span> idx.size(<span class="number">1</span>) &lt;= <span class="variable language_">self</span>.args.max_seq_len <span class="keyword">else</span> idx[:, -<span class="variable language_">self</span>.args.max_seq_len:]</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 前向传播获取序列中最后一个位置的 logits</span></span><br><span class="line">            logits = <span class="variable language_">self</span>(idx_cond).logits</span><br><span class="line">            logits = logits[:, -<span class="number">1</span>, :] <span class="comment"># 只保留最后一个时间步的输出</span></span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> temperature == <span class="number">0.0</span>:</span><br><span class="line">                <span class="comment"># 选择最有可能的索引</span></span><br><span class="line">                _, idx_next = torch.topk(logits, k=<span class="number">1</span>, dim=-<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># 缩放 logits 并应用 softmax</span></span><br><span class="line">                logits = logits / temperature</span><br><span class="line">                <span class="keyword">if</span> top_k <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    v, _ = torch.topk(logits, <span class="built_in">min</span>(top_k, logits.size(-<span class="number">1</span>)))</span><br><span class="line">                    logits[logits &lt; v[:, [-<span class="number">1</span>]]] = -<span class="built_in">float</span>(<span class="string">&#x27;Inf&#x27;</span>)</span><br><span class="line">                probs = F.softmax(logits, dim=-<span class="number">1</span>)</span><br><span class="line">                idx_next = torch.multinomial(probs, num_samples=<span class="number">1</span>)</span><br><span class="line">            </span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> idx_next == stop_id:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 将采样的索引添加到序列中并继续</span></span><br><span class="line">            idx = torch.cat((idx, idx_next), dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> idx[:, index:] <span class="comment"># 只返回生成的token</span></span><br></pre></td></tr></table></figure>

<p>同样大家可以使用下面的代码来对<code>Transformer</code>模块进行测试，可以看到代码最终输出的形状为<code>torch.Size([1, 1, 6144])</code>，与我们输入的形状一致，说明模块的实现是正确的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># LLaMA2Model.forward 接受两个参数，tokens和targets，其中tokens是输入的张量, 应为int类型</span></span><br><span class="line">x = torch.randint(<span class="number">0</span>, <span class="number">6144</span>, (<span class="number">1</span>, <span class="number">50</span>)) <span class="comment"># [bs, seq_len]</span></span><br><span class="line"><span class="comment"># 实例化LLaMA2Model</span></span><br><span class="line">model = Transformer(args=args)</span><br><span class="line"><span class="comment"># 计算model的全部参数</span></span><br><span class="line">num_params = <span class="built_in">sum</span>(p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Number of parameters:&#x27;</span>, num_params)</span><br><span class="line"></span><br><span class="line">out = model(x)</span><br><span class="line"><span class="built_in">print</span>(out.logits.shape) <span class="comment"># [batch_size, 1, vocab_size]</span></span><br></pre></td></tr></table></figure>

<p>OUT:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Number of parameters: 82594560</span><br><span class="line">torch.Size([1, 1, 6144])</span><br></pre></td></tr></table></figure>

<h2 id="5-2-训练-Tokenizer"><a href="#5-2-训练-Tokenizer" class="headerlink" title="5.2 训练 Tokenizer"></a>5.2 训练 Tokenizer</h2><p>在自然语言处理 (NLP) 中，Tokenizer 是一种将文本分解为较小单位（称为 token）的工具。这些 token 可以是词、子词、字符，甚至是特定的符号。Tokenization 是 NLP 中的第一步，直接影响后续处理和分析的效果。不同类型的 tokenizer 适用于不同的应用场景，以下是几种常见的 tokenizer 及其特点。</p>
<h3 id="5-3-1-Word-based-Tokenizer"><a href="#5-3-1-Word-based-Tokenizer" class="headerlink" title="5.3.1 Word-based Tokenizer"></a>5.3.1 Word-based Tokenizer</h3><p><strong>Word-based Tokenizer</strong> 是最简单和直观的一种分词方法。它将文本按空格和标点符号分割成单词。这种方法的优点在于其简单和直接，易于实现，且与人类对语言的直觉相符。然而，它也存在一些明显的缺点，如无法处理未登录词（OOV，out-of-vocabulary）和罕见词，对复合词（如“New York”）或缩略词（如“don’t”）的处理也不够精细。此外，Word-based Tokenizer 在处理不同语言时也会遇到挑战，因为一些语言（如中文、日文）没有显式的单词分隔符。</p>
<p>示例：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Input: &quot;Hello, world! There is Datawhale.&quot;</span><br><span class="line">Output: [&quot;Hello&quot;, &quot;,&quot;, &quot;world&quot;, &quot;!&quot;, &quot;There&quot;, &quot;is&quot;, &quot;Datawhale&quot;, &quot;.&quot;]</span><br></pre></td></tr></table></figure>

<p>在这个例子中，输入的句子被分割成一系列单词和标点符号，每个单词或标点符号都作为一个独立的 token。</p>
<h3 id="5-2-2-Character-based-Tokenizer"><a href="#5-2-2-Character-based-Tokenizer" class="headerlink" title="5.2.2 Character-based Tokenizer"></a>5.2.2 Character-based Tokenizer</h3><p><strong>Character-based Tokenizer</strong> 将文本中的每个字符视为一个独立的 token。这种方法能非常精细地处理文本，适用于处理拼写错误、未登录词或新词。由于每个字符都是一个独立的 token，因此这种方法可以捕捉到非常细微的语言特征。这对于一些特定的应用场景，如生成式任务或需要处理大量未登录词的任务，特别有用。但是，这种方法也会导致 token 序列变得非常长，增加了模型的计算复杂度和训练时间。此外，字符级的分割可能会丢失一些词级别的语义信息，使得模型难以理解上下文。</p>
<p>示例：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Input: &quot;Hello&quot;</span><br><span class="line">Output: [&quot;H&quot;, &quot;e&quot;, &quot;l&quot;, &quot;l&quot;, &quot;o&quot;]</span><br></pre></td></tr></table></figure>

<p>在这个例子中，单词“Hello”被分割成单个字符，每个字符作为一个独立的 token。这种方法能够处理任何语言和字符集，具有极大的灵活性。</p>
<h3 id="5-2-3-Subword-Tokenizer"><a href="#5-2-3-Subword-Tokenizer" class="headerlink" title="5.2.3 Subword Tokenizer"></a>5.2.3 Subword Tokenizer</h3><p><strong>Subword Tokenizer</strong> 介于词和字符之间，能够更好地平衡分词的细粒度和处理未登录词的能力。Subword Tokenizer 的关键思想是将文本分割成比单词更小的单位，但又比字符更大，这样既能处理未知词，又能保持一定的语义信息。常见的子词分词方法包括 BPE、WordPiece 和 Unigram。</p>
<h4 id="（1）Byte-Pair-Encoding-BPE"><a href="#（1）Byte-Pair-Encoding-BPE" class="headerlink" title="（1）Byte Pair Encoding (BPE)"></a>（1）Byte Pair Encoding (BPE)</h4><p><strong>BPE</strong> 是一种基于统计方法，通过反复合并频率最高的字符或字符序列对来生成子词词典。这种方法的优点在于其简单和高效，能够有效地处理未知词和罕见词，同时保持较低的词典大小。BPE 的合并过程是自底向上的，逐步将频率最高的字符对合并成新的子词，直到达到预定的词典大小或不再有高频的字符对。</p>
<p>示例：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Input: &quot;lower&quot;</span><br><span class="line">Output: [&quot;low&quot;, &quot;er&quot;]</span><br><span class="line"></span><br><span class="line">Input: &quot;newest&quot;</span><br><span class="line">Output: [&quot;new&quot;, &quot;est&quot;]</span><br></pre></td></tr></table></figure>

<p>在这个例子中，单词“lower”被分割成子词“low”和“er”，而“newest”被分割成“new”和“est”。这种方法有效地处理了词干和词缀，保持了单词的基本语义结构。</p>
<h4 id="（2）WordPiece"><a href="#（2）WordPiece" class="headerlink" title="（2）WordPiece"></a>（2）WordPiece</h4><p><strong>WordPiece</strong> 是另一种基于子词的分词方法，最初用于谷歌的 BERT 模型。与 BPE 类似，WordPiece 通过最大化子词序列的似然函数来生成词典，但在合并子词时更注重语言模型的优化。WordPiece 会优先选择能够最大化整体句子概率的子词，使得分词结果在语言模型中具有更高的概率。</p>
<p>示例：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Input: &quot;unhappiness&quot;</span><br><span class="line">Output: [&quot;un&quot;, &quot;##happiness&quot;]</span><br></pre></td></tr></table></figure>

<p>在这个例子中，单词“unhappiness”被分割成子词“un”和“##happiness”，其中“##”表示这是一个后缀子词。通过这种方式，WordPiece 能够更好地处理复合词和派生词，保留更多的语义信息。</p>
<h4 id="（3）Unigram"><a href="#（3）Unigram" class="headerlink" title="（3）Unigram"></a>（3）Unigram</h4><p><strong>Unigram</strong> 分词方法基于概率模型，通过选择具有最高概率的子词来分割文本。Unigram 词典是通过训练语言模型生成的，可以处理多种语言和不同类型的文本。Unigram 模型会为每个子词分配一个概率，然后根据这些概率进行最优分割。</p>
<p>示例：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Input: &quot;unhappiness&quot;</span><br><span class="line">Output: [&quot;un&quot;, &quot;happiness&quot;]</span><br><span class="line"></span><br><span class="line">Input: &quot;newest&quot;</span><br><span class="line">Output: [&quot;new&quot;, &quot;est&quot;]</span><br></pre></td></tr></table></figure>

<p>在这个例子中，单词“unhappiness”被分割成子词“un”和“happiness”，而“newest”被分割成“new”和“est”。这种方法通过概率模型有效地处理了子词分割，使得分割结果更符合语言使用习惯。</p>
<p>每种 Tokenizer 方法都有其特定的应用场景和优缺点，选择适合的 Tokenizer 对于自然语言处理任务的成功至关重要。</p>
<h3 id="5-2-4-训练一个-Tokenizer"><a href="#5-2-4-训练一个-Tokenizer" class="headerlink" title="5.2.4 训练一个 Tokenizer"></a>5.2.4 训练一个 Tokenizer</h3><p>这里我们选择使用 BPE 算法来训练一个 Subword Tokenizer。BPE 是一种简单而有效的分词方法，能够处理未登录词和罕见词，同时保持较小的词典大小。我们将使用 Hugging Face 的 <code>tokenizers</code> 库来训练一个 BPE Tokenizer。</p>
<h4 id="Step-1-安装和导入依赖库"><a href="#Step-1-安装和导入依赖库" class="headerlink" title="Step 1: 安装和导入依赖库"></a>Step 1: 安装和导入依赖库</h4><p>首先，我们需要安装 <code>tokenizers</code> 库，除此之外还需要安装 <code>datasets</code> 和 <code>transformers</code> 库，用于加载训练数据和加载训练完成后的 Tokenizer。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install tokenizers datasets transformers</span><br></pre></td></tr></table></figure>

<p>然后，导入所需的库。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, PreTrainedTokenizerFast</span><br><span class="line"><span class="keyword">from</span> tokenizers <span class="keyword">import</span> (</span><br><span class="line">    decoders,</span><br><span class="line">    models,</span><br><span class="line">    pre_tokenizers,</span><br><span class="line">    trainers,</span><br><span class="line">    Tokenizer,</span><br><span class="line">)</span><br><span class="line"><span class="keyword">from</span> tokenizers.normalizers <span class="keyword">import</span> NFKC</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> Generator</span><br></pre></td></tr></table></figure>

<h4 id="Step-2-加载训练数据"><a href="#Step-2-加载训练数据" class="headerlink" title="Step 2: 加载训练数据"></a>Step 2: 加载训练数据</h4><p>我们使用 <code>datasets.load_dataset()</code> 库加载一个英文文本数据集，用于训练 BPE Tokenizer。这里我们使用 <code>wikitext</code> 数据集，包含了维基百科的文章文本。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">dataset = load_dataset(<span class="string">&quot;wikitext&quot;</span>, <span class="string">&quot;wikitext-103-v1&quot;</span>, split=<span class="string">&quot;train&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备训练数据</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">batch_iterator</span>(<span class="params">batch_size=<span class="number">1000</span></span>):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(dataset), batch_size):</span><br><span class="line">        <span class="keyword">yield</span> dataset[i:i + batch_size][<span class="string">&quot;text&quot;</span>]</span><br></pre></td></tr></table></figure>

<p>如果你使用本地的文本数据集，可以将数据加载到一个列表中，然后传入 <code>batch_iterator()</code> 函数中。如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_text_from_files</span>(<span class="params">path_list</span>):</span><br><span class="line">    text_data = []</span><br><span class="line">    <span class="keyword">for</span> file_path <span class="keyword">in</span> path_list:</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(file_path, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> file:</span><br><span class="line">            text_data.extend(file.readlines())</span><br><span class="line">    <span class="keyword">return</span> text_data</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">batch_iterator</span>(<span class="params">text_data, batch_size=<span class="number">1000</span></span>):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(text_data), batch_size):</span><br><span class="line">        <span class="keyword">yield</span> text_data[i:i + batch_size]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设你的文件路径列表是</span></span><br><span class="line">path_list = [<span class="string">&#x27;text_data1.txt&#x27;</span>, <span class="string">&#x27;text_data2.txt&#x27;</span>, <span class="string">&#x27;text_data3.txt&#x27;</span>]</span><br><span class="line">text_data = load_text_from_files(path_list)</span><br></pre></td></tr></table></figure>

<h4 id="Step-3-创建配置文件"><a href="#Step-3-创建配置文件" class="headerlink" title="Step 3: 创建配置文件"></a>Step 3: 创建配置文件</h4><p>在训练 BPE Tokenizer 之前，我们需要创建一个完整的 <code>Tokenizer</code> 配置文件，包括 <code>tokenizer_config.json</code> 和 <code>special_tokens_map.json</code>。这些配置文件定义了 <code>Tokenizer</code> 的参数和特殊标记，用于训练和加载 <code>Tokenizer</code>。此处的<code>chat_template</code>我们与<code>Qwen2.5</code>模型保持一致。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">create_tokenizer_config</span>(<span class="params">save_dir: <span class="built_in">str</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;创建完整的tokenizer配置文件&quot;&quot;&quot;</span></span><br><span class="line">    config = &#123;</span><br><span class="line">        <span class="string">&quot;add_bos_token&quot;</span>: <span class="literal">False</span>,</span><br><span class="line">        <span class="string">&quot;add_eos_token&quot;</span>: <span class="literal">False</span>,</span><br><span class="line">        <span class="string">&quot;add_prefix_space&quot;</span>: <span class="literal">False</span>,</span><br><span class="line">        <span class="string">&quot;bos_token&quot;</span>: <span class="string">&quot;&lt;|im_start|&gt;&quot;</span>,</span><br><span class="line">        <span class="string">&quot;eos_token&quot;</span>: <span class="string">&quot;&lt;|im_end|&gt;&quot;</span>,</span><br><span class="line">        <span class="string">&quot;pad_token&quot;</span>: <span class="string">&quot;&lt;|im_end|&gt;&quot;</span>,</span><br><span class="line">        <span class="string">&quot;unk_token&quot;</span>: <span class="string">&quot;&lt;unk&gt;&quot;</span>,</span><br><span class="line">        <span class="string">&quot;model_max_length&quot;</span>: <span class="number">1000000000000000019884624838656</span>,</span><br><span class="line">        <span class="string">&quot;clean_up_tokenization_spaces&quot;</span>: <span class="literal">False</span>,</span><br><span class="line">        <span class="string">&quot;tokenizer_class&quot;</span>: <span class="string">&quot;PreTrainedTokenizerFast&quot;</span>,</span><br><span class="line">        <span class="string">&quot;chat_template&quot;</span>: (</span><br><span class="line">            <span class="string">&quot;&#123;% for message in messages %&#125;&quot;</span></span><br><span class="line">            <span class="string">&quot;&#123;% if message[&#x27;role&#x27;] == &#x27;system&#x27; %&#125;&quot;</span></span><br><span class="line">            <span class="string">&quot;&lt;|im_start|&gt;system\n&#123;&#123; message[&#x27;content&#x27;] &#125;&#125;&lt;|im_end|&gt;\n&quot;</span></span><br><span class="line">            <span class="string">&quot;&#123;% elif message[&#x27;role&#x27;] == &#x27;user&#x27; %&#125;&quot;</span></span><br><span class="line">            <span class="string">&quot;&lt;|im_start|&gt;user\n&#123;&#123; message[&#x27;content&#x27;] &#125;&#125;&lt;|im_end|&gt;\n&quot;</span></span><br><span class="line">            <span class="string">&quot;&#123;% elif message[&#x27;role&#x27;] == &#x27;assistant&#x27; %&#125;&quot;</span></span><br><span class="line">            <span class="string">&quot;&lt;|im_start|&gt;assistant\n&#123;&#123; message[&#x27;content&#x27;] &#125;&#125;&lt;|im_end|&gt;\n&quot;</span></span><br><span class="line">            <span class="string">&quot;&#123;% endif %&#125;&quot;</span></span><br><span class="line">            <span class="string">&quot;&#123;% endfor %&#125;&quot;</span></span><br><span class="line">            <span class="string">&quot;&#123;% if add_generation_prompt %&#125;&quot;</span></span><br><span class="line">            <span class="string">&quot;&#123;&#123; &#x27;&lt;|im_start|&gt;assistant\n&#x27; &#125;&#125;&quot;</span></span><br><span class="line">            <span class="string">&quot;&#123;% endif %&#125;&quot;</span></span><br><span class="line">        )</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 保存主配置文件</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(save_dir, <span class="string">&quot;tokenizer_config.json&quot;</span>), <span class="string">&quot;w&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        json.dump(config, f, ensure_ascii=<span class="literal">False</span>, indent=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建special_tokens_map.json</span></span><br><span class="line">    special_tokens_map = &#123;</span><br><span class="line">        <span class="string">&quot;bos_token&quot;</span>: <span class="string">&quot;&lt;|im_start|&gt;&quot;</span>,</span><br><span class="line">        <span class="string">&quot;eos_token&quot;</span>: <span class="string">&quot;&lt;|im_end|&gt;&quot;</span>,</span><br><span class="line">        <span class="string">&quot;unk_token&quot;</span>: <span class="string">&quot;&lt;unk&gt;&quot;</span>,</span><br><span class="line">        <span class="string">&quot;pad_token&quot;</span>: <span class="string">&quot;&lt;|im_end|&gt;&quot;</span>,</span><br><span class="line">        <span class="string">&quot;additional_special_tokens&quot;</span>: [<span class="string">&quot;&lt;s&gt;&quot;</span>, <span class="string">&quot;&lt;/s&gt;&quot;</span>]</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(save_dir, <span class="string">&quot;special_tokens_map.json&quot;</span>), <span class="string">&quot;w&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        json.dump(special_tokens_map, f, ensure_ascii=<span class="literal">False</span>, indent=<span class="number">4</span>)</span><br></pre></td></tr></table></figure>

<h4 id="Step-4-训练-BPE-Tokenizer"><a href="#Step-4-训练-BPE-Tokenizer" class="headerlink" title="Step 4: 训练 BPE Tokenizer"></a>Step 4: 训练 BPE Tokenizer</h4><p>在训练 BPE Tokenizer 之前，我们需要定义一个训练函数，用于训练 Tokenizer 并保存训练好的 Tokenizer 文件。这里我们使用 <code>tokenizers</code> 库中的 <code>Tokenizer</code> 类来训练 BPE Tokenizer。</p>
<p>可以看到我们在训练 Tokenizer 时，配置了一些特殊的 token，如 <code>&lt;unk&gt;</code>、<code>&lt;s&gt;</code>、<code>&lt;/s&gt;</code>、<code>&lt;|im_start|&gt;</code> 和 <code>&lt;|im_end|&gt;</code>。这些 token 用于标记未知词、句子的开始和结束，以及对话的开始和结束。这些特殊 token 可以帮助模型更好地理解文本数据，提高模型的泛化能力和效果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_tokenizer</span>(<span class="params">data_path: <span class="built_in">str</span>, save_dir: <span class="built_in">str</span>, vocab_size: <span class="built_in">int</span> = <span class="number">8192</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练并保存自定义tokenizer&quot;&quot;&quot;</span></span><br><span class="line">    os.makedirs(save_dir, exist_ok=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 初始化tokenizer</span></span><br><span class="line">    tokenizer = Tokenizer(models.BPE(unk_token=<span class="string">&quot;&lt;unk&gt;&quot;</span>))</span><br><span class="line">    tokenizer.normalizer = NFKC()  <span class="comment"># 添加文本规范化</span></span><br><span class="line">    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=<span class="literal">False</span>)</span><br><span class="line">    tokenizer.decoder = decoders.ByteLevel()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 配置特殊token</span></span><br><span class="line">    special_tokens = [</span><br><span class="line">        <span class="string">&quot;&lt;unk&gt;&quot;</span>, </span><br><span class="line">        <span class="string">&quot;&lt;s&gt;&quot;</span>, </span><br><span class="line">        <span class="string">&quot;&lt;/s&gt;&quot;</span>, </span><br><span class="line">        <span class="string">&quot;&lt;|im_start|&gt;&quot;</span>, </span><br><span class="line">        <span class="string">&quot;&lt;|im_end|&gt;&quot;</span></span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 配置训练器</span></span><br><span class="line">    trainer = trainers.BpeTrainer(</span><br><span class="line">        vocab_size=vocab_size,</span><br><span class="line">        special_tokens=special_tokens,</span><br><span class="line">        min_frequency=<span class="number">2</span>,  <span class="comment"># 提高低频词过滤</span></span><br><span class="line">        show_progress=<span class="literal">True</span>,</span><br><span class="line">        initial_alphabet=pre_tokenizers.ByteLevel.alphabet()</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练tokenizer</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Training tokenizer with data from <span class="subst">&#123;data_path&#125;</span>&quot;</span>)</span><br><span class="line">    texts = read_texts_from_jsonl(data_path)</span><br><span class="line">    tokenizer.train_from_iterator(texts, trainer=trainer, length=os.path.getsize(data_path))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 验证特殊token映射</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">assert</span> tokenizer.token_to_id(<span class="string">&quot;&lt;unk&gt;&quot;</span>) == <span class="number">0</span></span><br><span class="line">        <span class="keyword">assert</span> tokenizer.token_to_id(<span class="string">&quot;&lt;s&gt;&quot;</span>) == <span class="number">1</span></span><br><span class="line">        <span class="keyword">assert</span> tokenizer.token_to_id(<span class="string">&quot;&lt;/s&gt;&quot;</span>) == <span class="number">2</span></span><br><span class="line">        <span class="keyword">assert</span> tokenizer.token_to_id(<span class="string">&quot;&lt;|im_start|&gt;&quot;</span>) == <span class="number">3</span></span><br><span class="line">        <span class="keyword">assert</span> tokenizer.token_to_id(<span class="string">&quot;&lt;|im_end|&gt;&quot;</span>) == <span class="number">4</span></span><br><span class="line">    <span class="keyword">except</span> AssertionError <span class="keyword">as</span> e:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Special tokens mapping error:&quot;</span>, e)</span><br><span class="line">        <span class="keyword">raise</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 保存tokenizer文件</span></span><br><span class="line">    tokenizer.save(os.path.join(save_dir, <span class="string">&quot;tokenizer.json&quot;</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 创建配置文件</span></span><br><span class="line">    create_tokenizer_config(save_dir)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Tokenizer saved to <span class="subst">&#123;save_dir&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>


<h4 id="Step-5-使用训练好的-Tokenizer"><a href="#Step-5-使用训练好的-Tokenizer" class="headerlink" title="Step 5: 使用训练好的 Tokenizer"></a>Step 5: 使用训练好的 Tokenizer</h4><p>我们可以使用训练好的 Tokenizer 来处理文本数据，如编码、解码、生成对话等。下面是一个简单的示例，展示了如何使用训练好的 Tokenizer 来处理文本数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">eval_tokenizer</span>(<span class="params">tokenizer_path: <span class="built_in">str</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;评估tokenizer功能&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)</span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Error loading tokenizer: <span class="subst">&#123;e&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 测试基本属性</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\n=== Tokenizer基本信息 ===&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Vocab size: <span class="subst">&#123;<span class="built_in">len</span>(tokenizer)&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Special tokens: <span class="subst">&#123;tokenizer.all_special_tokens&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Special token IDs: <span class="subst">&#123;tokenizer.all_special_ids&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 测试聊天模板</span></span><br><span class="line">    messages = [</span><br><span class="line">        &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;你是一个AI助手。&quot;</span>&#125;,</span><br><span class="line">        &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;How are you?&quot;</span>&#125;,</span><br><span class="line">        &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;assistant&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;I&#x27;m fine, thank you. and you?&quot;</span>&#125;,</span><br><span class="line">        &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;I&#x27;m good too.&quot;</span>&#125;,</span><br><span class="line">        &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;assistant&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;That&#x27;s great to hear!&quot;</span>&#125;,</span><br><span class="line">    ]</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\n=== 聊天模板测试 ===&quot;</span>)</span><br><span class="line">    prompt = tokenizer.apply_chat_template(</span><br><span class="line">        messages, </span><br><span class="line">        tokenize=<span class="literal">False</span>, </span><br><span class="line">        <span class="comment"># add_generation_prompt=True</span></span><br><span class="line">    )</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Generated prompt:\n&quot;</span>, prompt, sep=<span class="string">&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 测试编码解码</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\n=== 编码解码测试 ===&quot;</span>)</span><br><span class="line">    encoded = tokenizer(prompt, truncation=<span class="literal">True</span>, max_length=<span class="number">256</span>)</span><br><span class="line">    decoded = tokenizer.decode(encoded[<span class="string">&quot;input_ids&quot;</span>], skip_special_tokens=<span class="literal">False</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Decoded text matches original:&quot;</span>, decoded == prompt)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 测试特殊token处理</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\n=== 特殊token处理 ===&quot;</span>)</span><br><span class="line">    test_text = <span class="string">&quot;&lt;|im_start|&gt;user\nHello&lt;|im_end|&gt;&quot;</span></span><br><span class="line">    encoded = tokenizer(test_text).input_ids</span><br><span class="line">    decoded = tokenizer.decode(encoded)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Original: <span class="subst">&#123;test_text&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Decoded:  <span class="subst">&#123;decoded&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Special tokens preserved:&quot;</span>, decoded == test_text)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">eval_tokenizer(<span class="string">&#x27;your tokenizer path&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>OUT:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">=== Tokenizer基本信息 ===</span><br><span class="line">Vocab size: 6144</span><br><span class="line">Special tokens: [&#x27;&lt;|im_start|&gt;&#x27;, &#x27;&lt;|im_end|&gt;&#x27;, &#x27;&lt;unk&gt;&#x27;, &#x27;&lt;s&gt;&#x27;, &#x27;&lt;/s&gt;&#x27;]</span><br><span class="line">Special token IDs: [3, 4, 0, 1, 2]</span><br><span class="line"></span><br><span class="line">=== 聊天模板测试 ===</span><br><span class="line">Generated prompt:</span><br><span class="line">&lt;|im_start|&gt;system</span><br><span class="line">你是一个AI助手。&lt;|im_end|&gt;</span><br><span class="line">&lt;|im_start|&gt;user</span><br><span class="line">How are you?&lt;|im_end|&gt;</span><br><span class="line">&lt;|im_start|&gt;assistant</span><br><span class="line">I&#x27;m fine, thank you. and you?&lt;|im_end|&gt;</span><br><span class="line">&lt;|im_start|&gt;user</span><br><span class="line">I&#x27;m good too.&lt;|im_end|&gt;</span><br><span class="line">&lt;|im_start|&gt;assistant</span><br><span class="line">That&#x27;s great to hear!&lt;|im_end|&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">=== 编码解码测试 ===</span><br><span class="line">Decoded text matches original: False</span><br><span class="line"></span><br><span class="line">=== 特殊token处理 ===</span><br><span class="line">Original: &lt;|im_start|&gt;user</span><br><span class="line">Hello&lt;|im_end|&gt;</span><br><span class="line">Decoded:  &lt;|im_start|&gt; user</span><br><span class="line">Hello&lt;|im_end|&gt;</span><br><span class="line">Special tokens preserved: False</span><br></pre></td></tr></table></figure>

<h2 id="5-3-预训练一个小型LLM"><a href="#5-3-预训练一个小型LLM" class="headerlink" title="5.3 预训练一个小型LLM"></a>5.3 预训练一个小型LLM</h2><p>在前面的章节中，我们熟悉了各种大模型的模型结构，以及如如何训练Tokenizer。在本节中，我们将动手训练一个八千万参数的LLM。</p>
<h3 id="5-3-0-数据下载"><a href="#5-3-0-数据下载" class="headerlink" title="5.3.0 数据下载"></a>5.3.0 数据下载</h3><p>首先，我们需要下载预训练数据集。在这里，我们使用两个开源的数据集，包含了大量的中文对话数据，可以用于训练对话生成模型。</p>
<ul>
<li><p>出门问问序列猴子开源数据集：出门问问序列猴子通用文本数据集由来自网页、百科、博客、问答、开源代码、书籍、报刊、专利、教材、考题等多种公开可获取的数据进行汇总清洗之后而形成的大语言模型预训练语料。总量大概在 10B Token。</p>
</li>
<li><p>BelleGroup：350万条中文对话数据集，包含了人机对话、人人对话、人物对话等多种对话数据，可以用于训练对话生成模型。</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下载预训练数据集</span></span><br><span class="line">os.system(<span class="string">&quot;modelscope download --dataset ddzhu123/seq-monkey mobvoi_seq_monkey_general_open_corpus.jsonl.tar.bz2 --local_dir your_local_dir&quot;</span>)</span><br><span class="line"><span class="comment"># 解压预训练数据集</span></span><br><span class="line">os.system(<span class="string">&quot;tar -xvf your_local_dir/mobvoi_seq_monkey_general_open_corpus.jsonl.tar.bz2&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载SFT数据集</span></span><br><span class="line">os.system(<span class="string">f&#x27;huggingface-cli download --repo-type dataset --resume-download BelleGroup/train_3.5M_CN --local-dir BelleGroup&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1 处理预训练数据</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">split_text</span>(<span class="params">text, chunk_size=<span class="number">512</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;将文本按指定长度切分成块&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> [text[i:i+chunk_size] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(text), chunk_size)]</span><br><span class="line"></span><br><span class="line">input_file = <span class="string">&#x27;mobvoi_seq_monkey_general_open_corpus.jsonl&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;seq_monkey_datawhale.jsonl&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> pretrain:</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(input_file, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        data = f.readlines()</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> tqdm(data, desc=<span class="string">f&quot;Processing lines in <span class="subst">&#123;input_file&#125;</span>&quot;</span>, leave=<span class="literal">False</span>):  <span class="comment"># 添加行级别的进度条</span></span><br><span class="line">            line = json.loads(line)</span><br><span class="line">            text = line[<span class="string">&#x27;text&#x27;</span>]</span><br><span class="line">            chunks = split_text(text)</span><br><span class="line">            <span class="keyword">for</span> chunk <span class="keyword">in</span> chunks:</span><br><span class="line">                pretrain.write(json.dumps(&#123;<span class="string">&#x27;text&#x27;</span>: chunk&#125;, ensure_ascii=<span class="literal">False</span>) + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2 处理SFT数据</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">convert_message</span>(<span class="params">data</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    将原始数据转换为标准格式</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    message = [</span><br><span class="line">        &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;你是一个AI助手&quot;</span>&#125;,</span><br><span class="line">    ]</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> data:</span><br><span class="line">        <span class="keyword">if</span> item[<span class="string">&#x27;from&#x27;</span>] == <span class="string">&#x27;human&#x27;</span>:</span><br><span class="line">            message.append(&#123;<span class="string">&#x27;role&#x27;</span>: <span class="string">&#x27;user&#x27;</span>, <span class="string">&#x27;content&#x27;</span>: item[<span class="string">&#x27;value&#x27;</span>]&#125;)</span><br><span class="line">        <span class="keyword">elif</span> item[<span class="string">&#x27;from&#x27;</span>] == <span class="string">&#x27;assistant&#x27;</span>:</span><br><span class="line">            message.append(&#123;<span class="string">&#x27;role&#x27;</span>: <span class="string">&#x27;assistant&#x27;</span>, <span class="string">&#x27;content&#x27;</span>: item[<span class="string">&#x27;value&#x27;</span>]&#125;)</span><br><span class="line">    <span class="keyword">return</span> message</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;BelleGroup_sft.jsonl&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> sft:</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;BelleGroup/train_3.5M_CN.json&#x27;</span>, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        data = f.readlines()</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> tqdm(data, desc=<span class="string">&quot;Processing&quot;</span>, unit=<span class="string">&quot;lines&quot;</span>):</span><br><span class="line">            item = json.loads(item)</span><br><span class="line">            message = convert_message(item[<span class="string">&#x27;conversations&#x27;</span>])</span><br><span class="line">            sft.write(json.dumps(message, ensure_ascii=<span class="literal">False</span>) + <span class="string">&#x27;\n&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="5-3-1-训练Tokenize"><a href="#5-3-1-训练Tokenize" class="headerlink" title="5.3.1 训练Tokenize"></a>5.3.1 训练Tokenize</h3><p>首先，我们需要为文本处理训练一个Tokenizer。Tokenizer的作用是将文本转换为数字序列，以便模型能够理解和处理。我们使用的数据集是 <a target="_blank" rel="noopener" href="https://www.modelscope.cn/datasets/ddzhu123/seq-monkey/files">出门问问序列猴子开源数据集</a> ，这个数据集包含了大量的中文文本数据，可以用于训练Tokenizer。</p>
<blockquote>
<p>注：由于数据集较大，如果大家在自己本地电脑训练的话进度比较慢，所以在这里我们提供了一个已经训练好的Tokenizer，大家可以直接使用。如果大家想要自己训练的话，可以参考下面的代码。</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python code/train_tokenizer.py</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, PreTrainedTokenizerFast</span><br><span class="line"><span class="keyword">from</span> tokenizers <span class="keyword">import</span> (</span><br><span class="line">    decoders,</span><br><span class="line">    models,</span><br><span class="line">    pre_tokenizers,</span><br><span class="line">    trainers,</span><br><span class="line">    Tokenizer,</span><br><span class="line">)</span><br><span class="line"><span class="keyword">from</span> tokenizers.normalizers <span class="keyword">import</span> NFKC</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> Generator</span><br><span class="line"></span><br><span class="line">random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_texts_from_jsonl</span>(<span class="params">file_path: <span class="built_in">str</span></span>) -&gt; Generator[<span class="built_in">str</span>, <span class="literal">None</span>, <span class="literal">None</span>]:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;读取JSONL文件并安全提取文本数据&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(file_path, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line_num, line <span class="keyword">in</span> <span class="built_in">enumerate</span>(f, <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                data = json.loads(line)</span><br><span class="line">                <span class="keyword">if</span> <span class="string">&#x27;text&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> data:</span><br><span class="line">                    <span class="keyword">raise</span> KeyError(<span class="string">f&quot;Missing &#x27;text&#x27; field in line <span class="subst">&#123;line_num&#125;</span>&quot;</span>)</span><br><span class="line">                <span class="keyword">yield</span> data[<span class="string">&#x27;text&#x27;</span>]</span><br><span class="line">            <span class="keyword">except</span> json.JSONDecodeError:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;Error decoding JSON in line <span class="subst">&#123;line_num&#125;</span>&quot;</span>)</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">except</span> KeyError <span class="keyword">as</span> e:</span><br><span class="line">                <span class="built_in">print</span>(e)</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_tokenizer_config</span>(<span class="params">save_dir: <span class="built_in">str</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;创建完整的tokenizer配置文件&quot;&quot;&quot;</span></span><br><span class="line">    config = &#123;</span><br><span class="line">        <span class="string">&quot;add_bos_token&quot;</span>: <span class="literal">False</span>,</span><br><span class="line">        <span class="string">&quot;add_eos_token&quot;</span>: <span class="literal">False</span>,</span><br><span class="line">        <span class="string">&quot;add_prefix_space&quot;</span>: <span class="literal">True</span>,</span><br><span class="line">        <span class="string">&quot;bos_token&quot;</span>: <span class="string">&quot;&lt;|im_start|&gt;&quot;</span>,</span><br><span class="line">        <span class="string">&quot;eos_token&quot;</span>: <span class="string">&quot;&lt;|im_end|&gt;&quot;</span>,</span><br><span class="line">        <span class="string">&quot;pad_token&quot;</span>: <span class="string">&quot;&lt;|im_end|&gt;&quot;</span>,</span><br><span class="line">        <span class="string">&quot;unk_token&quot;</span>: <span class="string">&quot;&lt;unk&gt;&quot;</span>,</span><br><span class="line">        <span class="string">&quot;model_max_length&quot;</span>: <span class="number">1000000000000000019884624838656</span>,</span><br><span class="line">        <span class="string">&quot;clean_up_tokenization_spaces&quot;</span>: <span class="literal">False</span>,</span><br><span class="line">        <span class="string">&quot;tokenizer_class&quot;</span>: <span class="string">&quot;PreTrainedTokenizerFast&quot;</span>,</span><br><span class="line">        <span class="string">&quot;chat_template&quot;</span>: (</span><br><span class="line">            <span class="string">&quot;&#123;% for message in messages %&#125;&quot;</span></span><br><span class="line">            <span class="string">&quot;&#123;% if message[&#x27;role&#x27;] == &#x27;system&#x27; %&#125;&quot;</span></span><br><span class="line">            <span class="string">&quot;&lt;|im_start|&gt;system\n&#123;&#123; message[&#x27;content&#x27;] &#125;&#125;&lt;|im_end|&gt;\n&quot;</span></span><br><span class="line">            <span class="string">&quot;&#123;% elif message[&#x27;role&#x27;] == &#x27;user&#x27; %&#125;&quot;</span></span><br><span class="line">            <span class="string">&quot;&lt;|im_start|&gt;user\n&#123;&#123; message[&#x27;content&#x27;] &#125;&#125;&lt;|im_end|&gt;\n&quot;</span></span><br><span class="line">            <span class="string">&quot;&#123;% elif message[&#x27;role&#x27;] == &#x27;assistant&#x27; %&#125;&quot;</span></span><br><span class="line">            <span class="string">&quot;&lt;|im_start|&gt;assistant\n&#123;&#123; message[&#x27;content&#x27;] &#125;&#125;&lt;|im_end|&gt;\n&quot;</span></span><br><span class="line">            <span class="string">&quot;&#123;% endif %&#125;&quot;</span></span><br><span class="line">            <span class="string">&quot;&#123;% endfor %&#125;&quot;</span></span><br><span class="line">            <span class="string">&quot;&#123;% if add_generation_prompt %&#125;&quot;</span></span><br><span class="line">            <span class="string">&quot;&#123;&#123; &#x27;&lt;|im_start|&gt;assistant\n&#x27; &#125;&#125;&quot;</span></span><br><span class="line">            <span class="string">&quot;&#123;% endif %&#125;&quot;</span></span><br><span class="line">        )</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 保存主配置文件</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(save_dir, <span class="string">&quot;tokenizer_config.json&quot;</span>), <span class="string">&quot;w&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        json.dump(config, f, ensure_ascii=<span class="literal">False</span>, indent=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建special_tokens_map.json</span></span><br><span class="line">    special_tokens_map = &#123;</span><br><span class="line">        <span class="string">&quot;bos_token&quot;</span>: <span class="string">&quot;&lt;|im_start|&gt;&quot;</span>,</span><br><span class="line">        <span class="string">&quot;eos_token&quot;</span>: <span class="string">&quot;&lt;|im_end|&gt;&quot;</span>,</span><br><span class="line">        <span class="string">&quot;unk_token&quot;</span>: <span class="string">&quot;&lt;unk&gt;&quot;</span>,</span><br><span class="line">        <span class="string">&quot;pad_token&quot;</span>: <span class="string">&quot;&lt;|im_end|&gt;&quot;</span>,</span><br><span class="line">        <span class="string">&quot;additional_special_tokens&quot;</span>: [<span class="string">&quot;&lt;s&gt;&quot;</span>, <span class="string">&quot;&lt;/s&gt;&quot;</span>]</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(save_dir, <span class="string">&quot;special_tokens_map.json&quot;</span>), <span class="string">&quot;w&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        json.dump(special_tokens_map, f, ensure_ascii=<span class="literal">False</span>, indent=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_tokenizer</span>(<span class="params">data_path: <span class="built_in">str</span>, save_dir: <span class="built_in">str</span>, vocab_size: <span class="built_in">int</span> = <span class="number">8192</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练并保存自定义tokenizer&quot;&quot;&quot;</span></span><br><span class="line">    os.makedirs(save_dir, exist_ok=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 初始化tokenizer</span></span><br><span class="line">    tokenizer = Tokenizer(models.BPE(unk_token=<span class="string">&quot;&lt;unk&gt;&quot;</span>))</span><br><span class="line">    tokenizer.normalizer = NFKC()  <span class="comment"># 添加文本规范化</span></span><br><span class="line">    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=<span class="literal">False</span>)</span><br><span class="line">    tokenizer.decoder = decoders.ByteLevel()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 配置特殊token</span></span><br><span class="line">    special_tokens = [</span><br><span class="line">        <span class="string">&quot;&lt;unk&gt;&quot;</span>, </span><br><span class="line">        <span class="string">&quot;&lt;s&gt;&quot;</span>, </span><br><span class="line">        <span class="string">&quot;&lt;/s&gt;&quot;</span>, </span><br><span class="line">        <span class="string">&quot;&lt;|im_start|&gt;&quot;</span>, </span><br><span class="line">        <span class="string">&quot;&lt;|im_end|&gt;&quot;</span></span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 配置训练器</span></span><br><span class="line">    trainer = trainers.BpeTrainer(</span><br><span class="line">        vocab_size=vocab_size,</span><br><span class="line">        special_tokens=special_tokens,</span><br><span class="line">        min_frequency=<span class="number">2</span>,  <span class="comment"># 提高低频词过滤</span></span><br><span class="line">        show_progress=<span class="literal">True</span>,</span><br><span class="line">        initial_alphabet=pre_tokenizers.ByteLevel.alphabet()</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练tokenizer</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Training tokenizer with data from <span class="subst">&#123;data_path&#125;</span>&quot;</span>)</span><br><span class="line">    texts = read_texts_from_jsonl(data_path)</span><br><span class="line">    tokenizer.train_from_iterator(texts, trainer=trainer, length=os.path.getsize(data_path))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 验证特殊token映射</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">assert</span> tokenizer.token_to_id(<span class="string">&quot;&lt;unk&gt;&quot;</span>) == <span class="number">0</span></span><br><span class="line">        <span class="keyword">assert</span> tokenizer.token_to_id(<span class="string">&quot;&lt;s&gt;&quot;</span>) == <span class="number">1</span></span><br><span class="line">        <span class="keyword">assert</span> tokenizer.token_to_id(<span class="string">&quot;&lt;/s&gt;&quot;</span>) == <span class="number">2</span></span><br><span class="line">        <span class="keyword">assert</span> tokenizer.token_to_id(<span class="string">&quot;&lt;|im_start|&gt;&quot;</span>) == <span class="number">3</span></span><br><span class="line">        <span class="keyword">assert</span> tokenizer.token_to_id(<span class="string">&quot;&lt;|im_end|&gt;&quot;</span>) == <span class="number">4</span></span><br><span class="line">    <span class="keyword">except</span> AssertionError <span class="keyword">as</span> e:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Special tokens mapping error:&quot;</span>, e)</span><br><span class="line">        <span class="keyword">raise</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 保存tokenizer文件</span></span><br><span class="line">    tokenizer.save(os.path.join(save_dir, <span class="string">&quot;tokenizer.json&quot;</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 创建配置文件</span></span><br><span class="line">    create_tokenizer_config(save_dir)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Tokenizer saved to <span class="subst">&#123;save_dir&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">eval_tokenizer</span>(<span class="params">tokenizer_path: <span class="built_in">str</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;评估tokenizer功能&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)</span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Error loading tokenizer: <span class="subst">&#123;e&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 测试基本属性</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\n=== Tokenizer基本信息 ===&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Vocab size: <span class="subst">&#123;<span class="built_in">len</span>(tokenizer)&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Special tokens: <span class="subst">&#123;tokenizer.all_special_tokens&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Special token IDs: <span class="subst">&#123;tokenizer.all_special_ids&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 测试聊天模板</span></span><br><span class="line">    messages = [</span><br><span class="line">        &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;你是一个AI助手。&quot;</span>&#125;,</span><br><span class="line">        &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;How are you?&quot;</span>&#125;,</span><br><span class="line">        &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;assistant&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;I&#x27;m fine, thank you. and you?&quot;</span>&#125;,</span><br><span class="line">        &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;I&#x27;m good too.&quot;</span>&#125;,</span><br><span class="line">        &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;assistant&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;That&#x27;s great to hear!&quot;</span>&#125;,</span><br><span class="line">    ]</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\n=== 聊天模板测试 ===&quot;</span>)</span><br><span class="line">    prompt = tokenizer.apply_chat_template(</span><br><span class="line">        messages, </span><br><span class="line">        tokenize=<span class="literal">False</span>, </span><br><span class="line">        <span class="comment"># add_generation_prompt=True</span></span><br><span class="line">    )</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Generated prompt:\n&quot;</span>, prompt, sep=<span class="string">&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 测试编码解码</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\n=== 编码解码测试 ===&quot;</span>)</span><br><span class="line">    encoded = tokenizer(prompt, truncation=<span class="literal">True</span>, max_length=<span class="number">256</span>)</span><br><span class="line">    decoded = tokenizer.decode(encoded[<span class="string">&quot;input_ids&quot;</span>], skip_special_tokens=<span class="literal">False</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Decoded text matches original:&quot;</span>, decoded == prompt)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 测试特殊token处理</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\n=== 特殊token处理 ===&quot;</span>)</span><br><span class="line">    test_text = <span class="string">&quot;&lt;|im_start|&gt;user\nHello&lt;|im_end|&gt;&quot;</span></span><br><span class="line">    encoded = tokenizer(test_text).input_ids</span><br><span class="line">    decoded = tokenizer.decode(encoded)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Original: <span class="subst">&#123;test_text&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Decoded:  <span class="subst">&#123;decoded&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Special tokens preserved:&quot;</span>, decoded == test_text)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    <span class="comment"># 配置路径</span></span><br><span class="line">    data_path = <span class="string">&quot;your data path&quot;</span></span><br><span class="line">    save_dir = <span class="string">&quot;tokenizer_k&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练tokenizer</span></span><br><span class="line">    train_tokenizer(</span><br><span class="line">        data_path=data_path,</span><br><span class="line">        save_dir=save_dir,</span><br><span class="line">        vocab_size=<span class="number">6144</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 评估tokenizer</span></span><br><span class="line">    eval_tokenizer(save_dir)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>

<p>训练完成之后可以可以使用 <code>eval_tokenizer()</code> 测试 Tokenizer 的功能，确保 Tokenizer 正常工作。在这个函数中，我们首先加载训练好的 Tokenizer，然后测试了 Tokenizer 的基本属性、聊天模板、编码解码等功能。这些测试可以帮助我们验证 Tokenizer 的正确性，确保它能够正常工作。正确的输出为：</p>
<p>OUT:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">=== Tokenizer基本信息 ===</span><br><span class="line">Vocab size: 6144</span><br><span class="line">Special tokens: [&#x27;&lt;|im_start|&gt;&#x27;, &#x27;&lt;|im_end|&gt;&#x27;, &#x27;&lt;unk&gt;&#x27;, &#x27;&lt;s&gt;&#x27;, &#x27;&lt;/s&gt;&#x27;]</span><br><span class="line">Special token IDs: [3, 4, 0, 1, 2]</span><br><span class="line"></span><br><span class="line">=== 聊天模板测试 ===</span><br><span class="line">Generated prompt:</span><br><span class="line">&lt;|im_start|&gt;system</span><br><span class="line">你是一个AI助手。&lt;|im_end|&gt;</span><br><span class="line">&lt;|im_start|&gt;user</span><br><span class="line">How are you?&lt;|im_end|&gt;</span><br><span class="line">&lt;|im_start|&gt;assistant</span><br><span class="line">I&#x27;m fine, thank you. and you?&lt;|im_end|&gt;</span><br><span class="line">&lt;|im_start|&gt;user</span><br><span class="line">I&#x27;m good too.&lt;|im_end|&gt;</span><br><span class="line">&lt;|im_start|&gt;assistant</span><br><span class="line">That&#x27;s great to hear!&lt;|im_end|&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">=== 编码解码测试 ===</span><br><span class="line">Decoded text matches original: False</span><br><span class="line"></span><br><span class="line">=== 特殊token处理 ===</span><br><span class="line">Original: &lt;|im_start|&gt;user</span><br><span class="line">Hello&lt;|im_end|&gt;</span><br><span class="line">Decoded:  &lt;|im_start|&gt; user</span><br><span class="line">Hello&lt;|im_end|&gt;</span><br><span class="line">Special tokens preserved: False</span><br></pre></td></tr></table></figure>

<h3 id="5-3-2-Dataset"><a href="#5-3-2-Dataset" class="headerlink" title="5.3.2 Dataset"></a>5.3.2 Dataset</h3><h4 id="PretrainDataset"><a href="#PretrainDataset" class="headerlink" title="PretrainDataset"></a>PretrainDataset</h4><p>在将数据送入到模型之前，我们还需要进行一些处理用于将文本数据转化为模型能够理解的Token。在这里我们使用的是Pytorch的Dataset类，用于加载数据集。我们定义了一个<code>PretrainDataset</code>类，用于加载已预处理好的数据集。我们继承了<code>torch.utils.data.IterableDataset</code>来定义该数据集，这使得我们可以更灵活、高效地处理数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PretrainDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data_path, tokenizer, max_length=<span class="number">512</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.data_path = data_path</span><br><span class="line">        <span class="variable language_">self</span>.tokenizer = tokenizer</span><br><span class="line">        <span class="variable language_">self</span>.max_length = max_length</span><br><span class="line">        <span class="variable language_">self</span>.padding = <span class="number">0</span></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(data_path, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="variable language_">self</span>.data = f.readlines()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index: <span class="built_in">int</span></span>):</span><br><span class="line">        sample = json.loads(<span class="variable language_">self</span>.data[index])</span><br><span class="line">        text = <span class="string">f&quot;<span class="subst">&#123;self.tokenizer.bos_token&#125;</span><span class="subst">&#123;sample[<span class="string">&#x27;text&#x27;</span>]&#125;</span>&quot;</span></span><br><span class="line">        input_id = <span class="variable language_">self</span>.tokenizer(text).data[<span class="string">&#x27;input_ids&#x27;</span>][:<span class="variable language_">self</span>.max_length]</span><br><span class="line">        text_len = <span class="built_in">len</span>(input_id)</span><br><span class="line">        <span class="comment"># 没满最大长度的剩余部分</span></span><br><span class="line">        padding_len = <span class="variable language_">self</span>.max_length - text_len</span><br><span class="line">        input_id = input_id + [<span class="variable language_">self</span>.padding] * padding_len</span><br><span class="line">        <span class="comment"># 0表示不计算损失</span></span><br><span class="line">        loss_mask = [<span class="number">1</span>] * text_len + [<span class="number">0</span>] * padding_len</span><br><span class="line"></span><br><span class="line">        input_id = np.array(input_id)</span><br><span class="line">        X = np.array(input_id[:-<span class="number">1</span>]).astype(np.int64)</span><br><span class="line">        Y = np.array(input_id[<span class="number">1</span>:]).astype(np.int64)</span><br><span class="line">        loss_mask = np.array(loss_mask[<span class="number">1</span>:]).astype(np.int64)</span><br><span class="line">        <span class="keyword">return</span> torch.from_numpy(X), torch.from_numpy(Y), torch.from_numpy(loss_mask)</span><br></pre></td></tr></table></figure>

<p>在以上代码和图5.1可以看出，<code>Pretrain Dataset</code> 主要是将 <code>text</code> 通过 <code>tokenizer</code> 转换成 <code>input_id</code>，然后将 <code>input_id</code> 拆分成 <code>X</code> 和 <code>Y</code>，其中 <code>X</code> 为 <code>input_id</code> 的前 n-1 个元素，<code>Y</code> 为 <code>input_id</code> 的后 n-1 <code>个元素。loss_mask</code> 主要是用来标记哪些位置需要计算损失，哪些位置不需要计算损失。</p>
<div align='center'>
    <img src="../images/5-images/pretrain_dataset.png" alt="alt text" width="100%">
    <p>图5.1 预训练损失函数计算</p>
</div>

<p>图中示例展示了当<code>max_length=9</code>时的处理过程：</p>
<ul>
<li><strong>输入序列</strong>：<code>[BOS, T1, T2, T3, T4, T5, T6, T7, EOS]</code></li>
<li><strong>样本拆分</strong>：<ul>
<li>X：<code>[BOS, T1, T2, T3, T4, T5, T6, T7]</code> → 模型输入上下文</li>
<li>Y：<code>[T1, T2, T3, T4, T5, T6, T7, EOS]</code> → 模型预测目标</li>
</ul>
</li>
<li><strong>损失掩码</strong>：<ul>
<li>有效位置：<code>[0, 1, 1, 1, 1, 1, 1, 1, 1]</code> → 仅对T1-EOS计算损失</li>
</ul>
</li>
</ul>
<h4 id="SFTDataset"><a href="#SFTDataset" class="headerlink" title="SFTDataset"></a>SFTDataset</h4><p><code>SFTDataset</code> 其实是一个多轮对话数据集，我们的目标是让模型学会如何进行多轮对话。在这个阶段我们的输入是上一轮的对话内容，输出是当前轮的对话内容。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SFTDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data_path, tokenizer, max_length=<span class="number">512</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.data_path = data_path</span><br><span class="line">        <span class="variable language_">self</span>.tokenizer = tokenizer</span><br><span class="line">        <span class="variable language_">self</span>.max_length = max_length</span><br><span class="line">        <span class="variable language_">self</span>.padding = <span class="number">0</span></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(data_path, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="variable language_">self</span>.data = f.readlines()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">generate_loss_mask</span>(<span class="params">self, input_ids</span>):</span><br><span class="line">        <span class="comment"># 生成 loss mask, 0 表示不计算损失, 1 表示计算损失</span></span><br><span class="line">        mask = [<span class="number">0</span>] * <span class="built_in">len</span>(input_ids)</span><br><span class="line">        a_sequence = [<span class="number">3</span>, <span class="number">1074</span>, <span class="number">537</span>, <span class="number">500</span>, <span class="number">203</span>]  <span class="comment"># &lt;|im_start|&gt;assistant\n</span></span><br><span class="line">        a_length = <span class="built_in">len</span>(a_sequence)</span><br><span class="line">        n = <span class="built_in">len</span>(input_ids)</span><br><span class="line">        i = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">while</span> i &lt;= n - a_length:</span><br><span class="line">            <span class="comment"># 检查当前位置是否匹配目标子序列</span></span><br><span class="line">            <span class="keyword">match</span> = <span class="literal">True</span></span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(a_length):</span><br><span class="line">                <span class="keyword">if</span> input_ids[i + k] != a_sequence[k]:</span><br><span class="line">                    <span class="keyword">match</span> = <span class="literal">False</span></span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">match</span>:</span><br><span class="line">                <span class="comment"># 从子序列结束的位置开始查找第一个4, 4 为 &lt;|im_end|&gt; EOS id</span></span><br><span class="line">                j = <span class="literal">None</span></span><br><span class="line">                <span class="keyword">for</span> idx <span class="keyword">in</span> <span class="built_in">range</span>(i + a_length, n):</span><br><span class="line">                    <span class="keyword">if</span> input_ids[idx] == <span class="number">4</span>:</span><br><span class="line">                        j = idx</span><br><span class="line">                        <span class="keyword">break</span></span><br><span class="line">                <span class="keyword">if</span> j <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    start = i + a_length</span><br><span class="line">                    end = j  <span class="comment"># 结束位置设为j（包含4）</span></span><br><span class="line">                    <span class="comment"># 标记区间为1（包括start到end）</span></span><br><span class="line">                    <span class="keyword">if</span> start &lt;= end:</span><br><span class="line">                        <span class="keyword">for</span> pos <span class="keyword">in</span> <span class="built_in">range</span>(start, end + <span class="number">1</span>):</span><br><span class="line">                            <span class="keyword">if</span> pos &lt; <span class="built_in">len</span>(mask):</span><br><span class="line">                                mask[pos] = <span class="number">1</span></span><br><span class="line">                <span class="comment"># 跳过当前子序列，避免重叠匹配</span></span><br><span class="line">                i += a_length</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                i += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> mask</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index: <span class="built_in">int</span></span>):</span><br><span class="line">        sample = json.loads(<span class="variable language_">self</span>.data[index])</span><br><span class="line">        text = <span class="variable language_">self</span>.tokenizer.apply_chat_template(sample, tokenize=<span class="literal">False</span>, add_generation_prompt=<span class="literal">False</span>)</span><br><span class="line">        input_id = <span class="variable language_">self</span>.tokenizer(text).data[<span class="string">&#x27;input_ids&#x27;</span>][:<span class="variable language_">self</span>.max_length]</span><br><span class="line">        text_len = <span class="built_in">len</span>(input_id)</span><br><span class="line">        <span class="comment"># 没满最大长度的剩余部分</span></span><br><span class="line">        padding_len = <span class="variable language_">self</span>.max_length - text_len</span><br><span class="line">        input_id = input_id + [<span class="variable language_">self</span>.padding] * padding_len</span><br><span class="line">        <span class="comment"># 0表示不计算损失</span></span><br><span class="line">        loss_mask = <span class="variable language_">self</span>.generate_loss_mask(input_id)</span><br><span class="line"></span><br><span class="line">        input_id = np.array(input_id)</span><br><span class="line">        X = np.array(input_id[:-<span class="number">1</span>]).astype(np.int64)</span><br><span class="line">        Y = np.array(input_id[<span class="number">1</span>:]).astype(np.int64)</span><br><span class="line">        loss_mask = np.array(loss_mask[<span class="number">1</span>:]).astype(np.int64)</span><br><span class="line">        <span class="keyword">return</span> torch.from_numpy(X), torch.from_numpy(Y), torch.from_numpy(loss_mask)</span><br></pre></td></tr></table></figure>

<p>在 SFT 阶段，这里使用的是多轮对话数据集，所以就需要区分哪些位置需要计算损失，哪些位置不需要计算损失。在上面的代码中，我使用了一个 <code>generate_loss_mask</code> 函数来生成 <code>loss_mask</code>。这个函数主要是用来生成 <code>loss_mask</code>，其中 <code>loss_mask</code> 的生成规则是：当遇到 <code>|&lt;im_start|&gt;assistant\n</code> 时，就开始计算损失，直到遇到 <code>|&lt;im_end|&gt;</code> 为止。这样就可以保证我们的模型在 SFT 阶段只计算当前轮的对话内容，如图5.2所示。</p>
<div align='center'>
    <img src="../images/5-images/sftdataset.png" alt="alt text" width="90%">
    <p>图5.2 SFT 损失函数计算</p>
</div>

<p>可以看到，其实 SFT Dataset 和 Pretrain Dataset 的 <code>X</code> 和 <code>Y</code> 是一样的，只是在 SFT Dataset 中我们需要生成一个 <code>loss_mask</code> 来标记哪些位置需要计算损失，哪些位置不需要计算损失。 图中 <code>Input ids</code> 中的蓝色小方格就是AI的回答，所以是需要模型学习的地方。所以在 <code>loss_mask</code> 中，蓝色小方格对应的位置是黄色，其他位置是灰色。在代码 <code>loss_mask</code> 中的 1 对应的位置计算损失，0 对应的位置不计算损失。</p>
<h3 id="5-3-3-预训练"><a href="#5-3-3-预训练" class="headerlink" title="5.3.3 预训练"></a>5.3.3 预训练</h3><p>在数据预处理完成后，我们就可以开始训练模型了。我们使用的模型是一个和LLama2结构一样的 Decoder only Transformer模型，使用Pytorch实现。相关代码在<code>code/k_model.py</code>文件中。此处不再赘述，源码中有详细的中文注释，且我们在之前的文章中也有详细的介绍。</p>
<p>在模型这一部分可以重点看一下生成式模型是如何实现生成token的，可以查看<code>k_model.py</code>文件中的<code>Transforerm</code>类中的<code>generate</code>方法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@torch.inference_mode()</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">generate</span>(<span class="params">self, idx, stop_id=<span class="literal">None</span>, max_new_tokens=<span class="number">256</span>, temperature=<span class="number">1.0</span>, top_k=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        给定输入序列 idx（形状为 (bz,seq_len) 的长整型张量），通过多次生成新 token 来完成序列。</span></span><br><span class="line"><span class="string">        在 model.eval() 模式下运行。效率较低的采样版本，没有使用键k/v cache。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        index = idx.shape[<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(max_new_tokens):</span><br><span class="line">            <span class="comment"># 如果序列上下文过长，截断它到最大长度</span></span><br><span class="line">            idx_cond = idx <span class="keyword">if</span> idx.size(<span class="number">1</span>) &lt;= <span class="variable language_">self</span>.args.max_seq_len <span class="keyword">else</span> idx[:, -<span class="variable language_">self</span>.args.max_seq_len:]</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 前向传播获取序列中最后一个位置的 logits</span></span><br><span class="line">            logits = <span class="variable language_">self</span>(idx_cond).logits</span><br><span class="line">            logits = logits[:, -<span class="number">1</span>, :] <span class="comment"># 只保留最后一个时间步的输出</span></span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> temperature == <span class="number">0.0</span>:</span><br><span class="line">                <span class="comment"># 选择最有可能的索引</span></span><br><span class="line">                _, idx_next = torch.topk(logits, k=<span class="number">1</span>, dim=-<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># 缩放 logits 并应用 softmax</span></span><br><span class="line">                logits = logits / temperature</span><br><span class="line">                <span class="keyword">if</span> top_k <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    v, _ = torch.topk(logits, <span class="built_in">min</span>(top_k, logits.size(-<span class="number">1</span>)))</span><br><span class="line">                    logits[logits &lt; v[:, [-<span class="number">1</span>]]] = -<span class="built_in">float</span>(<span class="string">&#x27;Inf&#x27;</span>)</span><br><span class="line">                probs = F.softmax(logits, dim=-<span class="number">1</span>)</span><br><span class="line">                idx_next = torch.multinomial(probs, num_samples=<span class="number">1</span>)</span><br><span class="line">            </span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> idx_next == stop_id:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 将采样的索引添加到序列中并继续</span></span><br><span class="line">            idx = torch.cat((idx, idx_next), dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> idx[:, index:] <span class="comment"># 只返回生成的token</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>在 <code>generate</code> 方法中，我们首先获取序列中最后一个位置的 <code>logits</code>，然后基于这些 <code>logits</code> 生成新的 <code>token</code>。接着，生成的新 <code>token</code> 会被添加到序列中，模型随后会继续生成下一个 <code>token</code>。通过这种迭代过程，我们能够生成完整的文本。</p>
<p>接下来就是最重要的部分，训练模型!</p>
<blockquote>
<p>注：在使用下面代码进行模型训练时，需要指定 <code>--data_path</code> 参数为预处理好的数据集路径，例如 <code>--data_path seq_monkey_datawhale.jsonl</code>，也需要指定要用哪几张GPU进行训练，例如 <code>--gpus 0,1</code>。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_lr</span>(<span class="params">it, <span class="built_in">all</span></span>):</span><br><span class="line">    warmup_iters = args.warmup_iters</span><br><span class="line">    lr_decay_iters = <span class="built_in">all</span></span><br><span class="line">    min_lr = args.learning_rate / <span class="number">10</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> it &lt; warmup_iters:</span><br><span class="line">        <span class="keyword">return</span> args.learning_rate * it / warmup_iters</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> it &gt; lr_decay_iters:</span><br><span class="line">        <span class="keyword">return</span> min_lr</span><br><span class="line">    </span><br><span class="line">    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)</span><br><span class="line">    <span class="keyword">assert</span> <span class="number">0</span> &lt;= decay_ratio &lt;= <span class="number">1</span></span><br><span class="line">    coeff = <span class="number">0.5</span> * (<span class="number">1.0</span> + math.cos(math.pi * decay_ratio))</span><br><span class="line">    <span class="keyword">return</span> min_lr + coeff * (args.learning_rate - min_lr)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_epoch</span>(<span class="params">epoch</span>):</span><br><span class="line">    start_time = time.time()</span><br><span class="line">    <span class="keyword">for</span> step, (X, Y, loss_mask) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">        X = X.to(args.device)</span><br><span class="line">        Y = Y.to(args.device)</span><br><span class="line">        loss_mask = loss_mask.to(args.device)</span><br><span class="line"></span><br><span class="line">        lr = get_lr(epoch * iter_per_epoch + step, args.epochs * iter_per_epoch)</span><br><span class="line">        <span class="keyword">for</span> param_group <span class="keyword">in</span> optimizer.param_groups:</span><br><span class="line">            param_group[<span class="string">&#x27;lr&#x27;</span>] = lr</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> ctx:</span><br><span class="line">            out = model(X, Y)</span><br><span class="line">            loss = out.last_loss / args.accumulation_steps</span><br><span class="line">            loss_mask = loss_mask.view(-<span class="number">1</span>)</span><br><span class="line">            loss = torch.<span class="built_in">sum</span>(loss * loss_mask) / loss_mask.<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line">        scaler.scale(loss).backward()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (step + <span class="number">1</span>) % args.accumulation_steps == <span class="number">0</span>:</span><br><span class="line">            scaler.unscale_(optimizer)</span><br><span class="line">            torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)</span><br><span class="line"></span><br><span class="line">            scaler.step(optimizer)</span><br><span class="line">            scaler.update()</span><br><span class="line"></span><br><span class="line">            optimizer.zero_grad(set_to_none=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> step % args.log_interval == <span class="number">0</span>:</span><br><span class="line">            spend_time = time.time() - start_time</span><br><span class="line">            Logger(</span><br><span class="line">                <span class="string">&#x27;Epoch:[&#123;&#125;/&#123;&#125;](&#123;&#125;/&#123;&#125;) loss:&#123;:.3f&#125; lr:&#123;:.7f&#125; epoch_Time:&#123;&#125;min:&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">                    epoch + <span class="number">1</span>,</span><br><span class="line">                    args.epochs,</span><br><span class="line">                    step,</span><br><span class="line">                    iter_per_epoch,</span><br><span class="line">                    loss.item() * args.accumulation_steps,</span><br><span class="line">                    optimizer.param_groups[-<span class="number">1</span>][<span class="string">&#x27;lr&#x27;</span>],</span><br><span class="line">                    spend_time / (step + <span class="number">1</span>) * iter_per_epoch // <span class="number">60</span> - spend_time // <span class="number">60</span>))</span><br><span class="line">            <span class="keyword">if</span> args.use_swanlab:</span><br><span class="line">                swanlab.log(&#123;</span><br><span class="line">                    <span class="string">&quot;loss&quot;</span>: loss.item() * args.accumulation_steps,</span><br><span class="line">                    <span class="string">&quot;lr&quot;</span>: optimizer.param_groups[-<span class="number">1</span>][<span class="string">&#x27;lr&#x27;</span>]</span><br><span class="line">                &#125;)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (step + <span class="number">1</span>) % args.save_interval == <span class="number">0</span>:</span><br><span class="line">            model.<span class="built_in">eval</span>()</span><br><span class="line">            ckp = <span class="string">f&#x27;<span class="subst">&#123;args.save_dir&#125;</span>/pretrain_<span class="subst">&#123;lm_config.dim&#125;</span>_<span class="subst">&#123;lm_config.n_layers&#125;</span>_<span class="subst">&#123;lm_config.vocab_size&#125;</span>.pth&#x27;</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 处理多卡保存</span></span><br><span class="line">            state_dict = model.module.state_dict() <span class="keyword">if</span> <span class="built_in">isinstance</span>(model, torch.nn.DataParallel) <span class="keyword">else</span> model.state_dict()</span><br><span class="line">            torch.save(state_dict, ckp)</span><br><span class="line">            model.train()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> (step + <span class="number">1</span>) % <span class="number">20000</span> == <span class="number">0</span>:</span><br><span class="line">            model.<span class="built_in">eval</span>()</span><br><span class="line">            ckp = <span class="string">f&#x27;<span class="subst">&#123;args.save_dir&#125;</span>/pretrain_<span class="subst">&#123;lm_config.dim&#125;</span>_<span class="subst">&#123;lm_config.n_layers&#125;</span>_<span class="subst">&#123;lm_config.vocab_size&#125;</span>_step<span class="subst">&#123;step+<span class="number">1</span>&#125;</span>.pth&#x27;</span></span><br><span class="line"></span><br><span class="line">            state_dict = model.module.state_dict() <span class="keyword">if</span> <span class="built_in">isinstance</span>(model, torch.nn.DataParallel) <span class="keyword">else</span> model.state_dict()</span><br><span class="line">            torch.save(state_dict, ckp)</span><br><span class="line">            model.train()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_model</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">count_parameters</span>(<span class="params">model</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">sum</span>(p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters() <span class="keyword">if</span> p.requires_grad)</span><br><span class="line"></span><br><span class="line">    tokenizer = AutoTokenizer.from_pretrained(<span class="string">&#x27;./tokenizer_k/&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    model = Transformer(lm_config)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 多卡初始化</span></span><br><span class="line">    num_gpus = torch.cuda.device_count()</span><br><span class="line">    <span class="keyword">if</span> num_gpus &gt; <span class="number">1</span>:</span><br><span class="line">        Logger(<span class="string">f&quot;Using <span class="subst">&#123;num_gpus&#125;</span> GPUs with DataParallel!&quot;</span>)</span><br><span class="line">        model = torch.nn.DataParallel(model)</span><br><span class="line">    </span><br><span class="line">    model = model.to(args.device)</span><br><span class="line">    Logger(<span class="string">f&#x27;LLM总参数量：<span class="subst">&#123;count_parameters(model) / <span class="number">1e6</span>:<span class="number">.3</span>f&#125;</span> 百万&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> model, tokenizer</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    parser = argparse.ArgumentParser(description=<span class="string">&quot;Tiny-LLM Pretraining&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--out_dir&quot;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&quot;output&quot;</span>, <span class="built_in">help</span>=<span class="string">&quot;Output directory&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--epochs&quot;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">1</span>, <span class="built_in">help</span>=<span class="string">&quot;Number of epochs&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--batch_size&quot;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">64</span>, <span class="built_in">help</span>=<span class="string">&quot;Batch size&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--learning_rate&quot;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">2e-4</span>, <span class="built_in">help</span>=<span class="string">&quot;Learning rate&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--device&quot;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>, <span class="built_in">help</span>=<span class="string">&quot;Device to use&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--dtype&quot;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&quot;bfloat16&quot;</span>, <span class="built_in">help</span>=<span class="string">&quot;Data type&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--use_swanlab&quot;</span>, <span class="built_in">type</span>=<span class="built_in">bool</span>, default=<span class="literal">True</span>, <span class="built_in">help</span>=<span class="string">&quot;Use Weights &amp; Biases&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--num_workers&quot;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">8</span>, <span class="built_in">help</span>=<span class="string">&quot;Number of workers for data loading&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--data_path&quot;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&quot;&quot;</span>, <span class="built_in">help</span>=<span class="string">&quot;Path to training data&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--accumulation_steps&quot;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">8</span>, <span class="built_in">help</span>=<span class="string">&quot;Gradient accumulation steps&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--grad_clip&quot;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">1.0</span>, <span class="built_in">help</span>=<span class="string">&quot;Gradient clipping threshold&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--warmup_iters&quot;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">0</span>, <span class="built_in">help</span>=<span class="string">&quot;Number of warmup iterations&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--log_interval&quot;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">100</span>, <span class="built_in">help</span>=<span class="string">&quot;Logging interval&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--save_interval&quot;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">1000</span>, <span class="built_in">help</span>=<span class="string">&quot;Model saving interval&quot;</span>)</span><br><span class="line">    <span class="comment"># 添加多卡参数</span></span><br><span class="line">    parser.add_argument(<span class="string">&quot;--gpus&quot;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&#x27;0,1&#x27;</span>, <span class="built_in">help</span>=<span class="string">&quot;Comma-separated GPU IDs (e.g. &#x27;0,1,2&#x27;)&quot;</span>)</span><br><span class="line"></span><br><span class="line">    args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 设置可见GPU</span></span><br><span class="line">    <span class="keyword">if</span> args.gpus <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        os.environ[<span class="string">&quot;CUDA_VISIBLE_DEVICES&quot;</span>] = args.gpus</span><br><span class="line">        <span class="comment"># 自动设置主设备为第一个GPU</span></span><br><span class="line">        <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">            args.device = <span class="string">&quot;cuda:0&quot;</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            args.device = <span class="string">&quot;cpu&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.use_swanlab:</span><br><span class="line">        swanlab.login(api_key=<span class="string">&#x27;your key&#x27;</span>)</span><br><span class="line">        run = swanlab.init(</span><br><span class="line">            project=<span class="string">&quot;Tiny-LLM&quot;</span>,</span><br><span class="line">            experiment_name=<span class="string">&quot;Pretrain-215M&quot;</span>,</span><br><span class="line">            config=args,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    lm_config = ModelConfig(</span><br><span class="line">        dim=<span class="number">1024</span>,</span><br><span class="line">        n_layers=<span class="number">18</span>,</span><br><span class="line">    )</span><br><span class="line">    max_seq_len = lm_config.max_seq_len</span><br><span class="line">    args.save_dir = os.path.join(args.out_dir)</span><br><span class="line">    os.makedirs(args.save_dir, exist_ok=<span class="literal">True</span>)</span><br><span class="line">    os.makedirs(args.out_dir, exist_ok=<span class="literal">True</span>)</span><br><span class="line">    torch.manual_seed(<span class="number">42</span>)</span><br><span class="line">    device_type = <span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> <span class="string">&quot;cuda&quot;</span> <span class="keyword">in</span> args.device <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span></span><br><span class="line"></span><br><span class="line">    ctx = nullcontext() <span class="keyword">if</span> device_type == <span class="string">&quot;cpu&quot;</span> <span class="keyword">else</span> torch.cuda.amp.autocast()</span><br><span class="line"></span><br><span class="line">    model, tokenizer = init_model()</span><br><span class="line">    </span><br><span class="line">    train_ds = PretrainDataset(args.data_path, tokenizer, max_length=max_seq_len)</span><br><span class="line">    train_loader = DataLoader(</span><br><span class="line">        train_ds,</span><br><span class="line">        batch_size=args.batch_size,</span><br><span class="line">        pin_memory=<span class="literal">True</span>,</span><br><span class="line">        drop_last=<span class="literal">False</span>,</span><br><span class="line">        shuffle=<span class="literal">True</span>,</span><br><span class="line">        num_workers=args.num_workers</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    scaler = torch.cuda.amp.GradScaler(enabled=(args.dtype <span class="keyword">in</span> [<span class="string">&#x27;float16&#x27;</span>, <span class="string">&#x27;bfloat16&#x27;</span>]))</span><br><span class="line">    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)</span><br><span class="line"></span><br><span class="line">    iter_per_epoch = <span class="built_in">len</span>(train_loader)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(args.epochs):</span><br><span class="line">        train_epoch(epoch)</span><br></pre></td></tr></table></figure>

<h3 id="5-3-4-SFT-训练"><a href="#5-3-4-SFT-训练" class="headerlink" title="5.3.4 SFT 训练"></a>5.3.4 SFT 训练</h3><p>SFT 训练和预训练的代码基本一样，只是导入的 Dataset 不一样。在这里我们使用的是 SFTDataset，用于多轮对话的训练。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> platform</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> contextlib <span class="keyword">import</span> nullcontext</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> k_model <span class="keyword">import</span> ModelConfig, Transformer</span><br><span class="line"><span class="keyword">from</span> dataset <span class="keyword">import</span> SFTDataset</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> swanlab</span><br><span class="line"></span><br><span class="line">warnings.filterwarnings(<span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">Logger</span>(<span class="params">content</span>):</span><br><span class="line">    <span class="built_in">print</span>(content)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_lr</span>(<span class="params">it, <span class="built_in">all</span></span>):</span><br><span class="line">    warmup_iters = args.warmup_iters</span><br><span class="line">    lr_decay_iters = <span class="built_in">all</span></span><br><span class="line">    min_lr = args.learning_rate / <span class="number">10</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> it &lt; warmup_iters:</span><br><span class="line">        <span class="keyword">return</span> args.learning_rate * it / warmup_iters</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> it &gt; lr_decay_iters:</span><br><span class="line">        <span class="keyword">return</span> min_lr</span><br><span class="line">    </span><br><span class="line">    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)</span><br><span class="line">    <span class="keyword">assert</span> <span class="number">0</span> &lt;= decay_ratio &lt;= <span class="number">1</span></span><br><span class="line">    coeff = <span class="number">0.5</span> * (<span class="number">1.0</span> + math.cos(math.pi * decay_ratio))</span><br><span class="line">    <span class="keyword">return</span> min_lr + coeff * (args.learning_rate - min_lr)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_epoch</span>(<span class="params">epoch</span>):</span><br><span class="line">    start_time = time.time()</span><br><span class="line">    <span class="keyword">for</span> step, (X, Y, loss_mask) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">        X = X.to(args.device)</span><br><span class="line">        Y = Y.to(args.device)</span><br><span class="line">        loss_mask = loss_mask.to(args.device)</span><br><span class="line"></span><br><span class="line">        lr = get_lr(epoch * iter_per_epoch + step, args.epochs * iter_per_epoch)</span><br><span class="line">        <span class="keyword">for</span> param_group <span class="keyword">in</span> optimizer.param_groups:</span><br><span class="line">            param_group[<span class="string">&#x27;lr&#x27;</span>] = lr</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> ctx:</span><br><span class="line">            out = model(X, Y)</span><br><span class="line">            loss = out.last_loss / args.accumulation_steps</span><br><span class="line">            loss_mask = loss_mask.view(-<span class="number">1</span>)</span><br><span class="line">            loss = torch.<span class="built_in">sum</span>(loss * loss_mask) / loss_mask.<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line">        scaler.scale(loss).backward()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (step + <span class="number">1</span>) % args.accumulation_steps == <span class="number">0</span>:</span><br><span class="line">            scaler.unscale_(optimizer)</span><br><span class="line">            torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)</span><br><span class="line"></span><br><span class="line">            scaler.step(optimizer)</span><br><span class="line">            scaler.update()</span><br><span class="line"></span><br><span class="line">            optimizer.zero_grad(set_to_none=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> step % args.log_interval == <span class="number">0</span>:</span><br><span class="line">            spend_time = time.time() - start_time</span><br><span class="line">            Logger(</span><br><span class="line">                <span class="string">&#x27;Epoch:[&#123;&#125;/&#123;&#125;](&#123;&#125;/&#123;&#125;) loss:&#123;:.3f&#125; lr:&#123;:.7f&#125; epoch_Time:&#123;&#125;min:&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">                    epoch + <span class="number">1</span>,</span><br><span class="line">                    args.epochs,</span><br><span class="line">                    step,</span><br><span class="line">                    iter_per_epoch,</span><br><span class="line">                    loss.item() * args.accumulation_steps,</span><br><span class="line">                    optimizer.param_groups[-<span class="number">1</span>][<span class="string">&#x27;lr&#x27;</span>],</span><br><span class="line">                    spend_time / (step + <span class="number">1</span>) * iter_per_epoch // <span class="number">60</span> - spend_time // <span class="number">60</span>))</span><br><span class="line">            <span class="keyword">if</span> args.use_swanlab:</span><br><span class="line">                swanlab.log(&#123;</span><br><span class="line">                    <span class="string">&quot;loss&quot;</span>: loss.item() * args.accumulation_steps,</span><br><span class="line">                    <span class="string">&quot;lr&quot;</span>: optimizer.param_groups[-<span class="number">1</span>][<span class="string">&#x27;lr&#x27;</span>]</span><br><span class="line">                &#125;)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (step + <span class="number">1</span>) % args.save_interval == <span class="number">0</span>:</span><br><span class="line">            model.<span class="built_in">eval</span>()</span><br><span class="line">            ckp = <span class="string">f&#x27;<span class="subst">&#123;args.save_dir&#125;</span>/sft_dim<span class="subst">&#123;lm_config.dim&#125;</span>_layers<span class="subst">&#123;lm_config.n_layers&#125;</span>_vocab_size<span class="subst">&#123;lm_config.vocab_size&#125;</span>.pth&#x27;</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 处理多卡保存</span></span><br><span class="line">            state_dict = model.module.state_dict() <span class="keyword">if</span> <span class="built_in">isinstance</span>(model, torch.nn.DataParallel) <span class="keyword">else</span> model.state_dict()</span><br><span class="line">            torch.save(state_dict, ckp)</span><br><span class="line">            model.train()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> (step + <span class="number">1</span>) % <span class="number">20000</span> == <span class="number">0</span>:</span><br><span class="line">            model.<span class="built_in">eval</span>()</span><br><span class="line">            ckp = <span class="string">f&#x27;<span class="subst">&#123;args.save_dir&#125;</span>/sft_dim<span class="subst">&#123;lm_config.dim&#125;</span>_layers<span class="subst">&#123;lm_config.n_layers&#125;</span>_vocab_size<span class="subst">&#123;lm_config.vocab_size&#125;</span>_step<span class="subst">&#123;step+<span class="number">1</span>&#125;</span>.pth&#x27;</span></span><br><span class="line"></span><br><span class="line">            state_dict = model.module.state_dict() <span class="keyword">if</span> <span class="built_in">isinstance</span>(model, torch.nn.DataParallel) <span class="keyword">else</span> model.state_dict()</span><br><span class="line">            torch.save(state_dict, ckp)</span><br><span class="line">            model.train()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_model</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">count_parameters</span>(<span class="params">model</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">sum</span>(p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters() <span class="keyword">if</span> p.requires_grad)</span><br><span class="line"></span><br><span class="line">    tokenizer = AutoTokenizer.from_pretrained(<span class="string">&#x27;./tokenizer_k/&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    model = Transformer(lm_config)</span><br><span class="line"></span><br><span class="line">    ckp = <span class="string">&#x27;./base_monkey_215M/pretrain_1024_18_6144.pth&#x27;</span></span><br><span class="line">    state_dict = torch.load(ckp, map_location=args.device)</span><br><span class="line">    unwanted_prefix = <span class="string">&#x27;_orig_mod.&#x27;</span></span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> <span class="built_in">list</span>(state_dict.items()):</span><br><span class="line">        <span class="keyword">if</span> k.startswith(unwanted_prefix):</span><br><span class="line">            state_dict[k[<span class="built_in">len</span>(unwanted_prefix):]] = state_dict.pop(k)</span><br><span class="line">    model.load_state_dict(state_dict, strict=<span class="literal">False</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 多卡初始化</span></span><br><span class="line">    num_gpus = torch.cuda.device_count()</span><br><span class="line">    <span class="keyword">if</span> num_gpus &gt; <span class="number">1</span>:</span><br><span class="line">        Logger(<span class="string">f&quot;Using <span class="subst">&#123;num_gpus&#125;</span> GPUs with DataParallel!&quot;</span>)</span><br><span class="line">        model = torch.nn.DataParallel(model)</span><br><span class="line">    </span><br><span class="line">    model = model.to(args.device)</span><br><span class="line">    Logger(<span class="string">f&#x27;LLM总参数量：<span class="subst">&#123;count_parameters(model) / <span class="number">1e6</span>:<span class="number">.3</span>f&#125;</span> 百万&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> model, tokenizer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    parser = argparse.ArgumentParser(description=<span class="string">&quot;Tiny-LLM Pretraining&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--out_dir&quot;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&quot;output&quot;</span>, <span class="built_in">help</span>=<span class="string">&quot;Output directory&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--epochs&quot;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">1</span>, <span class="built_in">help</span>=<span class="string">&quot;Number of epochs&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--batch_size&quot;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">64</span>, <span class="built_in">help</span>=<span class="string">&quot;Batch size&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--learning_rate&quot;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">2e-4</span>, <span class="built_in">help</span>=<span class="string">&quot;Learning rate&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--device&quot;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>, <span class="built_in">help</span>=<span class="string">&quot;Device to use&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--dtype&quot;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&quot;bfloat16&quot;</span>, <span class="built_in">help</span>=<span class="string">&quot;Data type&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--use_swanlab&quot;</span>, <span class="built_in">type</span>=<span class="built_in">bool</span>, default=<span class="literal">True</span>, <span class="built_in">help</span>=<span class="string">&quot;Use Weights &amp; Biases&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--num_workers&quot;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">4</span>, <span class="built_in">help</span>=<span class="string">&quot;Number of workers for data loading&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--data_path&quot;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&quot;&quot;</span>, <span class="built_in">help</span>=<span class="string">&quot;Path to training data&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--accumulation_steps&quot;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">4</span>, <span class="built_in">help</span>=<span class="string">&quot;Gradient accumulation steps&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--grad_clip&quot;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">1.0</span>, <span class="built_in">help</span>=<span class="string">&quot;Gradient clipping threshold&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--warmup_iters&quot;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">0</span>, <span class="built_in">help</span>=<span class="string">&quot;Number of warmup iterations&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--log_interval&quot;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">100</span>, <span class="built_in">help</span>=<span class="string">&quot;Logging interval&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--save_interval&quot;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">1000</span>, <span class="built_in">help</span>=<span class="string">&quot;Model saving interval&quot;</span>)</span><br><span class="line">    <span class="comment"># 添加多卡参数</span></span><br><span class="line">    parser.add_argument(<span class="string">&quot;--gpus&quot;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&#x27;0,1&#x27;</span>, <span class="built_in">help</span>=<span class="string">&quot;Comma-separated GPU IDs (e.g. &#x27;0,1,2&#x27;)&quot;</span>)</span><br><span class="line"></span><br><span class="line">    args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 设置可见GPU</span></span><br><span class="line">    <span class="keyword">if</span> args.gpus <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        os.environ[<span class="string">&quot;CUDA_VISIBLE_DEVICES&quot;</span>] = args.gpus</span><br><span class="line">        <span class="comment"># 自动设置主设备为第一个GPU</span></span><br><span class="line">        <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">            args.device = <span class="string">&quot;cuda:0&quot;</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            args.device = <span class="string">&quot;cpu&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.use_swanlab:</span><br><span class="line">        swanlab.login(api_key=<span class="string">&#x27;your key&#x27;</span>)</span><br><span class="line">        run = swanlab.init(</span><br><span class="line">            project=<span class="string">&quot;Tiny-LLM&quot;</span>,</span><br><span class="line">            experiment_name=<span class="string">&quot;BelleGropu-sft-215M&quot;</span>,</span><br><span class="line">            config=args,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    lm_config = ModelConfig(</span><br><span class="line">        dim=<span class="number">1024</span>,</span><br><span class="line">        n_layers=<span class="number">18</span>,</span><br><span class="line">    )</span><br><span class="line">    max_seq_len = lm_config.max_seq_len</span><br><span class="line">    args.save_dir = os.path.join(args.out_dir)</span><br><span class="line">    os.makedirs(args.save_dir, exist_ok=<span class="literal">True</span>)</span><br><span class="line">    os.makedirs(args.out_dir, exist_ok=<span class="literal">True</span>)</span><br><span class="line">    torch.manual_seed(<span class="number">42</span>)</span><br><span class="line">    device_type = <span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> <span class="string">&quot;cuda&quot;</span> <span class="keyword">in</span> args.device <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span></span><br><span class="line"></span><br><span class="line">    ctx = nullcontext() <span class="keyword">if</span> device_type == <span class="string">&quot;cpu&quot;</span> <span class="keyword">else</span> torch.cuda.amp.autocast()</span><br><span class="line"></span><br><span class="line">    model, tokenizer = init_model()</span><br><span class="line">    </span><br><span class="line">    train_ds = SFTDataset(args.data_path, tokenizer, max_length=max_seq_len)</span><br><span class="line">    train_loader = DataLoader(</span><br><span class="line">        train_ds,</span><br><span class="line">        batch_size=args.batch_size,</span><br><span class="line">        pin_memory=<span class="literal">True</span>,</span><br><span class="line">        drop_last=<span class="literal">False</span>,</span><br><span class="line">        shuffle=<span class="literal">True</span>,</span><br><span class="line">        num_workers=args.num_workers</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    scaler = torch.cuda.amp.GradScaler(enabled=(args.dtype <span class="keyword">in</span> [<span class="string">&#x27;float16&#x27;</span>, <span class="string">&#x27;bfloat16&#x27;</span>]))</span><br><span class="line">    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)</span><br><span class="line"></span><br><span class="line">    iter_per_epoch = <span class="built_in">len</span>(train_loader)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(args.epochs):</span><br><span class="line">        train_epoch(epoch)</span><br></pre></td></tr></table></figure>


<h3 id="5-3-4-使用模型生成文本"><a href="#5-3-4-使用模型生成文本" class="headerlink" title="5.3.4 使用模型生成文本"></a>5.3.4 使用模型生成文本</h3><p>在模型训练完成后，会在<code>output</code>目录下生成模型文件，这个文件就是我们训练好的模型。我们可以使用以下命令生成文本。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python model_sample.py</span><br></pre></td></tr></table></figure>

<p>我们来看下<code>model_sample.py</code>文件中的代码，这个文件中定义了一个<code>TextGenerator</code>类，用于生成文本。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TextGenerator</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, </span></span><br><span class="line"><span class="params">                 checkpoint=<span class="string">&#x27;out/SkyWork_pretrain_768_12_6144.pth&#x27;</span>,  <span class="comment"># 模型检查点路径</span></span></span><br><span class="line"><span class="params">                 tokenizer_model_path=<span class="string">&#x27;./tokenizer_k/&#x27;</span>,  <span class="comment"># 分词器模型路径</span></span></span><br><span class="line"><span class="params">                 seed=<span class="number">42</span>,  <span class="comment"># 随机种子，确保可重复性</span></span></span><br><span class="line"><span class="params">                 device=<span class="literal">None</span>,  <span class="comment"># 设备，优先使用 CUDA，如果没有可用的 CUDA，则使用 CPU</span></span></span><br><span class="line"><span class="params">                 dtype=<span class="string">&quot;bfloat16&quot;</span></span>):  <span class="comment"># 数据类型，默认为 float32，可以选择 float16 或 bfloat16</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        初始化 TextGenerator 类，加载模型、设置设备和分词器等。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 模型加载配置</span></span><br><span class="line">        <span class="variable language_">self</span>.checkpoint = checkpoint  <span class="comment"># 保存的模型检查点路径</span></span><br><span class="line">        <span class="variable language_">self</span>.tokenizer_model_path = tokenizer_model_path  <span class="comment"># 分词器模型文件路径</span></span><br><span class="line">        <span class="variable language_">self</span>.seed = seed  <span class="comment"># 随机数种子，用于生成的可重复性</span></span><br><span class="line">        <span class="variable language_">self</span>.device = device <span class="keyword">or</span> (<span class="string">&#x27;cuda:0&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)  <span class="comment"># 根据硬件条件选择设备</span></span><br><span class="line">        <span class="variable language_">self</span>.dtype = dtype  <span class="comment"># 模型的浮点数类型</span></span><br><span class="line">        <span class="variable language_">self</span>.device_type = <span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> <span class="string">&#x27;cuda&#x27;</span> <span class="keyword">in</span> <span class="variable language_">self</span>.device <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>  <span class="comment"># 判断当前设备是否为 CUDA</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 设置随机种子，确保生成的可重复性</span></span><br><span class="line">        torch.manual_seed(seed)  <span class="comment"># 设置 CPU 随机种子</span></span><br><span class="line">        torch.cuda.manual_seed(seed)  <span class="comment"># 设置 CUDA 随机种子</span></span><br><span class="line">        torch.backends.cuda.matmul.allow_tf32 = <span class="literal">True</span>  <span class="comment"># 允许 CUDA 使用 TF32 精度进行矩阵乘法运算</span></span><br><span class="line">        torch.backends.cudnn.allow_tf32 = <span class="literal">True</span>  <span class="comment"># 允许 cuDNN 使用 TF32 精度加速</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 根据 dtype 选择适当的自动混合精度上下文</span></span><br><span class="line">        ptdtype = &#123;<span class="string">&#x27;float32&#x27;</span>: torch.float32, <span class="string">&#x27;bfloat16&#x27;</span>: torch.bfloat16, <span class="string">&#x27;float16&#x27;</span>: torch.float16&#125;[<span class="variable language_">self</span>.dtype]</span><br><span class="line">        <span class="variable language_">self</span>.ctx = nullcontext() <span class="keyword">if</span> <span class="variable language_">self</span>.device_type == <span class="string">&#x27;cpu&#x27;</span> <span class="keyword">else</span> torch.amp.autocast(device_type=<span class="variable language_">self</span>.device_type, dtype=ptdtype)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 加载模型检查点文件</span></span><br><span class="line">        checkpoint_dict = torch.load(<span class="variable language_">self</span>.checkpoint, map_location=<span class="variable language_">self</span>.device)  <span class="comment"># 加载模型参数 # 初始化模型参数</span></span><br><span class="line">        <span class="variable language_">self</span>.model = Transformer(ModelConfig(dim=<span class="number">1024</span>, n_layers=<span class="number">18</span>))  <span class="comment"># 实例化 Transformer 模型</span></span><br><span class="line">        sunwanted_prefix = <span class="string">&#x27;_orig_mod.&#x27;</span></span><br><span class="line">        <span class="keyword">for</span> k, v <span class="keyword">in</span> <span class="built_in">list</span>(checkpoint_dict.items()):</span><br><span class="line">            <span class="keyword">if</span> k.startswith(sunwanted_prefix):</span><br><span class="line">                checkpoint_dict[k[<span class="built_in">len</span>(sunwanted_prefix):]] = checkpoint_dict.pop(k)</span><br><span class="line">        <span class="variable language_">self</span>.model.load_state_dict(checkpoint_dict, strict=<span class="literal">False</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算模型参数量</span></span><br><span class="line">        num_params = <span class="built_in">sum</span>(p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> <span class="variable language_">self</span>.model.parameters() <span class="keyword">if</span> p.requires_grad)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Model has <span class="subst">&#123;num_params / <span class="number">1e6</span>:<span class="number">.3</span>f&#125;</span> M parameters.&quot;</span>)</span><br><span class="line">        <span class="comment"># 设置模型为评估模式（evaluation mode），防止训练模式下的 dropout 等操作影响结果</span></span><br><span class="line">        <span class="variable language_">self</span>.model.<span class="built_in">eval</span>()</span><br><span class="line">        <span class="comment"># 将模型放置到正确的设备上（GPU 或 CPU）</span></span><br><span class="line">        <span class="variable language_">self</span>.model.to(<span class="variable language_">self</span>.device)</span><br><span class="line">        <span class="comment"># 初始化分词器</span></span><br><span class="line">        <span class="variable language_">self</span>.tokenizer = AutoTokenizer.from_pretrained(<span class="variable language_">self</span>.tokenizer_model_path)  <span class="comment"># 根据指定的路径加载分词器</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">chat_template</span>(<span class="params">self, prompt</span>):</span><br><span class="line">        message = [</span><br><span class="line">            &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;你是一个AI助手。&quot;</span>&#125;,</span><br><span class="line">            &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: prompt&#125;</span><br><span class="line">        ]</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.tokenizer.apply_chat_template(message, tokenize=<span class="literal">False</span>, add_generation_prompt=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sft_sample</span>(<span class="params">self, </span></span><br><span class="line"><span class="params">               start=<span class="string">&quot;Hello!&quot;</span>,  <span class="comment"># 生成文本的起始提示词，可以是任意字符串</span></span></span><br><span class="line"><span class="params">               num_samples=<span class="number">3</span>,  <span class="comment"># 生成样本的数量，默认生成 3 个样本</span></span></span><br><span class="line"><span class="params">               max_new_tokens=<span class="number">256</span>,  <span class="comment"># 每个样本生成的最大 token 数，默认最多生成 256 个 token</span></span></span><br><span class="line"><span class="params">               temperature=<span class="number">0.7</span>,  <span class="comment"># 控制生成的随机性，1.0 为标准，值越大越随机</span></span></span><br><span class="line"><span class="params">               top_k=<span class="number">300</span></span>):  <span class="comment"># 保留概率最高的 top_k 个 token，限制生成时的选择范围</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        根据给定的起始文本生成样本。</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        :param start: 生成文本的起始提示词</span></span><br><span class="line"><span class="string">        :param num_samples: 要生成的文本样本数</span></span><br><span class="line"><span class="string">        :param max_new_tokens: 每个样本生成的最大 token 数</span></span><br><span class="line"><span class="string">        :param temperature: 控制生成的随机性，值越小生成越确定，值越大生成越随机</span></span><br><span class="line"><span class="string">        :param top_k: 限制生成时选择的 token 范围</span></span><br><span class="line"><span class="string">        :return: 生成的文本样本列表</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        start = <span class="variable language_">self</span>.chat_template(start)</span><br><span class="line">        <span class="comment"># 将起始文本编码为 token id 序列</span></span><br><span class="line">        start_ids = <span class="variable language_">self</span>.tokenizer(start).data[<span class="string">&#x27;input_ids&#x27;</span>]</span><br><span class="line">        <span class="comment"># print(&#x27;start_ids:&#x27;, start_ids)</span></span><br><span class="line">        x = (torch.tensor(start_ids, dtype=torch.long, device=<span class="variable language_">self</span>.device)[<span class="literal">None</span>, ...])  <span class="comment"># 将编码后的 token id 转为 PyTorch 张量</span></span><br><span class="line">        generated_texts = []  <span class="comment"># 用于保存生成的文本样本</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():  <span class="comment"># 禁用梯度计算，提升效率</span></span><br><span class="line">            <span class="keyword">with</span> <span class="variable language_">self</span>.ctx:  <span class="comment"># 进入自动混合精度的上下文（如果是 GPU 并使用 float16 时）</span></span><br><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(num_samples):  <span class="comment"># 循环生成指定数量的样本</span></span><br><span class="line">                    y = <span class="variable language_">self</span>.model.generate(x, <span class="variable language_">self</span>.tokenizer.eos_token_id, max_new_tokens, temperature=temperature, top_k=top_k)  <span class="comment"># 生成文本</span></span><br><span class="line">                    generated_texts.append(<span class="variable language_">self</span>.tokenizer.decode(y[<span class="number">0</span>].tolist()))  <span class="comment"># 解码生成的 token 序列为可读文本</span></span><br><span class="line">        <span class="keyword">return</span> generated_texts  <span class="comment"># 返回生成的文本样本</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">pretrain_sample</span>(<span class="params">self, </span></span><br><span class="line"><span class="params">               start=<span class="string">&quot;Hello!&quot;</span>,  <span class="comment"># 生成文本的起始提示词，可以是任意字符串</span></span></span><br><span class="line"><span class="params">               num_samples=<span class="number">3</span>,  <span class="comment"># 生成样本的数量，默认生成 3 个样本</span></span></span><br><span class="line"><span class="params">               max_new_tokens=<span class="number">256</span>,  <span class="comment"># 每个样本生成的最大 token 数，默认最多生成 256 个 token</span></span></span><br><span class="line"><span class="params">               temperature=<span class="number">0.7</span>,  <span class="comment"># 控制生成的随机性，1.0 为标准，值越大越随机</span></span></span><br><span class="line"><span class="params">               top_k=<span class="number">300</span></span>):  <span class="comment"># 保留概率最高的 top_k 个 token，限制生成时的选择范围</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        根据给定的起始文本生成样本。</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        :param start: 生成文本的起始提示词</span></span><br><span class="line"><span class="string">        :param num_samples: 要生成的文本样本数</span></span><br><span class="line"><span class="string">        :param max_new_tokens: 每个样本生成的最大 token 数</span></span><br><span class="line"><span class="string">        :param temperature: 控制生成的随机性，值越小生成越确定，值越大生成越随机</span></span><br><span class="line"><span class="string">        :param top_k: 限制生成时选择的 token 范围</span></span><br><span class="line"><span class="string">        :return: 生成的文本样本列表</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 如果 start 是以 &#x27;FILE:&#x27; 开头，表示从文件中读取起始文本</span></span><br><span class="line">        <span class="keyword">if</span> start.startswith(<span class="string">&#x27;FILE:&#x27;</span>):</span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(start[<span class="number">5</span>:], <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">                start = f.read()  <span class="comment"># 读取文件内容作为起始文本</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 将起始文本编码为 token id 序列</span></span><br><span class="line">        start_ids = <span class="variable language_">self</span>.tokenizer(start).data[<span class="string">&#x27;input_ids&#x27;</span>]</span><br><span class="line">        <span class="comment"># print(&#x27;start_ids:&#x27;, start_ids)</span></span><br><span class="line">        x = (torch.tensor(start_ids, dtype=torch.long, device=<span class="variable language_">self</span>.device)[<span class="literal">None</span>, ...])  <span class="comment"># 将编码后的 token id 转为 PyTorch 张量</span></span><br><span class="line">        <span class="comment"># print(x.shape)</span></span><br><span class="line">        generated_texts = []  <span class="comment"># 用于保存生成的文本样本</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():  <span class="comment"># 禁用梯度计算，提升效率</span></span><br><span class="line">            <span class="keyword">with</span> <span class="variable language_">self</span>.ctx:  <span class="comment"># 进入自动混合精度的上下文（如果是 GPU 并使用 float16 时）</span></span><br><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(num_samples):  <span class="comment"># 循环生成指定数量的样本</span></span><br><span class="line">                    y = <span class="variable language_">self</span>.model.generate(x, max_new_tokens=max_new_tokens, temperature=temperature, top_k=top_k)  <span class="comment"># 生成文本</span></span><br><span class="line">                    generated_texts.append(<span class="variable language_">self</span>.tokenizer.decode(y[<span class="number">0</span>].tolist()))  <span class="comment"># 解码生成的 token 序列为可读文本</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> generated_texts  <span class="comment"># 返回生成的文本样本</span></span><br></pre></td></tr></table></figure>

<p>最后我们来看一下模型输出的结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">------------------- SFT Sample ------------------- </span><br><span class="line"></span><br><span class="line">Model has 215.127 M parameters.</span><br><span class="line"></span><br><span class="line">Sample 1:</span><br><span class="line">Question: 你好呀 </span><br><span class="line">AI answer: 你好!有什么我可以帮你的吗?</span><br><span class="line">--------------------</span><br><span class="line"></span><br><span class="line">Sample 2:</span><br><span class="line">Question: 中国的首都是哪里？ </span><br><span class="line">AI answer: 中国的首都是北京。</span><br><span class="line">--------------------</span><br><span class="line"></span><br><span class="line">Sample 3:</span><br><span class="line">Question: 1+1等于多少？ </span><br><span class="line">AI answer: 1+1等于2。</span><br><span class="line">--------------------</span><br><span class="line">------------------- Pretrain Sample ------------------- </span><br><span class="line"></span><br><span class="line">Model has 215.127 M parameters.</span><br><span class="line"></span><br><span class="line">Sample 1:</span><br><span class="line">&lt;|im_start|&gt;北京大学是中国最早建立的研究型大学之一,是我国最早设置研究生院的高校之一,是第一、二国教育委员会师资培训基地;北京大学是第一、二所国立大学,其校名与北京大学相同。</span><br><span class="line">北京大学录取标准:本科三批1万元,本科一批1万元,本科一批2000元,专科一批2000元,高中起点:非本科一批</span><br><span class="line">--------------------</span><br><span class="line"></span><br><span class="line">Sample 2:</span><br><span class="line">&lt;|im_start|&gt;中国矿业大学（北京）地球科学与测绘工程学院副教授黄河流域地质学科带头人古建平教授为大家介绍世界地质变化的概念及工作经验。</span><br><span class="line">古建平教授介绍了最近几年的植物学和地质学的基本概念,尤其是树都黄河、松涛、暗河等都有地质学工作者的身影,其中树都黄河以分布面积最大,是树都黄河中华砂岩公园的主景区。</span><br><span class="line">黄河内蒙古</span><br><span class="line">--------------------</span><br></pre></td></tr></table></figure>

<p>到这里，我们的模型就训绽完成了，恭喜你训练了一个属于你自己的大模型。</p>
<blockquote>
<p>大家在训练的时候可以将 batch 调的低一些，这样可以减少显存的占用，避免显存不足的问题。当然这样会增加训练时间，可以根据自己的显卡显存大小来调整 batch 的大小。实测 Pretrain batch 为 4 的情况下只需要 7G 显存，训练时长预计 533 小时。作者是在 4卡A100上进行训练的，预训练一共耗时26小时，SFT 阶段在 BelleGroup 350万条中文指令训练 4 小时。</p>
</blockquote>
<p><strong>参考资料</strong></p>
<p>[1] Andrej Karpathy. (2023). <em>llama2.c: Fullstack Llama 2 LLM solution in pure C</em>. GitHub repository. <a target="_blank" rel="noopener" href="https://github.com/karpathy/llama2.c">https://github.com/karpathy/llama2.c</a>  </p>
<p>[2] Andrej Karpathy. (2023). <em>llm.c: GPT-2&#x2F;GPT-3 pretraining in C&#x2F;CUDA</em>. GitHub repository. <a target="_blank" rel="noopener" href="https://github.com/karpathy/llm.c">https://github.com/karpathy/llm.c</a>  </p>
<p>[3] Hugging Face. (2023). <em>Tokenizers documentation</em>. <a target="_blank" rel="noopener" href="https://huggingface.co/docs/tokenizers/index">https://huggingface.co/docs/tokenizers/index</a>  </p>
<p>[4] Skywork Team. (2023). <em>SkyPile-150B: A large-scale bilingual dataset</em>. Hugging Face dataset. <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/Skywork/SkyPile-150B">https://huggingface.co/datasets/Skywork/SkyPile-150B</a>  </p>
<p>[5] BelleGroup. (2022). <em>train_3.5M_CN: Chinese dialogue dataset</em>. Hugging Face dataset. <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/BelleGroup/train_3.5M_CN">https://huggingface.co/datasets/BelleGroup/train_3.5M_CN</a>  </p>
<p>[6] Jingyao Gong. (2023). <em>minimind: Minimalist LLM implementation</em>. GitHub repository. <a target="_blank" rel="noopener" href="https://github.com/jingyaogong/minimind">https://github.com/jingyaogong/minimind</a>  </p>
<p>[7] Mobvoi. (2023). <em>seq-monkey-data: Llama2 training&#x2F;inference data</em>. GitHub repository. <a target="_blank" rel="noopener" href="https://github.com/mobvoi/seq-monkey-data">https://github.com/mobvoi/seq-monkey-data</a></p>

        
      </div>

         
    </div>
    
     
  </div>
  
    
<nav id="article-nav">
  <a class="article-nav-btn left "
    
      href="/2025/06/09/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/"
      title="大模型应用"
     >
    <i class="fa-solid fa-angle-left"></i>
    <p class="title-text">
      
        大模型应用
        
    </p>
  </a>
  <a class="article-nav-btn right "
    
      href="/2025/06/09/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B%E5%AE%9E%E8%B7%B5/"
      title="大模型训练流程实践"
     >

    <p class="title-text">
      
        大模型训练流程实践
        
    </p>
    <i class="fa-solid fa-angle-right"></i>
  </a>
</nav>


  
</article>






    </div>
    <div id="footer-wrapper">
      <footer id="footer">
  
  <div id="footer-info" class="inner">
    
    &copy; 2025 Kouir Wu<br>
    Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> & Theme <a target="_blank" rel="noopener" href="https://github.com/saicaca/hexo-theme-vivia">Vivia</a>
  </div>
</footer>

    </div>
    <div class="back-to-top-wrapper">
    <button id="back-to-top-btn" class="back-to-top-btn hide" onclick="topFunction()">
        <i class="fa-solid fa-angle-up"></i>
    </button>
</div>

<script>
    function topFunction() {
        window.scroll({ top: 0, behavior: 'smooth' });
    }
    let btn = document.getElementById('back-to-top-btn');
    function scrollFunction() {
        if (document.body.scrollTop > 600 || document.documentElement.scrollTop > 600) {
            btn.classList.remove('hide')
        } else {
            btn.classList.add('hide')
        }
    }
    window.onscroll = function() {
        scrollFunction();
    }
</script>

  </div>
  <script src="/js/light-dark-switch.js"></script>
</body>
</html>

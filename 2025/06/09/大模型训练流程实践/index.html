<!DOCTYPE html>


<html theme="dark" showBanner="true" hasBanner="true" > 
<link href="https://cdn.staticfile.org/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet">
<link href="https://cdn.staticfile.org/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet">
<link href="https://cdn.staticfile.org/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet">
<script src="/js/color.global.min.js" ></script>
<script src="/js/load-settings.js" ></script>
<head>
  <meta charset="utf-8">
  
  
  

  
  <title>大模型训练流程实践 | 季禾子寒舍</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="preload" href="/css/fonts/Roboto-Regular.ttf" as="font" type="font/ttf" crossorigin="anonymous">
  <link rel="preload" href="/css/fonts/Roboto-Bold.ttf" as="font" type="font/ttf" crossorigin="anonymous">

  <meta name="description" content="大模型训练流程实践6.1 模型预训练在上一章，我们逐步拆解了 LLM 的模型结构及训练过程，从零手写实现了 LLaMA 模型结构及 Pretrain、SFT 全流程，更深入地理解了 LLM 的模型原理及训练细节。但是，在实际应用中，手写实现的 LLM 训练存在以下问题：  手写实现 LLM 结构工作量大，难以实时跟进最新模型的结构创新； 从零实现的 LLM 训练无法较好地实现多卡分布式训练，训练效">
<meta property="og:type" content="article">
<meta property="og:title" content="大模型训练流程实践">
<meta property="og:url" content="https://kouirwu.github.io/2025/06/09/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B%E5%AE%9E%E8%B7%B5/index.html">
<meta property="og:site_name" content="季禾子寒舍">
<meta property="og:description" content="大模型训练流程实践6.1 模型预训练在上一章，我们逐步拆解了 LLM 的模型结构及训练过程，从零手写实现了 LLaMA 模型结构及 Pretrain、SFT 全流程，更深入地理解了 LLM 的模型原理及训练细节。但是，在实际应用中，手写实现的 LLM 训练存在以下问题：  手写实现 LLM 结构工作量大，难以实时跟进最新模型的结构创新； 从零实现的 LLM 训练无法较好地实现多卡分布式训练，训练效">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://kouirwu.github.io/2025/06/09/images/6-images/1-1.png">
<meta property="og:image" content="https://kouirwu.github.io/2025/06/09/images/6-images/1-2.png">
<meta property="og:image" content="https://kouirwu.github.io/2025/06/09/images/6-images/1-3.png">
<meta property="og:image" content="https://kouirwu.github.io/2025/06/09/images/6-images/1-4.png">
<meta property="og:image" content="https://kouirwu.github.io/2025/06/09/images/6-images/1-5.png">
<meta property="og:image" content="https://kouirwu.github.io/2025/06/09/images/6-images/1-6.png">
<meta property="og:image" content="https://kouirwu.github.io/2025/06/09/images/6-images/1-7.png">
<meta property="og:image" content="https://kouirwu.github.io/2025/06/09/images/6-images/3-1.png">
<meta property="og:image" content="https://kouirwu.github.io/2025/06/09/images/6-images/3-2.jpg">
<meta property="article:published_time" content="2025-06-09T09:03:32.000Z">
<meta property="article:modified_time" content="2025-06-09T09:07:19.478Z">
<meta property="article:author" content="Kouir Wu">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://kouirwu.github.io/2025/06/09/images/6-images/1-1.png">
  
    <link rel="alternate" href="/atom.xml" title="季禾子寒舍" type="application/atom+xml">
  
  
    <link rel="icon" media="(prefers-color-scheme: light)" href="/images/favicon-light-32.png" sizes="32x32">
    <link rel="icon" media="(prefers-color-scheme: light)" href="/images/favicon-light-128.png" sizes="128x128">
    <link rel="icon" media="(prefers-color-scheme: light)" href="/images/favicon-light-180.png" sizes="180x180">
    <link rel="icon" media="(prefers-color-scheme: light)" href="/images/favicon-light-192.png" sizes="192x192">
    <link rel="icon" media="(prefers-color-scheme: dark)" href="/images/favicon-dark-32.png" sizes="32x32">
    <link rel="icon" media="(prefers-color-scheme: dark)" href="/images/favicon-dark-128.png" sizes="128x128">
    <link rel="icon" media="(prefers-color-scheme: dark)" href="/images/favicon-dark-180.png" sizes="180x180">
    <link rel="icon" media="(prefers-color-scheme: dark)" href="/images/favicon-dark-192.png" sizes="192x192">
  
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  
  
    
<div id="banner" class="">
  <img src="/images/background.jpg" itemprop="image">
  <div id="banner-dim"></div>
</div>
 
   
  <div id="main-grid" class="  ">
    <div id="nav" class=""  >
      <navbar id="navbar">
  <nav id="title-nav">
    <a href="/">
      <div id="vivia-logo">
        <div class="dot"></div>
        <div class="dot"></div>
        <div class="dot"></div>
        <div class="dot"></div>
      </div>
      <div>季禾子寒舍 </div>
    </a>
  </nav>
  <nav id="main-nav">
    
      <a class="main-nav-link" href="/">主页</a>
    
      <a class="main-nav-link" href="/leetcode">Leetcode指南</a>
    
      <a class="main-nav-link" href="/machine_learning">机器学习</a>
    
      <a class="main-nav-link" href="/archives">归档</a>
    
      <a class="main-nav-link" href="/about">关于我</a>
    
  </nav>
  <nav id="sub-nav">
    <a id="theme-btn" class="nav-icon">
      <span class="light-mode-icon"><svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" width="20"><path d="M438.5-829.913v-48q0-17.452 11.963-29.476 11.964-12.024 29.326-12.024 17.363 0 29.537 12.024t12.174 29.476v48q0 17.452-11.963 29.476-11.964 12.024-29.326 12.024-17.363 0-29.537-12.024T438.5-829.913Zm0 747.826v-48q0-17.452 11.963-29.476 11.964-12.024 29.326-12.024 17.363 0 29.537 12.024t12.174 29.476v48q0 17.452-11.963 29.476-11.964 12.024-29.326 12.024-17.363 0-29.537-12.024T438.5-82.087ZM877.913-438.5h-48q-17.452 0-29.476-11.963-12.024-11.964-12.024-29.326 0-17.363 12.024-29.537t29.476-12.174h48q17.452 0 29.476 11.963 12.024 11.964 12.024 29.326 0 17.363-12.024 29.537T877.913-438.5Zm-747.826 0h-48q-17.452 0-29.476-11.963-12.024-11.964-12.024-29.326 0-17.363 12.024-29.537T82.087-521.5h48q17.452 0 29.476 11.963 12.024 11.964 12.024 29.326 0 17.363-12.024 29.537T130.087-438.5Zm660.174-290.87-34.239 32q-12.913 12.674-29.565 12.174-16.653-.5-29.327-13.174-12.674-12.673-12.554-28.826.12-16.152 12.794-28.826l33-35q12.913-12.674 30.454-12.674t30.163 12.847q12.709 12.846 12.328 30.826-.38 17.98-13.054 30.653ZM262.63-203.978l-32 34q-12.913 12.674-30.454 12.674t-30.163-12.847q-12.709-12.846-12.328-30.826.38-17.98 13.054-30.653l33.239-31q12.913-12.674 29.565-12.174 16.653.5 29.327 13.174 12.674 12.673 12.554 28.826-.12 16.152-12.794 28.826Zm466.74 33.239-32-33.239q-12.674-12.913-12.174-29.565.5-16.653 13.174-29.327 12.673-12.674 28.826-13.054 16.152-.38 28.826 12.294l35 33q12.674 12.913 12.674 30.454t-12.847 30.163q-12.846 12.709-30.826 12.328-17.98-.38-30.653-13.054ZM203.978-697.37l-34-33q-12.674-12.913-13.174-29.945-.5-17.033 12.174-29.707t31.326-13.293q18.653-.62 31.326 13.054l32 34.239q11.674 12.913 11.174 29.565-.5 16.653-13.174 29.327-12.673 12.674-28.826 12.554-16.152-.12-28.826-12.794ZM480-240q-100 0-170-70t-70-170q0-100 70-170t170-70q100 0 170 70t70 170q0 100-70 170t-170 70Zm-.247-82q65.703 0 111.475-46.272Q637-414.544 637-480.247t-45.525-111.228Q545.95-637 480.247-637t-111.475 45.525Q323-545.95 323-480.247t45.525 111.975Q414.05-322 479.753-322ZM481-481Z"/></svg></span>
      <span class="dark-mode-icon"><svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" width="20"><path d="M480.239-116.413q-152.63 0-258.228-105.478Q116.413-327.37 116.413-480q0-130.935 77.739-227.435t206.304-125.043q43.022-9.631 63.87 10.869t3.478 62.805q-8.891 22.043-14.315 44.463-5.424 22.42-5.424 46.689 0 91.694 64.326 155.879 64.325 64.186 156.218 64.186 24.369 0 46.978-4.946 22.609-4.945 44.413-14.076 42.826-17.369 62.967 1.142 20.142 18.511 10.511 61.054Q807.174-280 712.63-198.206q-94.543 81.793-232.391 81.793Zm0-95q79.783 0 143.337-40.217 63.554-40.218 95.793-108.283-15.608 4.044-31.097 5.326-15.49 1.283-31.859.805-123.706-4.066-210.777-90.539-87.071-86.473-91.614-212.092-.24-16.369.923-31.978 1.164-15.609 5.446-30.978-67.826 32.478-108.282 96.152Q211.652-559.543 211.652-480q0 111.929 78.329 190.258 78.329 78.329 190.258 78.329ZM466.13-465.891Z"/></svg></span>
    </a>
    
      <a id="nav-rss-link" class="nav-icon mobile-hide" href="/atom.xml" title="RSS 订阅">
        <svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" width="20"><path d="M198-120q-25.846 0-44.23-18.384-18.384-18.385-18.384-44.23 0-25.846 18.384-44.23 18.384-18.385 44.23-18.385 25.846 0 44.23 18.385 18.384 18.384 18.384 44.23 0 25.845-18.384 44.23Q223.846-120 198-120Zm538.385 0q-18.846 0-32.923-13.769-14.076-13.769-15.922-33.23-8.692-100.616-51.077-188.654-42.385-88.039-109.885-155.539-67.5-67.501-155.539-109.885Q283-663.462 182.385-672.154q-19.461-1.846-33.23-16.23-13.769-14.385-13.769-33.846t14.076-32.922q14.077-13.461 32.923-12.23 120.076 8.692 226.038 58.768 105.961 50.077 185.73 129.846 79.769 79.769 129.846 185.731 50.077 105.961 58.769 226.038 1.231 18.846-12.538 32.922Q756.461-120 736.385-120Zm-252 0q-18.231 0-32.423-13.461t-18.653-33.538Q418.155-264.23 348.886-333.5q-69.27-69.27-166.501-84.423-20.077-4.462-33.538-18.961-13.461-14.5-13.461-33.346 0-19.076 13.884-33.23 13.884-14.153 33.115-10.922 136.769 15.384 234.384 112.999 97.615 97.615 112.999 234.384 3.231 19.23-10.538 33.115Q505.461-120 484.385-120Z"/></svg>
      </a>
    
    <div id="nav-menu-btn" class="nav-icon">
      <svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" width="20"><path d="M177.37-252.282q-17.453 0-29.477-11.964-12.024-11.963-12.024-29.326t12.024-29.537q12.024-12.174 29.477-12.174h605.26q17.453 0 29.477 11.964 12.024 11.963 12.024 29.326t-12.024 29.537q-12.024 12.174-29.477 12.174H177.37Zm0-186.218q-17.453 0-29.477-11.963-12.024-11.964-12.024-29.326 0-17.363 12.024-29.537T177.37-521.5h605.26q17.453 0 29.477 11.963 12.024 11.964 12.024 29.326 0 17.363-12.024 29.537T782.63-438.5H177.37Zm0-186.217q-17.453 0-29.477-11.964-12.024-11.963-12.024-29.326t12.024-29.537q12.024-12.174 29.477-12.174h605.26q17.453 0 29.477 11.964 12.024 11.963 12.024 29.326t-12.024 29.537q-12.024 12.174-29.477 12.174H177.37Z"/></svg>
    </div>
  </nav>
</navbar>
<div id="nav-dropdown" class="hidden">
  <div id="dropdown-link-list">
    
      <a class="nav-dropdown-link" href="/">主页</a>
    
      <a class="nav-dropdown-link" href="/leetcode">Leetcode指南</a>
    
      <a class="nav-dropdown-link" href="/machine_learning">机器学习</a>
    
      <a class="nav-dropdown-link" href="/archives">归档</a>
    
      <a class="nav-dropdown-link" href="/about">关于我</a>
    
    
      <a class="nav-dropdown-link" href="/atom.xml" title="RSS 订阅">RSS</a>
     
    </div>
</div>
<script>
  let dropdownBtn = document.getElementById("nav-menu-btn");
  let dropdownEle = document.getElementById("nav-dropdown");
  dropdownBtn.onclick = function() {
    dropdownEle.classList.toggle("hidden");
  }
</script>
    </div>
    <div id="sidebar-wrapper">
      <sidebar id="sidebar">
  
    <div class="widget-wrap">
  <div class="info-card">
    <div class="avatar">
      
        <image src=/images/touxiang.jpg></image>
      
      <div class="img-dim"></div>
    </div>
    <div class="info">
      <div class="username">Kouir Wu </div>
      <div class="dot"></div>
      <div class="subtitle">当你深处深渊，退无可退的时候，眼前只剩下向上的一条路。 </div>
      <div class="link-list">
        
          <a class="link-btn" target="_blank" rel="noopener" href="https://twitter.com" title="Twitter"><i class="fa-brands fa-twitter"></i></a>
        
          <a class="link-btn" target="_blank" rel="noopener" href="https://github.com/KouirWu" title="GitHub"><i class="fa-brands fa-github"></i></a>
         
      </div>  
    </div>
  </div>
</div>

  
  <div class="sticky">
    
      
  <div class="widget-wrap">
    <div class="widget">
      <h3 class="widget-title">分类</h3>
      <div class="category-box">
            <a class="category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/">
                机器学习实战
                <div class="category-count">1</div>
            </a>
        <div class="children" style="display: none;"><div class="category-box">
            <a class="category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/Titanic/">
                Titanic
                <div class="category-count">1</div>
            </a>
        </div></div>
            <a class="category-link" href="/categories/Deepseek/">
                Deepseek
                <div class="category-count">2</div>
            </a>
        <div class="children" style="display: none;"><div class="category-box">
            <a class="category-link" href="/categories/Deepseek/Linux/">
                Linux
                <div class="category-count">2</div>
            </a>
        <div class="children" style="display: none;"><div class="category-box">
            <a class="category-link" href="/categories/Deepseek/Linux/Ollama/">
                Ollama
                <div class="category-count">2</div>
            </a>
        </div></div></div></div>
            <a class="category-link" href="/categories/%E5%89%8D%E7%AB%AF%E5%BC%80%E5%8F%91/">
                前端开发
                <div class="category-count">3</div>
            </a>
        <div class="children" style="display: none;"><div class="category-box">
            <a class="category-link" href="/categories/%E5%89%8D%E7%AB%AF%E5%BC%80%E5%8F%91/Vue3/">
                Vue3
                <div class="category-count">3</div>
            </a>
        <div class="children" style="display: none;"><div class="category-box">
            <a class="category-link" href="/categories/%E5%89%8D%E7%AB%AF%E5%BC%80%E5%8F%91/Vue3/%E5%B0%8F%E5%85%94%E9%B2%9C/">
                小兔鲜
                <div class="category-count">2</div>
            </a>
        
            <a class="category-link" href="/categories/%E5%89%8D%E7%AB%AF%E5%BC%80%E5%8F%91/Vue3/%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3/">
                问题解决
                <div class="category-count">1</div>
            </a>
        <div class="children" style="display: none;"><div class="category-box">
            <a class="category-link" href="/categories/%E5%89%8D%E7%AB%AF%E5%BC%80%E5%8F%91/Vue3/%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3/vue3%E9%99%AA%E8%AF%8A%E7%B3%BB%E7%BB%9F/">
                vue3陪诊系统
                <div class="category-count">1</div>
            </a>
        </div></div></div></div></div></div>
            <a class="category-link" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">
                人工智能
                <div class="category-count">1</div>
            </a>
        <div class="children" style="display: none;"><div class="category-box">
            <a class="category-link" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/Trae-AI/">
                Trae AI
                <div class="category-count">1</div>
            </a>
        </div></div>
            <a class="category-link" href="/categories/leetcode/">
                leetcode
                <div class="category-count">4</div>
            </a>
        <div class="children" style="display: none;"><div class="category-box">
            <a class="category-link" href="/categories/leetcode/hot100/">
                hot100
                <div class="category-count">4</div>
            </a>
        </div></div>
            <a class="category-link" href="/categories/LLM%E5%A4%A7%E6%A8%A1%E5%9E%8B/">
                LLM大模型
                <div class="category-count">3</div>
            </a>
        
            <a class="category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">
                机器学习
                <div class="category-count">1</div>
            </a>
        <div class="children" style="display: none;"><div class="category-box">
            <a class="category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/">
                环境配置
                <div class="category-count">1</div>
            </a>
        </div></div></div>
    </div>
  </div>
  <script>
    document.querySelectorAll('.category-link').forEach(link => {
      link.addEventListener('click', function(e) {
        const children = this.nextElementSibling;
        if (children && children.classList.contains('children')) {
          e.preventDefault();
          if (children.style.display === 'none') {
            children.style.display = 'block';
          } else {
            children.style.display = 'none';
          }
        }
      });
    });
    // 设置初始状态为折叠状态
    document.querySelectorAll('.children').forEach(children => {
      children.style.display = 'none';
    });
  </script>


    
      
  <div class="widget-wrap">
    <div class="widget">
      <h3 class="widget-title">标签</h3>
      <ul class="widget-tag-list" itemprop="keywords"><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/Deepseek/" rel="tag">Deepseek</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/Element-Plus/" rel="tag">Element Plus</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/Kaggle/" rel="tag">Kaggle</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/Linux/" rel="tag">Linux</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/MCP/" rel="tag">MCP</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/Ollama/" rel="tag">Ollama</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/Python/" rel="tag">Python</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/Titanic/" rel="tag">Titanic</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/Trae-AI/" rel="tag">Trae AI</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/Vue3/" rel="tag">Vue3</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/vue3%E9%99%AA%E8%AF%8A%E7%B3%BB%E7%BB%9F/" rel="tag">vue3陪诊系统</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E5%89%8D%E7%AB%AF%E5%BC%80%E5%8F%91/" rel="tag">前端开发</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E5%8F%8C%E6%8C%87%E9%92%88/" rel="tag">双指针</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E5%93%88%E5%B8%8C%E8%A1%A8/" rel="tag">哈希表</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2/" rel="tag">字符串</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E5%B0%8F%E5%85%94%E9%B2%9C/" rel="tag">小兔鲜</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E5%B0%8F%E7%99%BD%E6%95%99%E7%A8%8B/" rel="tag">小白教程</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E5%B9%B6%E6%9F%A5%E9%9B%86/" rel="tag">并查集</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E6%8E%92%E5%BA%8F/" rel="tag">排序</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/" rel="tag">数据分析</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E6%95%B0%E7%BB%84/" rel="tag">数组</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/" rel="tag">环境搭建</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E9%85%8D%E7%BD%AE/" rel="tag">配置</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3/" rel="tag">问题解决</a></li></ul>
    </div>
  </div>


    
      
  <div class="widget-wrap">
    <div class="widget">
      <h3 class="widget-title">最新文章</h3>
      <ul>
        
          <a class="recent-link" href="/2025/06/09/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/" title="大模型应用" >
            <div class="recent-link-text">
              大模型应用
            </div>
          </a>
        
          <a class="recent-link" href="/2025/06/09/%E5%8A%A8%E6%89%8B%E6%90%AD%E5%BB%BA%E5%A4%A7%E6%A8%A1%E5%9E%8B/" title="动手搭建大模型" >
            <div class="recent-link-text">
              动手搭建大模型
            </div>
          </a>
        
          <a class="recent-link" href="/2025/06/09/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B%E5%AE%9E%E8%B7%B5/" title="大模型训练流程实践" >
            <div class="recent-link-text">
              大模型训练流程实践
            </div>
          </a>
        
          <a class="recent-link" href="/2025/06/06/hot100%E7%A7%BB%E5%8A%A8%E9%9B%B6/" title="✨ LeetCode Hot 100 - 283. 移动零 ✨" >
            <div class="recent-link-text">
              ✨ LeetCode Hot 100 - 283. 移动零 ✨
            </div>
          </a>
        
          <a class="recent-link" href="/2025/06/06/hot100%E6%9C%80%E9%95%BF%E8%BF%9E%E7%BB%AD%E5%BA%8F%E5%88%97/" title="✨ LeetCode Hot 100 - 128. 最长连续序列 ✨" >
            <div class="recent-link-text">
              ✨ LeetCode Hot 100 - 128. 最长连续序列 ✨
            </div>
          </a>
        
      </ul>
    </div>
  </div>

    
  </div>
</sidebar>
    </div>
    <div id="content-body">
       


<article id="post-大模型训练流程实践" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  
    
   
  <div class="article-inner">
    <div class="article-main">
      <header class="article-header">
        
<div class="main-title-bar">
  <div class="main-title-dot"></div>
  
    
      <h1 class="p-name article-title" itemprop="headline name">
        大模型训练流程实践
      </h1>
    
  
</div>

        <div class='meta-info-bar'>
          <div class="meta-info">
  <time class="dt-published" datetime="2025-06-09T09:03:32.000Z" itemprop="datePublished">2025-06-09</time>
</div>
          <div class="need-seperator meta-info">
            <div class="meta-cate-flex">
  
  <a class="meta-cate-link" href="/categories/LLM%E5%A4%A7%E6%A8%A1%E5%9E%8B/">LLM大模型</a>
   
</div>
  
          </div>
          <div class="wordcount need-seperator meta-info">
            30k 词 
          </div>
        </div>
        
      </header>
      <div class="e-content article-entry" itemprop="articleBody">
        
          <h1 id="大模型训练流程实践"><a href="#大模型训练流程实践" class="headerlink" title="大模型训练流程实践"></a>大模型训练流程实践</h1><h2 id="6-1-模型预训练"><a href="#6-1-模型预训练" class="headerlink" title="6.1 模型预训练"></a>6.1 模型预训练</h2><p>在上一章，我们逐步拆解了 LLM 的模型结构及训练过程，从零手写实现了 LLaMA 模型结构及 Pretrain、SFT 全流程，更深入地理解了 LLM 的模型原理及训练细节。但是，在实际应用中，手写实现的 LLM 训练存在以下问题：</p>
<ul>
<li>手写实现 LLM 结构工作量大，难以实时跟进最新模型的结构创新；</li>
<li>从零实现的 LLM 训练无法较好地实现多卡分布式训练，训练效率较低；</li>
<li>和现有预训练 LLM 不兼容，无法使用预训练好的模型参数</li>
</ul>
<p>因此，在本章中，我们将介绍目前 LLM 领域的主流训练框架 Transformers，并结合分布式框架 deepspeed、高效微调框架 peft 等主流框架，实践使用 transformers 进行模型 Pretrain、SFT 全流程，更好地对接业界的主流 LLM 技术方案。</p>
<h3 id="6-1-1-框架介绍"><a href="#6-1-1-框架介绍" class="headerlink" title="6.1.1 框架介绍"></a>6.1.1 框架介绍</h3><p>Transformers 是由 Hugging Face 开发的 NLP 框架，通过模块化设计实现了对 BERT、GPT、LLaMA、T5、ViT 等上百种主流模型架构的统一支持。通过使用 Transformers，开发者无需重复实现基础网络结构，通过 AutoModel 类即可一键加载任意预训练，图6.1 为 Hugging Face Transformers 课程首页：</p>
<div align='center'>
    <img src="../images/6-images/1-1.png" alt="alt text" width="90%">
    <p>图6.1 Hugging Face Transformers</p>
</div>

<p>同时，框架内置的 Trainer 类封装了分布式训练的核心逻辑，支持 PyTorch 原生 DDP、DeepSpeed、Megatron-LM 等多种分布式训练策略。通过简单配置训练参数，即可实现数据并行、模型并行、流水线并行的混合并行训练，在 8 卡 A100 集群上可轻松支持百亿参数模型的高效训练。配合 SavingPolicy 和 LoggingCallback 等组件，实现了训练过程的自动化管理。其还支持与 Deepspeed、peft、wandb、Swanlab 等框架进行集成，直接通过参数设置即可无缝对接，从而快速、高效实现 LLM 训练。</p>
<p>对 LLM 时代的 NLP 研究者更为重要的是，HuggingFace 基于 Transformers 框架搭建了其庞大的 AI 社区，开放了数亿个预训练模型参数、25万+不同类型数据集，通过 Transformers、Dataset、Evaluate 等多个框架实现对预训练模型、数据集及评估函数的集成，从而帮助开发者可以便捷地使用任一预训练模型，在开源模型及数据集的基础上便捷地实现个人模型的开发与应用。</p>
<div align='center'>
    <img src="../images/6-images/1-2.png" alt="alt text" width="90%">
    <p>图6.2 Hugging Face Transformers 模型社区</p>
</div>

<p>在 LLM 时代，模型结构的调整和重新预训练越来越少，开发者更多的业务应用在于使用预训练好的 LLM 进行 Post Train 和 SFT，来支持自己的下游业务应用。且由于预训练模型体量大，便捷集成 deepspeed 等分布式训练框架逐渐成为 LLM 时代 NLP 模型训练的必备技能。因此，Transformers 已逐步成为学界、业界 NLP 技术的主流框架，不管是企业业务开发还是科研研究，都逐渐首选 Transformers 进行模型实现。同时，新发布的开源 LLM 如 DeepSeek、Qwen 也都会第一时间在 Transformers 社区开放其预训练权重与模型调用 Demo。通过使用 Transformers 框架，可以高效、便捷地完成 LLM 训练及开发，实现工业级的产出交付。接下来，我们就会以 Transformers 框架为基础，介绍如何通过 Transformers 框架实现 LLM 的 Pretrain 及 SFT。</p>
<h3 id="6-1-2-初始化-LLM"><a href="#6-1-2-初始化-LLM" class="headerlink" title="6.1.2 初始化 LLM"></a>6.1.2 初始化 LLM</h3><p>我们可以使用 transformers 的 AutoModel 类来直接初始化已经实现好的模型。对于任意预训练模型，其参数中都包含有模型的配置信息。如果是想要从头训练一个 LLM，可以使用一个已有的模型架构来直接初始化。这里，我们以 <a target="_blank" rel="noopener" href="https://huggingface.co/Qwen/Qwen2.5-1.5B/tree/main">Qwen-2.5-1.5B</a>的模型架构为例：</p>
<div align='center'>
    <img src="../images/6-images/1-3.png" alt="alt text" width="90%">
    <p>图6.3 Qwen-2.5-1.5B</p>
</div>

<p>该界面即为 HuggingFace 社区中的 Qwen-2.5-1.5B 模型参数，其中的 <code>config.json</code> 文件即是模型的配置信息，包括了模型的架构、隐藏层大小、模型层数等，如图6.4所示：</p>
<div align='center'>
    <img src="../images/6-images/1-4.png" alt="alt text" width="90%">
    <p>图6.4 Qwen-2.5-1.5B config.json 文件</p>
</div>

<p>我们可以沿用该模型的配置信息，初始化一个 Qwen-2.5-1.5B 模型来进行训练，也可以在该配置信息的基础上进行更改，如修改隐藏层大小、注意力头数等，来定制一个模型结构。HuggingFace 提供了 Python 工具来便捷下载想使用的模型参数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment"># 设置环境变量，此处使用 HuggingFace 镜像网站</span></span><br><span class="line">os.environ[<span class="string">&#x27;HF_ENDPOINT&#x27;</span>] = <span class="string">&#x27;https://hf-mirror.com&#x27;</span></span><br><span class="line"><span class="comment"># 下载模型</span></span><br><span class="line">os.system(<span class="string">&#x27;huggingface-cli download --resume-download Qwen/Qwen2.5-1.5B --local-dir your_local_dir&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>如图6.5，此处的 “Qwen&#x2F;Qwen2.5-1.5B”即为要下载模型的标识符，对于其他模型，可以直接复制 HuggingFace 上的模型名即可：</p>
<div align='center'>
    <img src="../images/6-images/1-5.png" alt="alt text" width="90%">
    <p>图6.5 模型下载标识</p>
</div>

<p>下载完成后，可以使用 AutoConfig 类直接加载下载好的配置文件：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载定义好的模型参数-此处以 Qwen-2.5-1.5B 为例</span></span><br><span class="line"><span class="comment"># 使用 transforemrs 的 Config 类进行加载</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoConfig</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载参数的本地路径</span></span><br><span class="line">model_path = <span class="string">&quot;qwen-1.5b&quot;</span></span><br><span class="line">config = AutoConfig.from_pretrained(model_name_or_path)</span><br></pre></td></tr></table></figure>

<p>也可以对配置文件进行自定义，然后以同样的方式加载即可。可以使用 AutoModel 类基于加载好的配置对象生成对应的模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用该配置生成一个定义好的模型</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForCausalLM</span><br><span class="line"></span><br><span class="line">model = AutoModelForCausalLM.from_config(config,trust_remote_code=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>由于 LLM 一般都是 CausalLM 架构，此处使用了 AutoModelForCausalLM 类进行加载。如果是用于分类任务训练，可使用 AutoModelForSequenceClassification 类来加载。查看该 model，图6.6可以看到其架构和定义的配置文件相同：</p>
<div align='center'>
    <img src="../images/6-images/1-6.png" alt="alt text" width="70%">
    <p>图6.6 模型结构输出结果</p>
</div>

<p>该 model 就是一个从零初始化的 Qwen-2.5-1.5B 模型了。一般情况下，我们很少从零初始化 LLM 进行预训练，较多的做法是加载一个预训练好的 LLM 权重，在自己的语料上进行后训练。这里，我们也介绍如何从下载好的模型参数中初始化一个预训练好的模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForCausalLM</span><br><span class="line"></span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(model_name_or_path,trust_remote_code=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>类似的，直接使用 from_pretrained 方法加载即可，此处的 model_name_or_path 即为下载好的参数的本地路径。</p>
<p>我们还需要初始化一个 tokenizer。此处，我们直接使用 Qwen-2.5-1.5B 对应的 tokenzier 参数即可：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载一个预训练好的 tokenizer</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)</span><br></pre></td></tr></table></figure>

<p>加载好的 tokenizer 即可直接使用，对任意文本进行分词处理。</p>
<h3 id="6-1-3-预训练数据处理"><a href="#6-1-3-预训练数据处理" class="headerlink" title="6.1.3 预训练数据处理"></a>6.1.3 预训练数据处理</h3><p>与第五章类似，我们使用出门问问序列猴子开源数据集作为预训练数据集，可以用与第五章一致的方式进行数据集的下载和解压。HuggingFace 的 datasets 库是和 transformers 框架配套的、用于数据下载和处理的第三方库。我们可以直接使用 datasets 的 load_dataset 函数来加载预训练数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载预训练数据</span></span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"></span><br><span class="line">ds = load_dataset(<span class="string">&#x27;json&#x27;</span>, data_files=<span class="string">&#x27;/mobvoi_seq_monkey_general_open_corpus.jsonl&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>注意，由于数据集较大，加载可能会出现时间较长或内存不够的情况，建议前期测试时将预训练数据集拆分一部分出来进行测试。加载出来的 ds 是一个 DatasetDict 对象，加载的数据会默认保存在 <code>train</code> 键对应的值中，可以通过以下代码查看：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ds[<span class="string">&quot;train&quot;</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure>

<div align='center'>
    <img src="../images/6-images/1-7.png" alt="alt text" width="100%">
    <p>图6.7 数据集展示</p>
</div>

<p>可以通过 feature 属性查看数据集的特征（也就是列），这里需要保存一下数据集的列名，因为后续数据处理时，再将文本 tokenize 之后，需要移除原先的文本：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看特征</span></span><br><span class="line">column_names = <span class="built_in">list</span>(ds[<span class="string">&quot;train&quot;</span>].features)</span><br><span class="line"><span class="comment"># columnes_name:[&quot;text&quot;]</span></span><br></pre></td></tr></table></figure>

<p>接着使用加载好的 tokenizer 对数据集进行处理，此处使用 map 函数来进行批量处理：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对数据集进行 tokenize</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tokenize_function</span>(<span class="params">examples</span>):</span><br><span class="line">    <span class="comment"># 使用预先加载的 tokenizer 进行分词</span></span><br><span class="line">    output = tokenizer([item <span class="keyword">for</span> item <span class="keyword">in</span> examples[<span class="string">&quot;text&quot;</span>]])</span><br><span class="line">    <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="comment"># 批量处理</span></span><br><span class="line">tokenized_datasets = ds.<span class="built_in">map</span>(</span><br><span class="line">    tokenize_function,</span><br><span class="line">    batched=<span class="literal">True</span>,</span><br><span class="line">    num_proc=<span class="number">10</span>,</span><br><span class="line">    remove_columns=column_names,</span><br><span class="line">    load_from_cache_file=<span class="literal">True</span>,</span><br><span class="line">    desc=<span class="string">&quot;Running tokenizer on dataset&quot;</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>处理完成后的数据集会包括’input_ids’, ‘attention_mask’两列，分别是文本 tokenize 之后的数值序列和注意力掩码（标识是否 padding）。map 方法会通过 remove_columns 参数将原先的‘text’移除，训练中不再使用。</p>
<p>由于预训练一般为 CLM 任务，一次性学习多个样本的序列语义不影响模型性能，且训练数据量大、训练时间长，对训练效率要求比较高。在预训练过程中，一般会把多个文本段拼接在一起，处理成统一长度的文本块，再对每个文本块进行训练。在这里，我们实现一个拼接函数将文本块拼接到 2048个 token 长度，再通过 map 方法来进行批量处理：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 预训练一般将文本拼接成固定长度的文本段</span></span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> chain</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里我们取块长为 2048</span></span><br><span class="line">block_size = <span class="number">2048</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">group_texts</span>(<span class="params">examples</span>):</span><br><span class="line">    <span class="comment"># 将文本段拼接起来</span></span><br><span class="line">    concatenated_examples = &#123;k: <span class="built_in">list</span>(chain(*examples[k])) <span class="keyword">for</span> k <span class="keyword">in</span> examples.keys()&#125;</span><br><span class="line">    <span class="comment"># 计算拼起来的整体长度</span></span><br><span class="line">    total_length = <span class="built_in">len</span>(concatenated_examples[<span class="built_in">list</span>(examples.keys())[<span class="number">0</span>]])</span><br><span class="line">    <span class="comment"># 如果长度太长，进行分块</span></span><br><span class="line">    <span class="keyword">if</span> total_length &gt;= block_size:</span><br><span class="line">        total_length = (total_length // block_size) * block_size</span><br><span class="line">    <span class="comment"># 按 block_size 进行切分</span></span><br><span class="line">    result = &#123;</span><br><span class="line">        k: [t[i : i + block_size] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, total_length, block_size)]</span><br><span class="line">        <span class="keyword">for</span> k, t <span class="keyword">in</span> concatenated_examples.items()</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment"># CLM 任务，labels 和 input 是相同的</span></span><br><span class="line">    result[<span class="string">&quot;labels&quot;</span>] = result[<span class="string">&quot;input_ids&quot;</span>].copy()</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"><span class="comment"># 批量处理</span></span><br><span class="line">lm_datasets = tokenized_datasets.<span class="built_in">map</span>(</span><br><span class="line">    group_texts,</span><br><span class="line">    batched=<span class="literal">True</span>,</span><br><span class="line">    num_proc=<span class="number">10</span>,</span><br><span class="line">    load_from_cache_file=<span class="literal">True</span>,</span><br><span class="line">    desc=<span class="string">f&quot;Grouping texts in chunks of <span class="subst">&#123;block_size&#125;</span>&quot;</span>,</span><br><span class="line">    batch_size = <span class="number">40000</span>,</span><br><span class="line">)</span><br><span class="line">train_dataset = lm_datasets[<span class="string">&quot;train&quot;</span>]</span><br></pre></td></tr></table></figure>

<p>处理得到的 train_dataset 就是一个可直接用于 CLM Pretrain 的预训练数据集了，其每个样本长度为 2048个 token。</p>
<h3 id="6-1-4-使用-Trainer-进行训练"><a href="#6-1-4-使用-Trainer-进行训练" class="headerlink" title="6.1.4 使用 Trainer 进行训练"></a>6.1.4 使用 Trainer 进行训练</h3><p>接下来，我们使用 transformers 提供的 Trainer 类进行训练。Trainer 封装了模型的训练逻辑，且做了较好的效率优化、可视化等工作，可以高效、便捷地完成 LLM 的训练。</p>
<p>首先我们需要配置训练的超参数，使用 TrainingArguments 类来实例化一个参数对象：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> TrainingArguments</span><br><span class="line"><span class="comment"># 配置训练参数</span></span><br><span class="line"></span><br><span class="line">training_args = TrainingArguments(</span><br><span class="line">    output_dir=<span class="string">&quot;output&quot;</span>,<span class="comment"># 训练参数输出路径</span></span><br><span class="line">    per_device_train_batch_size=<span class="number">4</span>,<span class="comment"># 训练的 batch_size</span></span><br><span class="line">    gradient_accumulation_steps=<span class="number">4</span>,<span class="comment"># 梯度累计步数，实际 bs = 设置的 bs * 累计步数</span></span><br><span class="line">    logging_steps=<span class="number">10</span>,<span class="comment"># 打印 loss 的步数间隔</span></span><br><span class="line">    num_train_epochs=<span class="number">1</span>,<span class="comment"># 训练的 epoch 数</span></span><br><span class="line">    save_steps=<span class="number">100</span>, <span class="comment"># 保存模型参数的步数间隔</span></span><br><span class="line">    learning_rate=<span class="number">1e-4</span>,<span class="comment"># 学习率</span></span><br><span class="line">    gradient_checkpointing=<span class="literal">True</span><span class="comment"># 开启梯度检查点</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>然后基于初始化的 model、tokenzier 和 training_args，并传入处理好的训练数据集，实例化一个 trainer 对象：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> Trainer, default_data_collator</span><br><span class="line"><span class="keyword">from</span> torchdata.datapipes.<span class="built_in">iter</span> <span class="keyword">import</span> IterableWrapper</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练器</span></span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model=model,</span><br><span class="line">    args=training_args,</span><br><span class="line">    train_dataset= IterableWrapper(train_dataset),</span><br><span class="line">    eval_dataset= <span class="literal">None</span>,</span><br><span class="line">    tokenizer=tokenizer,</span><br><span class="line">    <span class="comment"># 默认为 MLM 的 collator，使用 CLM 的 collater</span></span><br><span class="line">    data_collator=default_data_collator</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>再使用 train 方法，即会按照配置好的训练超参进行训练和保存：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">trainer.train()</span><br></pre></td></tr></table></figure>

<blockquote>
<p>注：上述代码存放于 <code>./code/pretrian.ipynb</code> 文件中。</p>
</blockquote>
<h3 id="6-1-5-使用-DeepSpeed-实现分布式训练"><a href="#6-1-5-使用-DeepSpeed-实现分布式训练" class="headerlink" title="6.1.5 使用 DeepSpeed 实现分布式训练"></a>6.1.5 使用 DeepSpeed 实现分布式训练</h3><p>由于预训练规模大、时间长，一般不推荐使用 Jupyter Notebook 来运行，容易发生中断。且由于预训练规模大，一般需要使用多卡进行分布式训练，否则训练时间太长。在这里，我们介绍如何基于上述代码，使用 DeepSpeed 框架实现分布式训练，从而完成业界可用的 LLM Pretrain。</p>
<p>长时间训练一般使用 bash 脚本设定超参，再启动写好的 python 脚本实现训练。我们使用一个 Python 脚本（<code>./code/pretrain.py</code>）来实现训练全流程。</p>
<p>先导入所需第三方库：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">from</span> dataclasses <span class="keyword">import</span> dataclass, field</span><br><span class="line"><span class="keyword">from</span> torchdata.datapipes.<span class="built_in">iter</span> <span class="keyword">import</span> IterableWrapper</span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> chain</span><br><span class="line"><span class="keyword">import</span> deepspeed</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Optional</span>,<span class="type">List</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"><span class="keyword">import</span> transformers</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> (</span><br><span class="line">    AutoConfig,</span><br><span class="line">    AutoModelForCausalLM,</span><br><span class="line">    AutoTokenizer,</span><br><span class="line">    HfArgumentParser,</span><br><span class="line">    Trainer,</span><br><span class="line">    TrainingArguments,</span><br><span class="line">    default_data_collator,</span><br><span class="line">    set_seed,</span><br><span class="line">)</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">from</span> transformers.testing_utils <span class="keyword">import</span> CaptureLogger</span><br><span class="line"><span class="keyword">from</span> transformers.trainer_utils <span class="keyword">import</span> get_last_checkpoint</span><br><span class="line"><span class="keyword">import</span> wandb</span><br></pre></td></tr></table></figure>

<p>首先需要定义几个超参的类型，用于处理 sh 脚本中设定的超参值。由于 transformers 本身有 TraingingArguments 类，其中包括了训练的一些必备超参数。我们这里只需定义 TrainingArguments 中未包含的超参即可，主要包括模型相关的超参（定义在 ModelArguments）和数据相关的超参（定义在 DataTrainingArguments）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 超参类</span></span><br><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ModelArguments</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    关于模型的参数</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    model_name_or_path: <span class="type">Optional</span>[<span class="built_in">str</span>] = field(</span><br><span class="line">        default=<span class="literal">None</span>,</span><br><span class="line">        metadata=&#123;</span><br><span class="line">            <span class="string">&quot;help&quot;</span>: (</span><br><span class="line">                <span class="string">&quot;后训练使用，为预训练模型参数地址&quot;</span></span><br><span class="line">            )</span><br><span class="line">        &#125;,</span><br><span class="line">    )</span><br><span class="line">    config_name: <span class="type">Optional</span>[<span class="built_in">str</span>] = field(</span><br><span class="line">        default=<span class="literal">None</span>, metadata=&#123;<span class="string">&quot;help&quot;</span>: <span class="string">&quot;预训练使用，Config 文件地址&quot;</span>&#125;</span><br><span class="line">    )</span><br><span class="line">    tokenizer_name: <span class="type">Optional</span>[<span class="built_in">str</span>] = field(</span><br><span class="line">        default=<span class="literal">None</span>, metadata=&#123;<span class="string">&quot;help&quot;</span>: <span class="string">&quot;预训练 Tokenizer 地址&quot;</span>&#125;</span><br><span class="line">    )</span><br><span class="line">    torch_dtype: <span class="type">Optional</span>[<span class="built_in">str</span>] = field(</span><br><span class="line">        default=<span class="literal">None</span>,</span><br><span class="line">        metadata=&#123;</span><br><span class="line">            <span class="string">&quot;help&quot;</span>: (</span><br><span class="line">                <span class="string">&quot;模型训练使用的数据类型，推荐 bfloat16&quot;</span></span><br><span class="line">            ),</span><br><span class="line">            <span class="string">&quot;choices&quot;</span>: [<span class="string">&quot;auto&quot;</span>, <span class="string">&quot;bfloat16&quot;</span>, <span class="string">&quot;float16&quot;</span>, <span class="string">&quot;float32&quot;</span>],</span><br><span class="line">        &#125;,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DataTrainingArguments</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    关于训练的参数</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    train_files: <span class="type">Optional</span>[<span class="type">List</span>[<span class="built_in">str</span>]]  = field(default=<span class="literal">None</span>, metadata=&#123;<span class="string">&quot;help&quot;</span>: <span class="string">&quot;训练数据路径&quot;</span>&#125;)</span><br><span class="line">    block_size: <span class="type">Optional</span>[<span class="built_in">int</span>] = field(</span><br><span class="line">        default=<span class="literal">None</span>,</span><br><span class="line">        metadata=&#123;</span><br><span class="line">            <span class="string">&quot;help&quot;</span>: (</span><br><span class="line">                <span class="string">&quot;设置的文本块长度&quot;</span></span><br><span class="line">            )</span><br><span class="line">        &#125;,</span><br><span class="line">    )</span><br><span class="line">    preprocessing_num_workers: <span class="type">Optional</span>[<span class="built_in">int</span>] = field(</span><br><span class="line">        default=<span class="literal">None</span>,</span><br><span class="line">        metadata=&#123;<span class="string">&quot;help&quot;</span>: <span class="string">&quot;预处理使用线程数.&quot;</span>&#125;,</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>

<p>然后即可定义一个主函数实现上述训练过程的封装。首先通过 transformers 提供的 HfArgumentParser 工具来加载 sh 脚本中设定的超参：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载脚本参数</span></span><br><span class="line">parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))</span><br><span class="line">model_args, data_args, training_args = parser.parse_args_into_dataclasses()</span><br></pre></td></tr></table></figure>

<p>在大规模的训练中，一般使用 log 来保存训练过程的信息，一般不推荐使用 print 直接打印，容易发生关键训练信息的丢失。这里，我们直接使用 python 自带的 logging 库来实现日志记录。首先需要进行 log 的设置：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置日志</span></span><br><span class="line">logging.basicConfig(</span><br><span class="line">    <span class="built_in">format</span>=<span class="string">&quot;%(asctime)s - %(levelname)s - %(name)s - %(message)s&quot;</span>,</span><br><span class="line">    datefmt=<span class="string">&quot;%m/%d/%Y %H:%M:%S&quot;</span>,</span><br><span class="line">    handlers=[logging.StreamHandler(sys.stdout)],</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将日志级别设置为 INFO</span></span><br><span class="line">transformers.utils.logging.set_verbosity_info()</span><br><span class="line">log_level = training_args.get_process_log_level()</span><br><span class="line">logger.setLevel(log_level)</span><br><span class="line">datasets.utils.logging.set_verbosity(log_level)</span><br><span class="line">transformers.utils.logging.set_verbosity(log_level)</span><br><span class="line">transformers.utils.logging.enable_default_handler()</span><br><span class="line">transformers.utils.logging.enable_explicit_format()</span><br></pre></td></tr></table></figure>

<p>这里将日志的级别设置为 INFO。logging 的日志共有 DEBUG、INFO、WARNING、ERROR 以及 CRITICAL 五个级别，将日志设置为哪个级别，就会只输出该级别及该级别之上的信息。设置完成后，在需要记录日志的地方，直接使用 logger 即可，记录时会指定记录日志的级别，例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练整体情况记录</span></span><br><span class="line">logger.warning(</span><br><span class="line">    <span class="string">f&quot;Process rank: <span class="subst">&#123;training_args.local_rank&#125;</span>, device: <span class="subst">&#123;training_args.device&#125;</span>, n_gpu: <span class="subst">&#123;training_args.n_gpu&#125;</span>&quot;</span></span><br><span class="line">    + <span class="string">f&quot;distributed training: <span class="subst">&#123;<span class="built_in">bool</span>(training_args.local_rank != -<span class="number">1</span>)&#125;</span>, 16-bits training: <span class="subst">&#123;training_args.fp16&#125;</span>&quot;</span></span><br><span class="line">)</span><br><span class="line">logger.info(<span class="string">f&quot;Training/evaluation parameters <span class="subst">&#123;training_args&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>后续就不再赘述脚本中的日志记录。</p>
<p>在大规模训练中，发生中断是往往难以避免的，训练一般会固定间隔保存 checkpoint，中断之后基于最近的 checkpoint 恢复训练即可。因此，我们需要首先检测是否存在旧的 checkpoint 并从 checkpoint 恢复训练：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 检查 checkpoint</span></span><br><span class="line">last_checkpoint = <span class="literal">None</span></span><br><span class="line"><span class="keyword">if</span> os.path.isdir(training_args.output_dir):</span><br><span class="line">    <span class="comment"># 使用 transformers 自带的 get_last_checkpoint 自动检测</span></span><br><span class="line">    last_checkpoint = get_last_checkpoint(training_args.output_dir)</span><br><span class="line">    <span class="keyword">if</span> last_checkpoint <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">and</span> <span class="built_in">len</span>(os.listdir(training_args.output_dir)) &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(</span><br><span class="line">            <span class="string">f&quot;输出路径 (<span class="subst">&#123;training_args.output_dir&#125;</span>) 非空 &quot;</span></span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">elif</span> last_checkpoint <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> training_args.resume_from_checkpoint <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        logger.info(</span><br><span class="line">            <span class="string">f&quot;从 <span class="subst">&#123;last_checkpoint&#125;</span>恢复训练&quot;</span></span><br><span class="line">        )</span><br></pre></td></tr></table></figure>

<p>接着以上文介绍过的方式初始化模型，此处将从零初始化和基于已有预训练模型初始化包装在一起：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化模型</span></span><br><span class="line"><span class="keyword">if</span> model_args.config_name <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    <span class="comment"># from scrach</span></span><br><span class="line">    config = AutoConfig.from_pretrained(model_args.config_name)</span><br><span class="line">    logger.warning(<span class="string">&quot;你正在从零初始化一个模型&quot;</span>)</span><br><span class="line">    logger.info(<span class="string">f&quot;模型参数配置地址：<span class="subst">&#123;model_args.config_name&#125;</span>&quot;</span>)</span><br><span class="line">    logger.info(<span class="string">f&quot;模型参数：<span class="subst">&#123;config&#125;</span>&quot;</span>)</span><br><span class="line">    model = AutoModelForCausalLM.from_config(config,trust_remote_code=<span class="literal">True</span>)</span><br><span class="line">    n_params = <span class="built_in">sum</span>(&#123;p.data_ptr(): p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters()&#125;.values())</span><br><span class="line">    logger.info(<span class="string">f&quot;预训练一个新模型 - Total size=<span class="subst">&#123;n_params/<span class="number">2</span>**<span class="number">20</span>:<span class="number">.2</span>f&#125;</span>M params&quot;</span>)</span><br><span class="line"><span class="keyword">elif</span> model_args.model_name_or_path <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    logger.warning(<span class="string">&quot;你正在初始化一个预训练模型&quot;</span>)</span><br><span class="line">    logger.info(<span class="string">f&quot;模型参数地址：<span class="subst">&#123;model_args.model_name_or_path&#125;</span>&quot;</span>)</span><br><span class="line">    model = AutoModelForCausalLM.from_pretrained(model_args.model_name_or_path,trust_remote_code=<span class="literal">True</span>)</span><br><span class="line">    n_params = <span class="built_in">sum</span>(&#123;p.data_ptr(): p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters()&#125;.values())</span><br><span class="line">    logger.info(<span class="string">f&quot;继承一个预训练模型 - Total size=<span class="subst">&#123;n_params/<span class="number">2</span>**<span class="number">20</span>:<span class="number">.2</span>f&#125;</span>M params&quot;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    logger.error(<span class="string">&quot;config_name 和 model_name_or_path 不能均为空&quot;</span>)</span><br><span class="line">    <span class="keyword">raise</span> ValueError(<span class="string">&quot;config_name 和 model_name_or_path 不能均为空&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>再类似的进行 tokenizer 的加载和预训练数据的处理。该部分和上文完全一致，此处不再赘述，读者可以在代码中详细查看细节。类似的，使用 Trainer 进行训练：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">logger.info(<span class="string">&quot;初始化 Trainer&quot;</span>)</span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model=model,</span><br><span class="line">    args=training_args,</span><br><span class="line">    train_dataset= IterableWrapper(train_dataset),</span><br><span class="line">    tokenizer=tokenizer,</span><br><span class="line">    data_collator=default_data_collator</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从 checkpoint 加载</span></span><br><span class="line">checkpoint = <span class="literal">None</span></span><br><span class="line"><span class="keyword">if</span> training_args.resume_from_checkpoint <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    checkpoint = training_args.resume_from_checkpoint</span><br><span class="line"><span class="keyword">elif</span> last_checkpoint <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        checkpoint = last_checkpoint</span><br><span class="line"></span><br><span class="line">logger.info(<span class="string">&quot;开始训练&quot;</span>)</span><br><span class="line">train_result = trainer.train(resume_from_checkpoint=checkpoint)</span><br><span class="line">trainer.save_model() </span><br></pre></td></tr></table></figure>
<p>注意，由于上文检测了是否存在 checkpoint，此处使用 resume_from_checkpoint 来实现从 checkpoint 恢复训练的功能。</p>
<p>由于在大规模训练中监测训练进度、loss 下降趋势尤为重要，在脚本中，我们使用了 wandb 作为训练检测的工具。在脚本开始进行了 wandb 的初始化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化 WandB</span></span><br><span class="line">wandb.init(project=<span class="string">&quot;pretrain&quot;</span>, name=<span class="string">&quot;from_scrach&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>在启动训练后，终端会输出 wandb 监测的 url，点击即可观察训练进度。此处不再赘述 wandb 的使用细节，欢迎读者查阅相关的资料说明。</p>
<p>完成上述代码后，我们使用一个 sh 脚本（<code>./code/pretrain.sh</code>）定义超参数的值，并通过 Deepspeed 启动训练，从而实现高效的多卡分布式训练：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置可见显卡</span></span><br><span class="line">CUDA_VISIBLE_DEVICES=0,1</span><br><span class="line"></span><br><span class="line">deepspeed pretrain.py \</span><br><span class="line">    --config_name autodl-tmp/qwen-1.5b \</span><br><span class="line">    --tokenizer_name autodl-tmp/qwen-1.5b \</span><br><span class="line">    --train_files autodl-tmp/dataset/pretrain_data/mobvoi_seq_monkey_general_open_corpus_small.jsonl \</span><br><span class="line">    --per_device_train_batch_size 16 \</span><br><span class="line">    --gradient_accumulation_steps 4 \</span><br><span class="line">    --do_train \</span><br><span class="line">    --output_dir autodl-tmp/output/pretrain \</span><br><span class="line">    --evaluation_strategy  no \</span><br><span class="line">    --learning_rate 1e-4 \</span><br><span class="line">    --num_train_epochs 1 \</span><br><span class="line">    --warmup_steps 200 \</span><br><span class="line">    --logging_dir autodl-tmp/output/pretrain/logs \</span><br><span class="line">    --logging_strategy steps \</span><br><span class="line">    --logging_steps 5 \</span><br><span class="line">    --save_strategy steps \</span><br><span class="line">    --save_steps 100 \</span><br><span class="line">    --preprocessing_num_workers 10 \</span><br><span class="line">    --save_total_limit 1 \</span><br><span class="line">    --seed 12 \</span><br><span class="line">    --block_size 2048 \</span><br><span class="line">    --bf16 \</span><br><span class="line">    --gradient_checkpointing \</span><br><span class="line">    --deepspeed ./ds_config_zero2.json \</span><br><span class="line">    --report_to wandb</span><br><span class="line">    <span class="comment"># --resume_from_checkpoint $&#123;output_model&#125;/checkpoint-20400 \</span></span><br></pre></td></tr></table></figure>
<p>在安装了 Deepspeed 第三方库后，可以直接通过 Deepspeed 命令来启动多卡训练。上述脚本命令主要是定义了各种超参数的值，可参考使用。在第四章中，我们介绍了 DeepSpeed 分布式训练的原理和 ZeRO 阶段设置，在这里，我们使用 ZeRO-2 进行训练。此处加载了 <code>ds_config_zero.json</code> 作为 DeepSpeed 的配置参数：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;fp16&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;enabled&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;loss_scale&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;loss_scale_window&quot;</span><span class="punctuation">:</span> <span class="number">1000</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;initial_scale_power&quot;</span><span class="punctuation">:</span> <span class="number">16</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;hysteresis&quot;</span><span class="punctuation">:</span> <span class="number">2</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;min_loss_scale&quot;</span><span class="punctuation">:</span> <span class="number">1</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;bf16&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;enabled&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;optimizer&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;AdamW&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;params&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;lr&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;betas&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;eps&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;weight_decay&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line"></span><br><span class="line">    <span class="attr">&quot;scheduler&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;WarmupLR&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;params&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;warmup_min_lr&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;warmup_max_lr&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;warmup_num_steps&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line"></span><br><span class="line">    <span class="attr">&quot;zero_optimization&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;stage&quot;</span><span class="punctuation">:</span> <span class="number">2</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;offload_optimizer&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;device&quot;</span><span class="punctuation">:</span> <span class="string">&quot;none&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;pin_memory&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span></span><br><span class="line">        <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;allgather_partitions&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;allgather_bucket_size&quot;</span><span class="punctuation">:</span> <span class="number">2e8</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;overlap_comm&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;reduce_scatter&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;reduce_bucket_size&quot;</span><span class="punctuation">:</span> <span class="number">2e8</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;contiguous_gradients&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line"></span><br><span class="line">    <span class="attr">&quot;gradient_accumulation_steps&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;gradient_clipping&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;steps_per_print&quot;</span><span class="punctuation">:</span> <span class="number">100</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;train_batch_size&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;train_micro_batch_size_per_gpu&quot;</span><span class="punctuation">:</span> <span class="string">&quot;auto&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;wall_clock_breakdown&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>最后，在终端 bash 运行该 <code>pretrain.sh</code> 脚本即可开始训练。</p>
<h2 id="6-2-模型有监督微调"><a href="#6-2-模型有监督微调" class="headerlink" title="6.2 模型有监督微调"></a>6.2 模型有监督微调</h2><p>在上一节，我们介绍了如何使用 Transformers 框架快速、高效地进行模型预训练。在本部分，我们将基于上部分内容，介绍如何使用 Transformers 框架对预训练好的模型进行有监督微调。</p>
<h3 id="6-2-1-Pretrain-VS-SFT"><a href="#6-2-1-Pretrain-VS-SFT" class="headerlink" title="6.2.1 Pretrain VS SFT"></a>6.2.1 Pretrain VS SFT</h3><p>首先需要回顾一下，对 LLM 进行预训练和进行有监督微调的核心差异在于什么。在第四章中提到过，目前成型的 LLM 一般通过 Pretrain-SFT-RLHF 三个阶段来训练，在 Pretrain 阶段，会对海量无监督文本进行自监督建模，来学习文本语义规则和文本中的世界知识；在 SFT 阶段，一般通过对 Pretrain 好的模型进行指令微调，即训练模型根据用户指令完成对应任务，从而使模型能够遵循用户指令，根据用户指令进行规划、行动和输出。因此，Pretrain 和 SFT 均使用 CLM 建模，其核心差异在于，Pretrain 使用海量无监督文本进行训练，模型直接对文本执行“预测下一个 token”的任务；而 SFT 使用构建成对的指令对数据，模型根据输入的指令，建模后续的输出。反映到具体的训练实现上，Pretrain 会对全部 text 进行 loss 计算，要求模型对整个文本实现建模预测；而 SFT 仅对输出进行 loss 计算，不计算指令部分的 loss。</p>
<p>因此，相较于上一节完成的 Pretrain 代码，SFT 部分仅需要修改数据处理环节，实现对指令对数据转化为训练样本的构建，其余部分和 Pretrain 是完全一致的实现逻辑。本部分代码脚本为<code>./code/finetune.py</code>。</p>
<h3 id="6-2-2-微调数据处理"><a href="#6-2-2-微调数据处理" class="headerlink" title="6.2.2 微调数据处理"></a>6.2.2 微调数据处理</h3><p>同样与第五章类似，我们此处使用贝壳开源的 BelleGroup 数据集进行 SFT。</p>
<p>在 SFT 过程中，我们会定义一个 Chat Template，这个 Template 即表示了如何将对话数据转化为一个模型可以建模拟合的文本序列。当我们使用做过 SFT 的模型进行下游任务微调时，一般需要查看该模型的 Chat Template 并进行适配，即是为了不损伤其在 SFT 中学到的指令遵循能力。由于我们此处使用 Pretrain 模型进行 SFT，可以自定义一个 Chat Template。由于我们使用了 Qwen-2.5-1.5B 模型结构进行 Pretrain，此处我们沿承使用 Qwen-2.5 的 Chat Template。如果读者没有足够的资源进行上一部分模型的 Pretrain 的话，此处也可以使用官方的 Qwen-2.5-1.5B 模型作为 SFT 的基座模型。</p>
<p>我们首先定义几个特殊 token，特殊 token 在模型进行拟合中有特殊的作用，包括文本序列开始（BOS）、文本序列结束（EOS）、换行符等。定义特殊 token，有助于避免模型在拟合过程中的语义混淆：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 不同的 tokenizer 需要特别定义</span></span><br><span class="line"><span class="comment"># BOS</span></span><br><span class="line">im_start = tokenizer(<span class="string">&quot;&lt;|im_start|&gt;&quot;</span>).input_ids</span><br><span class="line"><span class="comment"># EOS</span></span><br><span class="line">im_end = tokenizer(<span class="string">&quot;&lt;|im_end|&gt;&quot;</span>).input_ids</span><br><span class="line"><span class="comment"># PAD</span></span><br><span class="line">IGNORE_TOKEN_ID = tokenizer.pad_token_id</span><br><span class="line"><span class="comment"># 换行符</span></span><br><span class="line">nl_tokens = tokenizer(<span class="string">&#x27;\n&#x27;</span>).input_ids</span><br><span class="line"><span class="comment"># 角色标识符</span></span><br><span class="line">_system = tokenizer(<span class="string">&#x27;system&#x27;</span>).input_ids + nl_tokens</span><br><span class="line">_user = tokenizer(<span class="string">&#x27;human&#x27;</span>).input_ids + nl_tokens</span><br><span class="line">_assistant = tokenizer(<span class="string">&#x27;assistant&#x27;</span>).input_ids + nl_tokens</span><br></pre></td></tr></table></figure>

<p>Qwen 系列的 Chat Template 一般有三个对话角色：System、User 和 Assistant。System 是系统提示词，负责激活模型的能力，默认为“You are a helpful assistant.”，一般不会在 SFT 过程中更改使用。User 即为用户给出的提示词，此处由于数据集中的对话角色为 “human”，我们将 “user” 修改为了“human”。Assistant 即为 LLM 给出的回复，也就是模型在 SFT 过程中需要拟合的文本。</p>
<p>接着，由于该数据集是一个多轮对话数据集，我们需要对多轮对话进行拼接处理，将多轮对话拼接到一个文本序列中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 拼接多轮对话</span></span><br><span class="line">input_ids, targets = [], []</span><br><span class="line"><span class="comment"># 多个样本</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(<span class="built_in">len</span>(sources))):</span><br><span class="line">    <span class="comment"># source 为一个多轮对话样本</span></span><br><span class="line">    source = sources[i]</span><br><span class="line">    <span class="comment"># 从 user 开始</span></span><br><span class="line">    <span class="keyword">if</span> source[<span class="number">0</span>][<span class="string">&quot;from&quot;</span>] != <span class="string">&quot;human&quot;</span>:</span><br><span class="line">        source = source[<span class="number">1</span>:]</span><br><span class="line">    <span class="comment"># 分别是输入和输出</span></span><br><span class="line">    input_id, target = [], []</span><br><span class="line">    <span class="comment"># system: 【BOS】system\nYou are a helpful assistant.【EOS】\n</span></span><br><span class="line">    system = im_start + _system + tokenizer(system_message).input_ids + im_end + nl_tokens</span><br><span class="line">    input_id += system</span><br><span class="line">    <span class="comment"># system 不需要拟合</span></span><br><span class="line">    target += im_start + [IGNORE_TOKEN_ID] * (<span class="built_in">len</span>(system)-<span class="number">3</span>) + im_end + nl_tokens</span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">len</span>(input_id) == <span class="built_in">len</span>(target)</span><br><span class="line">    <span class="comment"># 依次拼接</span></span><br><span class="line">    <span class="keyword">for</span> j, sentence <span class="keyword">in</span> <span class="built_in">enumerate</span>(source):</span><br><span class="line">        <span class="comment"># sentence 为一轮对话</span></span><br><span class="line">        role = roles[sentence[<span class="string">&quot;from&quot;</span>]]</span><br><span class="line">        <span class="comment"># user：&lt;|im_start|&gt;human\ninstruction【EOS】\n</span></span><br><span class="line">        <span class="comment"># assistant：&lt;|im_start|&gt;assistant\nresponse【EOS】\n</span></span><br><span class="line">        _input_id = tokenizer(role).input_ids + nl_tokens + \</span><br><span class="line">            tokenizer(sentence[<span class="string">&quot;value&quot;</span>]).input_ids + im_end + nl_tokens</span><br><span class="line">        input_id += _input_id</span><br><span class="line">        <span class="keyword">if</span> role == <span class="string">&#x27;&lt;|im_start|&gt;human&#x27;</span>:</span><br><span class="line">            <span class="comment"># user 不需要拟合</span></span><br><span class="line">            _target = im_start + [IGNORE_TOKEN_ID] * (<span class="built_in">len</span>(_input_id)-<span class="number">3</span>) + im_end + nl_tokens</span><br><span class="line">        <span class="keyword">elif</span> role == <span class="string">&#x27;&lt;|im_start|&gt;assistant&#x27;</span>:</span><br><span class="line">            <span class="comment"># assistant 需要拟合</span></span><br><span class="line">            _target = im_start + [IGNORE_TOKEN_ID] * <span class="built_in">len</span>(tokenizer(role).input_ids) + \</span><br><span class="line">                _input_id[<span class="built_in">len</span>(tokenizer(role).input_ids)+<span class="number">1</span>:-<span class="number">2</span>] + im_end + nl_tokens</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">print</span>(role)</span><br><span class="line">            <span class="keyword">raise</span> NotImplementedError</span><br><span class="line">        target += _target</span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">len</span>(input_id) == <span class="built_in">len</span>(target)</span><br><span class="line">    <span class="comment"># 最后进行 PAD</span></span><br><span class="line">    input_id += [tokenizer.pad_token_id] * (max_len - <span class="built_in">len</span>(input_id))</span><br><span class="line">    target += [IGNORE_TOKEN_ID] * (max_len - <span class="built_in">len</span>(target))</span><br><span class="line">    input_ids.append(input_id[:max_len])</span><br><span class="line">    targets.append(target[:max_len])</span><br></pre></td></tr></table></figure>
<p>上述代码沿承了 Qwen 的 Chat Template 逻辑，读者也可以根据自己的偏好进行修改，其核心点在于 User 的文本不需要拟合，因此 targets 中 User 对应的文本内容是使用的 IGNORE_TOKEN_ID 进行遮蔽，而 Assistant 对应的文本内容则是文本原文，是需要计算 loss 的。目前主流 LLM IGNORE_TOKEN_ID 一般设置为 -100。</p>
<p>完成拼接后，将 tokenize 后的数值序列转化为 <code>Torch.tensor</code>，再拼接成 Dataset 所需的字典返回即可：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">input_ids = torch.tensor(input_ids)</span><br><span class="line">targets = torch.tensor(targets)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="built_in">dict</span>(</span><br><span class="line">    input_ids=input_ids,</span><br><span class="line">    labels=targets,</span><br><span class="line">    attention_mask=input_ids.ne(tokenizer.pad_token_id),</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>完成上述处理逻辑后，需要自定义一个 Dataset 类，在该类中调用该逻辑进行数据的处理：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SupervisedDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, raw_data, tokenizer, max_len: <span class="built_in">int</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(SupervisedDataset, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 加载并预处理数据</span></span><br><span class="line">        sources = [example[<span class="string">&quot;conversations&quot;</span>] <span class="keyword">for</span> example <span class="keyword">in</span> raw_data]</span><br><span class="line">        <span class="comment"># preprocess 即上文定义的数据预处理逻辑</span></span><br><span class="line">        data_dict = preprocess(sources, tokenizer, max_len)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.input_ids = data_dict[<span class="string">&quot;input_ids&quot;</span>]</span><br><span class="line">        <span class="variable language_">self</span>.labels = data_dict[<span class="string">&quot;labels&quot;</span>]</span><br><span class="line">        <span class="variable language_">self</span>.attention_mask = data_dict[<span class="string">&quot;attention_mask&quot;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.input_ids)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, i</span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, torch.Tensor]:</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">dict</span>(</span><br><span class="line">            input_ids=<span class="variable language_">self</span>.input_ids[i],</span><br><span class="line">            labels=<span class="variable language_">self</span>.labels[i],</span><br><span class="line">            attention_mask=<span class="variable language_">self</span>.attention_mask[i],</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>

<p>该类继承自 Torch 的 Dataset 类，可以直接在 Trainer 中使用。完成数据处理后，基于上一节脚本，修改数据处理逻辑即可，后续模型训练等几乎完全一致，此处附上主函数逻辑：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载脚本参数</span></span><br><span class="line">parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))</span><br><span class="line">model_args, data_args, training_args = parser.parse_args_into_dataclasses()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化 WandB</span></span><br><span class="line">wandb.init(project=<span class="string">&quot;sft&quot;</span>, name=<span class="string">&quot;qwen-1.5b&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置日志</span></span><br><span class="line">logging.basicConfig(</span><br><span class="line">    <span class="built_in">format</span>=<span class="string">&quot;%(asctime)s - %(levelname)s - %(name)s - %(message)s&quot;</span>,</span><br><span class="line">    datefmt=<span class="string">&quot;%m/%d/%Y %H:%M:%S&quot;</span>,</span><br><span class="line">    handlers=[logging.StreamHandler(sys.stdout)],</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将日志级别设置为 INFO</span></span><br><span class="line">transformers.utils.logging.set_verbosity_info()</span><br><span class="line">log_level = training_args.get_process_log_level()</span><br><span class="line">logger.setLevel(log_level)</span><br><span class="line">datasets.utils.logging.set_verbosity(log_level)</span><br><span class="line">transformers.utils.logging.set_verbosity(log_level)</span><br><span class="line">transformers.utils.logging.enable_default_handler()</span><br><span class="line">transformers.utils.logging.enable_explicit_format()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练整体情况记录</span></span><br><span class="line">logger.warning(</span><br><span class="line">    <span class="string">f&quot;Process rank: <span class="subst">&#123;training_args.local_rank&#125;</span>, device: <span class="subst">&#123;training_args.device&#125;</span>, n_gpu: <span class="subst">&#123;training_args.n_gpu&#125;</span>&quot;</span></span><br><span class="line">    + <span class="string">f&quot;distributed training: <span class="subst">&#123;<span class="built_in">bool</span>(training_args.local_rank != -<span class="number">1</span>)&#125;</span>, 16-bits training: <span class="subst">&#123;training_args.fp16&#125;</span>&quot;</span></span><br><span class="line">)</span><br><span class="line">logger.info(<span class="string">f&quot;Training/evaluation parameters <span class="subst">&#123;training_args&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查 checkpoint</span></span><br><span class="line">last_checkpoint = <span class="literal">None</span></span><br><span class="line"><span class="keyword">if</span> os.path.isdir(training_args.output_dir):</span><br><span class="line">    last_checkpoint = get_last_checkpoint(training_args.output_dir)</span><br><span class="line">    <span class="keyword">if</span> last_checkpoint <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">and</span> <span class="built_in">len</span>(os.listdir(training_args.output_dir)) &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(</span><br><span class="line">            <span class="string">f&quot;输出路径 (<span class="subst">&#123;training_args.output_dir&#125;</span>) 非空 &quot;</span></span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">elif</span> last_checkpoint <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> training_args.resume_from_checkpoint <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        logger.info(</span><br><span class="line">            <span class="string">f&quot;从 <span class="subst">&#123;last_checkpoint&#125;</span>恢复训练&quot;</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置随机数种子.</span></span><br><span class="line">set_seed(training_args.seed)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化模型</span></span><br><span class="line">logger.warning(<span class="string">&quot;加载预训练模型&quot;</span>)</span><br><span class="line">logger.info(<span class="string">f&quot;模型参数地址：<span class="subst">&#123;model_args.model_name_or_path&#125;</span>&quot;</span>)</span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(model_args.model_name_or_path,trust_remote_code=<span class="literal">True</span>)</span><br><span class="line">n_params = <span class="built_in">sum</span>(&#123;p.data_ptr(): p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters()&#125;.values())</span><br><span class="line">logger.info(<span class="string">f&quot;继承一个预训练模型 - Total size=<span class="subst">&#123;n_params/<span class="number">2</span>**<span class="number">20</span>:<span class="number">.2</span>f&#125;</span>M params&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化 Tokenizer</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path)</span><br><span class="line">logger.info(<span class="string">&quot;完成 tokenzier 加载&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载微调数据</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(data_args.train_files) <span class="keyword">as</span> f:</span><br><span class="line">    lst = [json.loads(line) <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines()[:<span class="number">10000</span>]]</span><br><span class="line">logger.info(<span class="string">&quot;完成训练集加载&quot;</span>)</span><br><span class="line">logger.info(<span class="string">f&quot;训练集地址：<span class="subst">&#123;data_args.train_files&#125;</span>&quot;</span>)</span><br><span class="line">logger.info(<span class="string">f&#x27;训练样本总数:<span class="subst">&#123;<span class="built_in">len</span>(lst)&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="comment"># logger.info(f&quot;训练集采样：&#123;ds[&quot;train&quot;][0]&#125;&quot;)</span></span><br><span class="line"></span><br><span class="line">train_dataset = SupervisedDataset(lst, tokenizer=tokenizer, max_len=<span class="number">2048</span>)</span><br><span class="line"></span><br><span class="line">logger.info(<span class="string">&quot;初始化 Trainer&quot;</span>)</span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model=model,</span><br><span class="line">    args=training_args,</span><br><span class="line">    train_dataset= IterableWrapper(train_dataset),</span><br><span class="line">    tokenizer=tokenizer</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从 checkpoint 加载</span></span><br><span class="line">checkpoint = <span class="literal">None</span></span><br><span class="line"><span class="keyword">if</span> training_args.resume_from_checkpoint <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    checkpoint = training_args.resume_from_checkpoint</span><br><span class="line"><span class="keyword">elif</span> last_checkpoint <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        checkpoint = last_checkpoint</span><br><span class="line"></span><br><span class="line">logger.info(<span class="string">&quot;开始训练&quot;</span>)</span><br><span class="line">train_result = trainer.train(resume_from_checkpoint=checkpoint)</span><br><span class="line">trainer.save_model() </span><br></pre></td></tr></table></figure>

<p>启动方式也同样在 sh 脚本中使用 deepspeed 启动即可，此处不再赘述，源码见 .&#x2F;code&#x2F;finetune.sh。</p>
<h2 id="6-3-高效微调"><a href="#6-3-高效微调" class="headerlink" title="6.3 高效微调"></a>6.3 高效微调</h2><p>在前面几节，我们详细介绍了基于 Transformers 框架对模型进行 Pretrain、SFT 以及 RLHF 的原理和实践细节。但是，由于 LLM 参数量大，训练数据多，通过上述方式对模型进行训练（主要指 SFT 及 RLHF）需要调整模型全部参数，资源压力非常大。对资源有限的企业或课题组来说，如何高效、快速对模型进行领域或任务的微调，以低成本地使用 LLM 完成目标任务，是非常重要的。</p>
<h3 id="6-3-1-高效微调方案"><a href="#6-3-1-高效微调方案" class="headerlink" title="6.3.1 高效微调方案"></a>6.3.1 高效微调方案</h3><p>针对全量微调的昂贵问题，目前主要有两种解决方案：</p>
<p><strong>Adapt Tuning</strong>。即在模型中添加 Adapter 层，在微调时冻结原参数，仅更新 Adapter 层。</p>
<p>具体而言，其在预训练模型每层中插入用于下游任务的参数，即 Adapter 模块，在微调时冻结模型主体，仅训练特定于任务的参数，如图6.8所示。</p>
<div align='center'>
    <img src="../images/6-images/3-1.png" alt="alt text" width="90%">
    <p>图6.8 Adapt Tuning</p>
</div>

<p>每个 Adapter 模块由两个前馈子层组成，第一个前馈子层将 Transformer 块的输出作为输入，将原始输入维度 $d$ 投影到 $m$，通过控制 $m$ 的大小来限制 Adapter 模块的参数量，通常情况下 $m &lt;&lt; d$。在输出阶段，通过第二个前馈子层还原输入维度，将 $m$ 重新投影到 $d$，作为 Adapter 模块的输出(如上图右侧结构)。</p>
<p>LoRA 事实上就是一种改进的 Adapt Tuning 方法。但 Adapt Tuning 方法存在推理延迟问题，由于增加了额外参数和额外计算量，导致微调之后的模型计算速度相较原预训练模型更慢。</p>
<p><strong>Prefix Tuning</strong>。该种方法固定预训练 LM，为 LM 添加可训练，任务特定的前缀，这样就可以为不同任务保存不同的前缀，微调成本也小。具体而言，在每一个输入 token 前构造一段与下游任务相关的 virtual tokens 作为 prefix，在微调时只更新 prefix 部分的参数，而其他参数冻结不变。</p>
<p>也是目前常用的微量微调方法的 Ptuning，其实就是 Prefix Tuning 的一种改进。但 Prefix Tuning 也存在固定的缺陷：模型可用序列长度减少。由于加入了 virtual tokens，占用了可用序列长度，因此越高的微调质量，模型可用序列长度就越低。</p>
<h3 id="6-3-2-LoRA-微调"><a href="#6-3-2-LoRA-微调" class="headerlink" title="6.3.2 LoRA 微调"></a>6.3.2 LoRA 微调</h3><p>如果一个大模型是将数据映射到高维空间进行处理，这里假定在处理一个细分的小任务时，是不需要那么复杂的大模型的，可能只需要在某个子空间范围内就可以解决，那么也就不需要对全量参数进行优化了，我们可以定义当对某个子空间参数进行优化时，能够达到全量参数优化的性能的一定水平（如90%精度）时，那么这个子空间参数矩阵的秩就可以称为对应当前待解决问题的本征秩（intrinsic rank）。</p>
<p>预训练模型本身就隐式地降低了本征秩，当针对特定任务进行微调后，模型中权重矩阵其实具有更低的本征秩（intrinsic rank）。同时，越简单的下游任务，对应的本征秩越低。（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2012.13255">Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning</a>）因此，权重更新的那部分参数矩阵尽管随机投影到较小的子空间，仍然可以有效的学习，可以理解为针对特定的下游任务这些权重矩阵就不要求满秩。我们可以通过优化密集层在适应过程中变化的秩分解矩阵来间接训练神经网络中的一些密集层，从而实现仅优化密集层的秩分解矩阵来达到微调效果。</p>
<p>例如，假设预训练参数为 $\theta^D_0$，在特定下游任务上密集层权重参数矩阵对应的本征秩为 $\theta^d$，对应特定下游任务微调参数为 $\theta^D$，那么有：</p>
<p>$$\theta^D &#x3D; \theta^D_0 + \theta^d M$$</p>
<p>这个 $M$ 即为 LoRA 优化的秩分解矩阵。</p>
<p>想对于其他高效微调方法，LoRA 存在以下优势：</p>
<ol>
<li>可以针对不同的下游任务构建小型 LoRA 模块，从而在共享预训练模型参数基础上有效地切换下游任务。</li>
<li>LoRA 使用自适应优化器（Adaptive Optimizer），不需要计算梯度或维护大多数参数的优化器状态，训练更有效、硬件门槛更低。</li>
<li>LoRA 使用简单的线性设计，在部署时将可训练矩阵与冻结权重合并，不存在推理延迟。</li>
<li>LoRA 与其他方法正交，可以组合。</li>
</ol>
<p>因此，LoRA 成为目前高效微调 LLM 的主流方法，尤其是对于资源受限、有监督训练数据受限的情况下，LoRA 微调往往会成为 LLM 微调的首选方法。</p>
<h3 id="6-3-3-LoRA-微调的原理"><a href="#6-3-3-LoRA-微调的原理" class="headerlink" title="6.3.3 LoRA 微调的原理"></a>6.3.3 LoRA 微调的原理</h3><h4 id="（1）低秩参数化更新矩阵"><a href="#（1）低秩参数化更新矩阵" class="headerlink" title="（1）低秩参数化更新矩阵"></a>（1）低秩参数化更新矩阵</h4><p>LoRA 假设权重更新的过程中也有一个较低的本征秩，对于预训练的权重参数矩阵 $W0 \in R^{d \times k}$ ($d$ 为上一层输出维度，$k$ 为下一层输入维度)，使用低秩分解来表示其更新：</p>
<p>$$W_0 + {\Delta}W &#x3D; W_0 + BA \space\space  where \space B \in R^{d \times r}, A \in R^{r \times k}$$</p>
<p>在训练过程中，$W_0$ 冻结不更新，$A$、$B$ 包含可训练参数。</p>
<p>因此，LoRA 的前向传递函数为：</p>
<p>$$h &#x3D; W_0 x + \Delta W x &#x3D; W_0 x + B A x$$</p>
<p>在开始训练时，对 $A$ 使用随机高斯初始化，对 $B$ 使用零初始化，然后使用 Adam 进行优化。</p>
<p>训练思路如图6.9所示：</p>
<div align='center'>
    <img src="../images/6-images/3-2.jpg" alt="alt text" width="90%">
    <p>图6.9 LoRA</p>
</div>

<h4 id="（2）应用于-Transformer"><a href="#（2）应用于-Transformer" class="headerlink" title="（2）应用于 Transformer"></a>（2）应用于 Transformer</h4><p>在 Transformer 结构中，LoRA 技术主要应用在注意力模块的四个权重矩阵：$W_q$、$W_k$、$W_v$、$W_0$，而冻结 MLP 的权重矩阵。</p>
<p>通过消融实验发现同时调整 $W_q$ 和 $W_v$ 会产生最佳结果。</p>
<p>在上述条件下，可训练参数个数为：</p>
<p>$$\Theta &#x3D; 2 \times L_{LoRA} \times d_{model} \times r$$</p>
<p>其中，$L_{LoRA}$ 为应用 LoRA 的权重矩阵的个数，$d_{model}$ 为 Transformer 的输入输出维度，$r$ 为设定的 LoRA 秩。</p>
<p>一般情况下，r 取到 4、8、16。</p>
<h3 id="6-3-4-LoRA-的代码实现"><a href="#6-3-4-LoRA-的代码实现" class="headerlink" title="6.3.4 LoRA 的代码实现"></a>6.3.4 LoRA 的代码实现</h3><p>目前一般通过 peft 库来实现模型的 LoRA 微调。peft 库是 huggingface 开发的第三方库，其中封装了包括 LoRA、Adapt Tuning、P-tuning 等多种高效微调方法，可以基于此便捷地实现模型的 LoRA 微调。</p>
<p>本文简单解析 peft 库中的 LoRA 微调代码，简单分析 LoRA 微调的代码实现。</p>
<h4 id="（1）实现流程"><a href="#（1）实现流程" class="headerlink" title="（1）实现流程"></a>（1）实现流程</h4><p>LoRA 微调的内部实现流程主要包括以下几个步骤：</p>
<ol>
<li><p>确定要使用 LoRA 的层。peft 库目前支持调用 LoRA 的层包括：nn.Linear、nn.Embedding、nn.Conv2d 三种。</p>
</li>
<li><p>对每一个要使用 LoRA 的层，替换为 LoRA 层。所谓 LoRA 层，实则是在该层原结果基础上增加了一个旁路，通过低秩分解（即矩阵 $A$ 和矩阵 $B$）来模拟参数更新。</p>
</li>
<li><p>冻结原参数，进行微调，更新 LoRA 层参数。</p>
</li>
</ol>
<h4 id="（2）确定-LoRA-层"><a href="#（2）确定-LoRA-层" class="headerlink" title="（2）确定 LoRA 层"></a>（2）确定 LoRA 层</h4><p>在进行 LoRA 微调时，首先需要确定 LoRA 微调参数，其中一个重要参数即是 target_modules。target_modules 一般是一个字符串列表，每一个字符串是需要进行 LoRA 的层名称，例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">target_modules = [<span class="string">&quot;q_proj&quot;</span>,<span class="string">&quot;v_proj&quot;</span>]</span><br></pre></td></tr></table></figure>

<p>这里的 q_proj 即为注意力机制中的 $W_q$， v_proj 即为注意力机制中的 $W_v$。我们可以根据模型架构和任务要求自定义需要进行 LoRA 操作的层。</p>
<p>在创建 LoRA 模型时，会获取该参数，然后在原模型中找到对应的层，该操作主要通过使用 re 对层名进行正则匹配实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 找到模型的各个组件中，名字里带&quot;q_proj&quot;，&quot;v_proj&quot;的</span></span><br><span class="line">target_module_found = re.fullmatch(<span class="variable language_">self</span>.peft_config.target_modules, key)</span><br><span class="line"><span class="comment"># 这里的 key，是模型的组件名</span></span><br></pre></td></tr></table></figure>

<h4 id="（3）替换-LoRA-层"><a href="#（3）替换-LoRA-层" class="headerlink" title="（3）替换 LoRA 层"></a>（3）替换 LoRA 层</h4><p>对于找到的每一个目标层，会创建一个新的 LoRA 层进行替换。</p>
<p>LoRA 层在具体实现上，是定义了一个基于 Lora 基类的 Linear 类，该类同时继承了 nn.Linear 和 LoraLayer。LoraLayer 即是 Lora 基类，其主要构造了 LoRA 的各种超参：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LoraLayer</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        r: <span class="built_in">int</span>, <span class="comment"># LoRA 的秩</span></span></span><br><span class="line"><span class="params">        lora_alpha: <span class="built_in">int</span>, <span class="comment"># 归一化参数</span></span></span><br><span class="line"><span class="params">        lora_dropout: <span class="built_in">float</span>, <span class="comment"># LoRA 层的 dropout 比例</span></span></span><br><span class="line"><span class="params">        merge_weights: <span class="built_in">bool</span>, <span class="comment"># eval 模式中，是否将 LoRA 矩阵的值加到原权重矩阵上</span></span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="variable language_">self</span>.r = r</span><br><span class="line">        <span class="variable language_">self</span>.lora_alpha = lora_alpha</span><br><span class="line">        <span class="comment"># Optional dropout</span></span><br><span class="line">        <span class="keyword">if</span> lora_dropout &gt; <span class="number">0.0</span>:</span><br><span class="line">            <span class="variable language_">self</span>.lora_dropout = nn.Dropout(p=lora_dropout)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="variable language_">self</span>.lora_dropout = <span class="keyword">lambda</span> x: x</span><br><span class="line">        <span class="comment"># Mark the weight as unmerged</span></span><br><span class="line">        <span class="variable language_">self</span>.merged = <span class="literal">False</span></span><br><span class="line">        <span class="variable language_">self</span>.merge_weights = merge_weights</span><br><span class="line">        <span class="variable language_">self</span>.disable_adapters = <span class="literal">False</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>nn.Linear 就是 Pytorch 的线性层实现。Linear 类就是具体的 LoRA 层，其主要实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Linear</span>(nn.Linear, LoraLayer):</span><br><span class="line">    <span class="comment"># LoRA 层</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        in_features: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">        out_features: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">        r: <span class="built_in">int</span> = <span class="number">0</span>,</span></span><br><span class="line"><span class="params">        lora_alpha: <span class="built_in">int</span> = <span class="number">1</span>,</span></span><br><span class="line"><span class="params">        lora_dropout: <span class="built_in">float</span> = <span class="number">0.0</span>,</span></span><br><span class="line"><span class="params">        fan_in_fan_out: <span class="built_in">bool</span> = <span class="literal">False</span>, </span></span><br><span class="line"><span class="params">        merge_weights: <span class="built_in">bool</span> = <span class="literal">True</span>,</span></span><br><span class="line"><span class="params">        **kwargs,</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="comment"># 继承两个基类的构造函数</span></span><br><span class="line">        nn.Linear.__init__(<span class="variable language_">self</span>, in_features, out_features, **kwargs)</span><br><span class="line">        LoraLayer.__init__(<span class="variable language_">self</span>, r=r, lora_alpha=lora_alpha, lora_dropout=lora_dropout, merge_weights=merge_weights)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.fan_in_fan_out = fan_in_fan_out</span><br><span class="line">        <span class="comment"># Actual trainable parameters</span></span><br><span class="line">        <span class="keyword">if</span> r &gt; <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># 参数矩阵 A</span></span><br><span class="line">            <span class="variable language_">self</span>.lora_A = nn.Linear(in_features, r, bias=<span class="literal">False</span>)</span><br><span class="line">            <span class="comment"># 参数矩阵 B</span></span><br><span class="line">            <span class="variable language_">self</span>.lora_B = nn.Linear(r, out_features, bias=<span class="literal">False</span>)</span><br><span class="line">            <span class="comment"># 归一化系数</span></span><br><span class="line">            <span class="variable language_">self</span>.scaling = <span class="variable language_">self</span>.lora_alpha / <span class="variable language_">self</span>.r</span><br><span class="line">            <span class="comment"># 冻结原参数，仅更新 A 和 B</span></span><br><span class="line">            <span class="variable language_">self</span>.weight.requires_grad = <span class="literal">False</span></span><br><span class="line">        <span class="comment"># 初始化 A 和 B</span></span><br><span class="line">        <span class="variable language_">self</span>.reset_parameters()</span><br><span class="line">        <span class="keyword">if</span> fan_in_fan_out:</span><br><span class="line">            <span class="variable language_">self</span>.weight.data = <span class="variable language_">self</span>.weight.data.T</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>替换时，直接将原层的 weight 和 bias 复制给新的 LoRA 层，再将新的 LoRA 层分配到指定设备即可。</p>
<h4 id="（4）训练"><a href="#（4）训练" class="headerlink" title="（4）训练"></a>（4）训练</h4><p>实现了 LoRA 层的替换后，进行微调训练即可。由于在 LoRA 层中已冻结原参数，在训练中只有 A 和 B 的参数会被更新，从而实现了高效微调。训练的整体过程与原 Fine-tune 类似，此处不再赘述。由于采用了 LoRA 方式，forward 函数也会对应调整：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: torch.Tensor</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.disable_adapters:</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.r &gt; <span class="number">0</span> <span class="keyword">and</span> <span class="variable language_">self</span>.merged:</span><br><span class="line">            <span class="variable language_">self</span>.weight.data -= (</span><br><span class="line">                transpose(<span class="variable language_">self</span>.lora_B.weight @ <span class="variable language_">self</span>.lora_A.weight, <span class="variable language_">self</span>.fan_in_fan_out) * <span class="variable language_">self</span>.scaling</span><br><span class="line">            )</span><br><span class="line">            <span class="variable language_">self</span>.merged = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> F.linear(x, transpose(<span class="variable language_">self</span>.weight, <span class="variable language_">self</span>.fan_in_fan_out), bias=<span class="variable language_">self</span>.bias)</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;主要分支&#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> <span class="variable language_">self</span>.r &gt; <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> <span class="variable language_">self</span>.merged:</span><br><span class="line">        result = F.linear(x, transpose(<span class="variable language_">self</span>.weight, <span class="variable language_">self</span>.fan_in_fan_out), bias=<span class="variable language_">self</span>.bias)</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.r &gt; <span class="number">0</span>:</span><br><span class="line">            result += <span class="variable language_">self</span>.lora_B(<span class="variable language_">self</span>.lora_A(<span class="variable language_">self</span>.lora_dropout(x))) * <span class="variable language_">self</span>.scaling</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> F.linear(x, transpose(<span class="variable language_">self</span>.weight, <span class="variable language_">self</span>.fan_in_fan_out), bias=<span class="variable language_">self</span>.bias)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>上述代码由于考虑到参数合并问题，有几个分支，此处我们仅阅读第二个分支即 elif 分支即可。基于 LoRA 的前向计算过程如前文公式所示，首先计算原参数与输入的乘积，再加上 A、B 分别与输入的乘积即可。</p>
<h3 id="6-3-5-使用-peft-实现-LoRA-微调"><a href="#6-3-5-使用-peft-实现-LoRA-微调" class="headerlink" title="6.3.5 使用 peft 实现 LoRA 微调"></a>6.3.5 使用 peft 实现 LoRA 微调</h3><p>peft 进行了很好的封装，支持我们便捷、高效地对大模型进行微调。此处以第二节的 LLM SFT 为例，简要介绍如何使用 peft 对大模型进行微调。如果是应用在 RLHF 上，整体思路是一致的。</p>
<p>首先加载所需使用库：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModel</span><br><span class="line"><span class="keyword">from</span> peft <span class="keyword">import</span> get_peft_model, LoraConfig, TaskType, PeftModel</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> Trainer</span><br></pre></td></tr></table></figure>

<p>其次加载原模型与原 tokenizer，此处和第二节一致：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载基座模型</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line">model = AutoModel.from_pretrained(</span><br><span class="line">    MODEL_PATH, trust_remote_code=<span class="literal">True</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>接着，设定 peft 参数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">peft_config = LoraConfig(</span><br><span class="line">            task_type=TaskType.CAUSAL_LM,</span><br><span class="line">            inference_mode=<span class="literal">False</span>,</span><br><span class="line">            r=<span class="number">8</span>,</span><br><span class="line">            lora_alpha=<span class="number">32</span>,</span><br><span class="line">            lora_dropout=<span class="number">0.1</span>,</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>

<p>注意，对不同的模型，LoRA 参数可能有所区别。例如，对于 ChatGLM，无需指定 target_modeules，peft 可以自行找到；对于 BaiChuan，就需要手动指定。task_type 是模型的任务类型，大模型一般都是 CAUSAL_LM 即传统语言模型。</p>
<p>然后获取 LoRA 模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = get_peft_model(model, peft_config)</span><br></pre></td></tr></table></figure>

<p>此处的 get_peft_model 的底层操作，即为上文分析的具体实现。</p>
<p>最后使用 transformers 提供的 Trainer 进行训练即可，训练占用的显存就会有大幅度的降低：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">trainer = Trainer(</span><br><span class="line">    model=model,</span><br><span class="line">    args=training_args,</span><br><span class="line">    train_dataset= IterableWrapper(train_dataset),</span><br><span class="line">    tokenizer=tokenizer</span><br><span class="line">)</span><br><span class="line">trainer.train()</span><br></pre></td></tr></table></figure>

<p>如果是应用在 DPO、KTO 上，则也相同的加入 LoRA 参数并通过 <code>get_peft_model</code> 获取一个 LoRA 模型即可，其他的不需要进行任何修改。但要注意的是，LoRA 微调能够大幅度降低显卡占用，且在下游任务适配上能够取得较好的效果，但如果是需要学习对应知识的任务，LoRA 由于只调整低秩矩阵，难以实现知识的注入，一般效果不佳，因此不推荐使用 LoRA 进行模型预训练或后训练。</p>
<p><strong>参考资料</strong></p>
<p>[1] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. (2019). <em>Parameter-Efficient Transfer Learning for NLP.</em> arXiv preprint arXiv:1902.00751.</p>
<p>[2] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. (2021). <em>LoRA: Low-Rank Adaptation of Large Language Models.</em> arXiv preprint arXiv:2106.09685.</p>
<p>[3] Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. (2020). <em>Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning.</em> arXiv preprint arXiv:2012.13255.</p>
<p>[4] Xiang Lisa Li 和 Percy Liang. (2021). <em>Prefix-Tuning: Optimizing Continuous Prompts for Generation.</em> arXiv preprint arXiv:2101.00190.</p>

        
      </div>

         
    </div>
    
     
  </div>
  
    
<nav id="article-nav">
  <a class="article-nav-btn left "
    
      href="/2025/06/09/%E5%8A%A8%E6%89%8B%E6%90%AD%E5%BB%BA%E5%A4%A7%E6%A8%A1%E5%9E%8B/"
      title="动手搭建大模型"
     >
    <i class="fa-solid fa-angle-left"></i>
    <p class="title-text">
      
        动手搭建大模型
        
    </p>
  </a>
  <a class="article-nav-btn right "
    
      href="/2025/06/06/hot100%E7%A7%BB%E5%8A%A8%E9%9B%B6/"
      title="✨ LeetCode Hot 100 - 283. 移动零 ✨"
     >

    <p class="title-text">
      
        ✨ LeetCode Hot 100 - 283. 移动零 ✨
        
    </p>
    <i class="fa-solid fa-angle-right"></i>
  </a>
</nav>


  
</article>






    </div>
    <div id="footer-wrapper">
      <footer id="footer">
  
  <div id="footer-info" class="inner">
    
    &copy; 2025 Kouir Wu<br>
    Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> & Theme <a target="_blank" rel="noopener" href="https://github.com/saicaca/hexo-theme-vivia">Vivia</a>
  </div>
</footer>

    </div>
    <div class="back-to-top-wrapper">
    <button id="back-to-top-btn" class="back-to-top-btn hide" onclick="topFunction()">
        <i class="fa-solid fa-angle-up"></i>
    </button>
</div>

<script>
    function topFunction() {
        window.scroll({ top: 0, behavior: 'smooth' });
    }
    let btn = document.getElementById('back-to-top-btn');
    function scrollFunction() {
        if (document.body.scrollTop > 600 || document.documentElement.scrollTop > 600) {
            btn.classList.remove('hide')
        } else {
            btn.classList.add('hide')
        }
    }
    window.onscroll = function() {
        scrollFunction();
    }
</script>

  </div>
  <script src="/js/light-dark-switch.js"></script>
</body>
</html>
